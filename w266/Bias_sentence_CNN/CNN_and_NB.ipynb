{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sentences = pd.read_csv(\"bias_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    113299\n",
       "label       113299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_annotator(sent, list1, list2):\n",
    "    ind1 = ind2 = len(sent)\n",
    "    if any(word in sent for word in list1):\n",
    "        ind1 = max([sent.lower().find(word) for word in list1])\n",
    "    elif any(word in sent for word in list2):\n",
    "        ind2 = max([sent.lower().find(word) for word in list2])\n",
    "    else:\n",
    "        ind1 = ind2 = len(sent)\n",
    "        \n",
    "    if ind1 < ind2:\n",
    "        return 'liberal'\n",
    "    elif ind2 < ind1:\n",
    "        return 'conservative'\n",
    "    else: \n",
    "        return 'neutral'       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_list = ['liberal','democrat','pro-choice','agnostic','abortion','gay','freedom'\n",
    "            ,'climate','secular','civil','liberty','equality','regulation'\n",
    "            ,'violence','ACLU','right','far-right']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['conservative','republican','pro-life','catholic','free market','tax cut','second amendment'\n",
    "            ,'fair trade','GOP','NRA','christian','left','left-wing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database = pd.read_csv(\"ibcData.csv\",sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['liberal', 'conservative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels_ibc = []\n",
    " \n",
    "for i in range(ibc_database.count()[0]):\n",
    "    weak_labels_ibc.append(weak_annotator(ibc_database.iloc[i][0],lib_list,con_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels_bias = []\n",
    " \n",
    "for i in range(bias_sentences.count()['sentence']):\n",
    "    weak_labels_bias.append(weak_annotator(bias_sentences.iloc[i]['sentence'],lib_list,con_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sentences['label'] = weak_labels_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the last 365 days, we've had problems: crit...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As expected, the end of the quarter and the ye...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This traditional ''window-dressing'' helped sh...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last year I was spot-on about the collapse of ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE average consumer's telephone bill has ris...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence    label\n",
       "0  In the last 365 days, we've had problems: crit...  neutral\n",
       "1  As expected, the end of the quarter and the ye...  neutral\n",
       "2  This traditional ''window-dressing'' helped sh...  neutral\n",
       "3  Last year I was spot-on about the collapse of ...  neutral\n",
       "4   THE average consumer's telephone bill has ris...  neutral"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "762"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('liberal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3334"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "230"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('conservative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database['weak_label'] = weak_labels_ibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>weak_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forcing middle-class workers to bear a greater...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Because it would not be worthwhile to bring a ...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indeed , Lind argues that high profits and hig...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In fairness , it should be noted that he devot...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psychological tactics are social control techn...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1 weak_label\n",
       "0  Forcing middle-class workers to bear a greater...  liberal    neutral\n",
       "1  Because it would not be worthwhile to bring a ...  liberal    neutral\n",
       "2  Indeed , Lind argues that high profits and hig...  liberal    neutral\n",
       "3  In fairness , it should be noted that he devot...  liberal    liberal\n",
       "4  Psychological tactics are social control techn...  liberal    neutral"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize IBC database\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "X = ibc_database[0]\n",
    "U = bias_sentences['sentence']\n",
    "\n",
    "X_tokens = []\n",
    "U_tokens = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X_tokens.append(tokenizer.tokenize(X[i]))\n",
    "    \n",
    "for i in range(len(U)):\n",
    "    U_tokens.append(tokenizer.tokenize(U[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalize IBC database\n",
    "\n",
    "X_tokens_canon = []\n",
    "U_tokens_canon = []\n",
    "\n",
    "for i in range(len(X_tokens)):\n",
    "    X_tokens_canon.append(utils.canonicalize_words(X_tokens[i]))\n",
    "    \n",
    "for i in range(len(U_tokens)):\n",
    "    U_tokens_canon.append(utils.canonicalize_words(U_tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of lists consisting of canonicalized sentences\n",
    "\n",
    "X_final = list(itertools.chain.from_iterable(X_tokens_canon))\n",
    "U_final = list(itertools.chain.from_iterable(U_tokens_canon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15721\n",
      "54687\n"
     ]
    }
   ],
   "source": [
    "# number of unique tokens in database\n",
    "print(len(set(X_final)))\n",
    "print(len(set(U_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigrams, could be used for creating distribution plots\n",
    "\n",
    "unigram_counts_X = collections.Counter()\n",
    "unigram_counts_U = collections.Counter()\n",
    "\n",
    "for word in X_final:  \n",
    "    unigram_counts_X[word] += 1\n",
    "    \n",
    "for word in U_final:  \n",
    "    unigram_counts_U[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_X = unigram_counts_X.keys()\n",
    "\n",
    "counts_X = []\n",
    "for w in words_X:\n",
    "    counts_X.append(unigram_counts_X[w])\n",
    "    \n",
    "words_U = unigram_counts_U.keys()\n",
    "\n",
    "counts_U = []\n",
    "for w in words_U:\n",
    "    counts_U.append(unigram_counts_U[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the words according to their counts\n",
    "\n",
    "unigram_list_X = [x for _,x in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "unigram_count_X = [y for y,_ in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "\n",
    "unigram_list_U = [x for _,x in sorted(zip(counts_U,words_U),  reverse=True)]\n",
    "unigram_count_U = [y for y,_ in sorted(zip(counts_U,words_U),  reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9cafb59e-b113-4c89-9727-d0c94c84aca5\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9cafb59e-b113-4c89-9727-d0c94c84aca5' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9cafb59e-b113-4c89-9727-d0c94c84aca5' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"9cafb59e-b113-4c89-9727-d0c94c84aca5\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of unigrams\n",
    "\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_X, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"cd71c058-d743-45ec-812e-e3fb5e67b017\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"e40d6d9a-1374-4dc8-ad34-0f4999b1565f\":{\"roots\":{\"references\":[{\"attributes\":{\"formatter\":{\"id\":\"93d4c55e-8137-464a-b797-1892514ed64d\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"431106ca-df11-4082-8c68-64d269f78bca\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b829d982-779a-45a4-9ecd-a08496463971\",\"type\":\"CategoricalTicker\"}},\"id\":\"ea11c071-d86b-44d5-a0d9-180fc38f553d\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"58010d00-8ecc-49f7-a5b7-ac5692356bb9\",\"type\":\"BasicTicker\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"0a20d73a-d01a-45b7-9191-cfb4a1d6b907\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"93d4c55e-8137-464a-b797-1892514ed64d\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"0429adbb-497c-4a40-ab77-cb62dc77e510\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"431106ca-df11-4082-8c68-64d269f78bca\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"58010d00-8ecc-49f7-a5b7-ac5692356bb9\",\"type\":\"BasicTicker\"}},\"id\":\"bf23c644-edbc-4ce0-8746-3ac227ce793a\",\"type\":\"LinearAxis\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"558cba0f-a3d6-486c-8ee6-d5b17ed7f194\",\"type\":\"PanTool\"},{\"id\":\"d98952ef-10fd-43d2-84b3-a3084bb3f039\",\"type\":\"WheelZoomTool\"},{\"id\":\"6d0c690c-6abb-4576-adc0-b6e8e5064f85\",\"type\":\"BoxZoomTool\"},{\"id\":\"d63dc71b-0fac-4c63-96df-b60346acddbe\",\"type\":\"SaveTool\"},{\"id\":\"c281c94a-6203-41ab-94c7-ba5423989bb1\",\"type\":\"ResetTool\"},{\"id\":\"5482a113-8518-4d79-a079-d47e2df477a3\",\"type\":\"HelpTool\"},{\"id\":\"6b2aa792-298c-47af-bc86-2dc3ae11b02d\",\"type\":\"HoverTool\"}]},\"id\":\"95ae924b-293f-4706-b860-d9741cc927d0\",\"type\":\"Toolbar\"},{\"attributes\":{\"overlay\":{\"id\":\"f81509c8-6444-4439-b314-b875f79522f3\",\"type\":\"BoxAnnotation\"}},\"id\":\"6d0c690c-6abb-4576-adc0-b6e8e5064f85\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"c281c94a-6203-41ab-94c7-ba5423989bb1\",\"type\":\"ResetTool\"},{\"attributes\":{\"below\":[{\"id\":\"ea11c071-d86b-44d5-a0d9-180fc38f553d\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"bf23c644-edbc-4ce0-8746-3ac227ce793a\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"ea11c071-d86b-44d5-a0d9-180fc38f553d\",\"type\":\"CategoricalAxis\"},{\"id\":\"02f4fadb-29e2-4a90-b48b-15c12bac43f0\",\"type\":\"Grid\"},{\"id\":\"bf23c644-edbc-4ce0-8746-3ac227ce793a\",\"type\":\"LinearAxis\"},{\"id\":\"a541e4a3-a463-48c5-9724-f93926a83ce8\",\"type\":\"Grid\"},{\"id\":\"f81509c8-6444-4439-b314-b875f79522f3\",\"type\":\"BoxAnnotation\"},{\"id\":\"3ae8706e-11a7-456c-b17b-804bbfc35a14\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"412caeef-f65c-4669-a3df-7dc0094521f7\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"95ae924b-293f-4706-b860-d9741cc927d0\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"4359fc20-5287-4b02-a795-1bd698abe627\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"e19232b0-960e-4476-ac7e-f9449651c639\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"f12a73f7-fe86-43cd-b514-3848048328fe\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"2f44a546-52d7-4c93-84aa-1c8e2b26b6ce\",\"type\":\"LinearScale\"}},\"id\":\"431106ca-df11-4082-8c68-64d269f78bca\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"558cba0f-a3d6-486c-8ee6-d5b17ed7f194\",\"type\":\"PanTool\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"3ae8706e-11a7-456c-b17b-804bbfc35a14\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"6b2aa792-298c-47af-bc86-2dc3ae11b02d\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"d63dc71b-0fac-4c63-96df-b60346acddbe\",\"type\":\"SaveTool\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"412caeef-f65c-4669-a3df-7dc0094521f7\",\"type\":\"Title\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"431106ca-df11-4082-8c68-64d269f78bca\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"58010d00-8ecc-49f7-a5b7-ac5692356bb9\",\"type\":\"BasicTicker\"}},\"id\":\"a541e4a3-a463-48c5-9724-f93926a83ce8\",\"type\":\"Grid\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"91a511f4-835f-4099-8b26-8e691ce58b43\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"b829d982-779a-45a4-9ecd-a08496463971\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"source\":{\"id\":\"d76920c1-c6a7-451b-ac71-b34c95179e98\",\"type\":\"ColumnDataSource\"}},\"id\":\"9de679bb-3795-4c19-ad10-96e829751ee9\",\"type\":\"CDSView\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"431106ca-df11-4082-8c68-64d269f78bca\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"b829d982-779a-45a4-9ecd-a08496463971\",\"type\":\"CategoricalTicker\"}},\"id\":\"02f4fadb-29e2-4a90-b48b-15c12bac43f0\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"d98952ef-10fd-43d2-84b3-a3084bb3f039\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"5482a113-8518-4d79-a079-d47e2df477a3\",\"type\":\"HelpTool\"},{\"attributes\":{\"callback\":null,\"end\":11931.6,\"start\":0},\"id\":\"f12a73f7-fe86-43cd-b514-3848048328fe\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"2f44a546-52d7-4c93-84aa-1c8e2b26b6ce\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"0429adbb-497c-4a40-ab77-cb62dc77e510\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"f81509c8-6444-4439-b314-b875f79522f3\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null,\"factors\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]},\"id\":\"4359fc20-5287-4b02-a795-1bd698abe627\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"e19232b0-960e-4476-ac7e-f9449651c639\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"data_source\":{\"id\":\"d76920c1-c6a7-451b-ac71-b34c95179e98\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"bab0d636-7a8f-41cb-a545-bd3ba4902eca\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"0a20d73a-d01a-45b7-9191-cfb4a1d6b907\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"91a511f4-835f-4099-8b26-8e691ce58b43\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"9de679bb-3795-4c19-ad10-96e829751ee9\",\"type\":\"CDSView\"}},\"id\":\"3ae8706e-11a7-456c-b17b-804bbfc35a14\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"top\",\"x\"],\"data\":{\"top\":[9943,9672,5550,5241,5165,4325,3054,2841,2762,1811,1593,1249,1106,1071,1046,1014,942,881,832,827,793,744,741,722,712,680,658,640,602,564,526,522,521,516,511,489,485,484,479,475,469,456,440,440,432,420,414,406,394,384,374,371,350,345,335,329,323,318,317,313,296,295,292,289,288,287,285,284,282,280,277,261,259,257,256,251,249,244,241,240,236,235,230,226,225,223,222,217,213,213,210,207,205,203,203,203,202,200,197,195],\"x\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]},\"selected\":null,\"selection_policy\":null},\"id\":\"d76920c1-c6a7-451b-ac71-b34c95179e98\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"bab0d636-7a8f-41cb-a545-bd3ba4902eca\",\"type\":\"VBar\"}],\"root_ids\":[\"431106ca-df11-4082-8c68-64d269f78bca\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.15\"}};\n",
       "  var render_items = [{\"docid\":\"e40d6d9a-1374-4dc8-ad34-0f4999b1565f\",\"elementid\":\"cd71c058-d743-45ec-812e-e3fb5e67b017\",\"modelid\":\"431106ca-df11-4082-8c68-64d269f78bca\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "431106ca-df11-4082-8c68-64d269f78bca"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_X[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_X[:nplot], width=0.4, top=unigram_count_X[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_X)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_U, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"152ea671-9190-4a01-8038-1ba9ae0b796a\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"d64358bf-5a9a-494c-ae17-5512d3dbab9b\":{\"roots\":{\"references\":[{\"attributes\":{\"callback\":null,\"factors\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]},\"id\":\"fe9a78de-b3b8-4ea4-9ead-2606aadcb870\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"45f35f9e-0df6-4c6b-8835-4e75a7dc44c2\",\"type\":\"ResetTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"ee4635c4-6fee-4275-8d10-8ab5f1ac43ec\",\"type\":\"VBar\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c9a77b66-9077-48a9-8e0f-25d049102912\",\"type\":\"CategoricalTicker\"}},\"id\":\"c46d2e13-ce7b-4a1f-add8-257184c3c627\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"e7e1ba4a-cc4f-426f-af93-f31e49db453c\",\"type\":\"BoxAnnotation\"},{\"attributes\":{},\"id\":\"94a7a59e-9fe6-49a6-b465-3265d6e20f45\",\"type\":\"SaveTool\"},{\"attributes\":{\"data_source\":{\"id\":\"dc230842-e8e1-442d-b4e7-ffaf7facc4a7\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"11f41234-2d3a-4baa-9ac3-fda70f83e2de\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"511e96e8-be13-4beb-b75e-19a6c03a23f4\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"ee4635c4-6fee-4275-8d10-8ab5f1ac43ec\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"df6ac70a-10cd-44ea-8a6a-0e333856be8c\",\"type\":\"CDSView\"}},\"id\":\"6a63104d-5f85-4ad2-9284-d983c5784357\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"6a63104d-5f85-4ad2-9284-d983c5784357\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"626cc09c-94c2-43f7-a7bc-9db914ffbf0a\",\"type\":\"HoverTool\"},{\"attributes\":{\"overlay\":{\"id\":\"e7e1ba4a-cc4f-426f-af93-f31e49db453c\",\"type\":\"BoxAnnotation\"}},\"id\":\"841f4444-15a8-4a76-baad-b4e5c89ecdf0\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"b833e77a-957b-454b-bab8-b37477c15642\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"c9a77b66-9077-48a9-8e0f-25d049102912\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"formatter\":{\"id\":\"632f60c8-30d9-4845-8a44-a739eb1d14c1\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c9a77b66-9077-48a9-8e0f-25d049102912\",\"type\":\"CategoricalTicker\"}},\"id\":\"bcd25e1e-e5eb-4cb6-a462-95af91105e33\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"b9e01314-c490-400b-b06a-0e258e233af2\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"f5065e16-8269-40d7-9d88-6a0cd5a2f50d\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"top\",\"x\"],\"data\":{\"top\":[262663,175107,112277,101315,97575,86820,70380,68361,59032,46670,44546,38023,34077,29305,26325,23835,21807,20233,19947,19424,18851,17921,17222,17030,16010,15420,15128,14976,14823,14735,14392,13661,13363,13160,12929,12329,12124,11678,11527,11527,11479,10724,9889,9341,8894,8630,8517,8503,8382,8310,8236,7688,7671,7642,7580,7428,7302,6925,6912,6435,6331,6101,6051,6030,5871,5867,5847,5705,5677,5584,5583,5326,5303,5277,5077,4877,4833,4785,4757,4679,4650,4629,4591,4584,4577,4567,4464,4424,4346,4296,4242,4172,4168,4088,4060,4025,3934,3927,3924,3862],\"x\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]},\"selected\":null,\"selection_policy\":null},\"id\":\"dc230842-e8e1-442d-b4e7-ffaf7facc4a7\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"795c5152-f9b4-43d2-b1c7-f49f13479a6b\",\"type\":\"LinearScale\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"0c883f95-8d6e-45a3-b66a-00d1976ad3fb\",\"type\":\"PanTool\"},{\"id\":\"b833e77a-957b-454b-bab8-b37477c15642\",\"type\":\"WheelZoomTool\"},{\"id\":\"841f4444-15a8-4a76-baad-b4e5c89ecdf0\",\"type\":\"BoxZoomTool\"},{\"id\":\"94a7a59e-9fe6-49a6-b465-3265d6e20f45\",\"type\":\"SaveTool\"},{\"id\":\"45f35f9e-0df6-4c6b-8835-4e75a7dc44c2\",\"type\":\"ResetTool\"},{\"id\":\"85fe9540-f25b-433a-b748-cf0b683ccb1e\",\"type\":\"HelpTool\"},{\"id\":\"626cc09c-94c2-43f7-a7bc-9db914ffbf0a\",\"type\":\"HoverTool\"}]},\"id\":\"eb7cb72e-60ec-4eab-87ce-19ff7235a467\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"632f60c8-30d9-4845-8a44-a739eb1d14c1\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d8d6b030-276a-4b2d-8db1-4b3aacba6392\",\"type\":\"BasicTicker\"}},\"id\":\"715ae581-1424-4df4-975a-edab369964a7\",\"type\":\"Grid\"},{\"attributes\":{\"below\":[{\"id\":\"bcd25e1e-e5eb-4cb6-a462-95af91105e33\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"5bb3ccdb-f33d-434a-9bbc-24ecf35ab6ea\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"bcd25e1e-e5eb-4cb6-a462-95af91105e33\",\"type\":\"CategoricalAxis\"},{\"id\":\"c46d2e13-ce7b-4a1f-add8-257184c3c627\",\"type\":\"Grid\"},{\"id\":\"5bb3ccdb-f33d-434a-9bbc-24ecf35ab6ea\",\"type\":\"LinearAxis\"},{\"id\":\"715ae581-1424-4df4-975a-edab369964a7\",\"type\":\"Grid\"},{\"id\":\"e7e1ba4a-cc4f-426f-af93-f31e49db453c\",\"type\":\"BoxAnnotation\"},{\"id\":\"6a63104d-5f85-4ad2-9284-d983c5784357\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"f5065e16-8269-40d7-9d88-6a0cd5a2f50d\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"eb7cb72e-60ec-4eab-87ce-19ff7235a467\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"fe9a78de-b3b8-4ea4-9ead-2606aadcb870\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"552d3138-cdcb-4691-95a3-7d573caf6c97\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"8d8f83b3-221e-4415-9134-462f09267eb5\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"795c5152-f9b4-43d2-b1c7-f49f13479a6b\",\"type\":\"LinearScale\"}},\"id\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"0c883f95-8d6e-45a3-b66a-00d1976ad3fb\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"d8d6b030-276a-4b2d-8db1-4b3aacba6392\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"552d3138-cdcb-4691-95a3-7d573caf6c97\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"callback\":null,\"end\":315195.6,\"start\":0},\"id\":\"8d8f83b3-221e-4415-9134-462f09267eb5\",\"type\":\"DataRange1d\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"b9e01314-c490-400b-b06a-0e258e233af2\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"d8d6b030-276a-4b2d-8db1-4b3aacba6392\",\"type\":\"BasicTicker\"}},\"id\":\"5bb3ccdb-f33d-434a-9bbc-24ecf35ab6ea\",\"type\":\"LinearAxis\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"11f41234-2d3a-4baa-9ac3-fda70f83e2de\",\"type\":\"VBar\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"511e96e8-be13-4beb-b75e-19a6c03a23f4\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"dc230842-e8e1-442d-b4e7-ffaf7facc4a7\",\"type\":\"ColumnDataSource\"}},\"id\":\"df6ac70a-10cd-44ea-8a6a-0e333856be8c\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"85fe9540-f25b-433a-b748-cf0b683ccb1e\",\"type\":\"HelpTool\"}],\"root_ids\":[\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.15\"}};\n",
       "  var render_items = [{\"docid\":\"d64358bf-5a9a-494c-ae17-5512d3dbab9b\",\"elementid\":\"152ea671-9190-4a01-8038-1ba9ae0b796a\",\"modelid\":\"3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "3fe09f32-f80d-4de1-88c6-a7eff1c9a7bc"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_U[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_U[:nplot], width=0.4, top=unigram_count_U[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_U)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially we set vocabulary size equal to total number of unique tokens in the two dataset\n",
    "\n",
    "mergelist = set(unigram_list_X + unigram_list_U)\n",
    "\n",
    "V = len(mergelist)\n",
    "\n",
    "mergelist_XU = X_final + U_final\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(mergelist_XU, size=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum length of sentences\n",
    "lengths_X = map(len,X_tokens_canon)\n",
    "lengths_U = map(len,U_tokens_canon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_X = list(lengths_X)\n",
    "lengths_U = list(lengths_U)\n",
    "max_len1 = max(lengths_X)\n",
    "max_len2 = max(lengths_U)\n",
    "max_len = max(max_len1,max_len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = []\n",
    "U_ids = []\n",
    "for i in range(len(X_tokens_canon)):\n",
    "    X_ids.append(vocab.words_to_ids(X_tokens_canon[i]))\n",
    "    \n",
    "for i in range(len(U_tokens_canon)):\n",
    "    U_ids.append(vocab.words_to_ids(U_tokens_canon[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final padded sentences\n",
    "X_sent, ns = utils.pad_np_array(X_ids, max_len=max_len1, pad_id=0)\n",
    "U_sent, u_ns = utils.pad_np_array(U_ids, max_len=max_len2, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4326, 86)\n",
      "(113299, 493)\n"
     ]
    }
   ],
   "source": [
    "print(X_sent.shape)\n",
    "print(U_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ibc_database[1]\n",
    "Y_u = bias_sentences['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(Y)\n",
    "Y_u = list(Y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final labels\n",
    "Y_label = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] == 'liberal':\n",
    "        Y_label.append(0)\n",
    "    elif Y[i] == 'neutral':\n",
    "        Y_label.append(1)\n",
    "    else:\n",
    "        Y_label.append(2)\n",
    "\n",
    "Y_u_label = []\n",
    "\n",
    "for i in range(len(Y_u)):\n",
    "    if Y_u[i] == 'liberal':\n",
    "        Y_u_label.append(0)\n",
    "    elif Y_u[i] == 'neutral':\n",
    "        Y_u_label.append(1)\n",
    "    else:\n",
    "        Y_u_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_w = ibc_database['weak_label']\n",
    "\n",
    "Y_w =list(Y_w)\n",
    "Y_weak = []\n",
    "for i in range(len(Y)):\n",
    "    if Y_w[i] == 'liberal':\n",
    "        Y_weak.append(0)\n",
    "    elif Y_w[i] == 'neutral':\n",
    "        Y_weak.append(1)\n",
    "    else:\n",
    "        Y_weak.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = []\n",
    "for i in range(X_sent.shape[0]):\n",
    "    X_sent_w.append(np.append(X_sent[i], [Y_weak[i],ns[i]]))\n",
    "\n",
    "U_sent_w = []\n",
    "for i in range(U_sent.shape[0]):\n",
    "    U_sent_w.append(np.append(U_sent[i], [u_ns[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = np.reshape(X_sent_w, newshape= (X_sent.shape[0],X_sent.shape[1]+2))\n",
    "U_sent_w = np.reshape(U_sent_w, newshape= (U_sent.shape[0],U_sent.shape[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113299"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_u_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train,X_test,y_train, y_test = train_test_split(X_sent_w, Y_label, test_size=0.2, random_state=42)\n",
    "U_train,U_test,y_u_train, y_u_test = train_test_split(U_sent_w, Y_u_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_train = []\n",
    "ns_test = []\n",
    "\n",
    "ns_u_train = []\n",
    "ns_u_test = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    ns_train.append(X_train[i][-1])\n",
    "for i in range(X_test.shape[0]):\n",
    "    ns_test.append(X_test[i][-1])\n",
    "    \n",
    "for i in range(U_train.shape[0]):\n",
    "    ns_u_train.append(U_train[i][-1])\n",
    "for i in range(U_test.shape[0]):\n",
    "    ns_u_test.append(U_test[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n_train = np.delete(X_train,len(X_train[1])-1,axis=1)\n",
    "X_n_test  = np.delete(X_test,len(X_test[1])-1,axis=1)\n",
    "\n",
    "U_n_train = np.delete(U_train,len(U_train[1])-1,axis=1)\n",
    "U_n_test  = np.delete(U_test,len(U_test[1])-1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our model\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(name = 'W_embed', shape=[V,embed_dim],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_)\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        \n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation,    \n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                             , bias_initializer=tf.zeros_initializer(), name = 'Hidden_%d'%i)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.nn.dropout(h_, keep_prob=dropout_rate)\n",
    "            \n",
    "        #if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(inputs = h_, rate=dropout_rate, training = is_training)\n",
    "        \n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,is_training=None,\n",
    "                **unused_kw):\n",
    "    \n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    \n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001) \n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],dtype=tf.float32), -1)\n",
    "    xs_ =  mask_ * xs_\n",
    "    x_ = tf.reduce_sum(xs_,axis = 1)\n",
    "    h_ = fully_connected_layers(x_, hidden_dims, activation=tf.tanh, dropout_rate=dropout_rate,is_training=is_training)   \n",
    "        \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500 examples, moving-average loss 1.21\n",
      "1,000 examples, moving-average loss 1.22\n",
      "1,500 examples, moving-average loss 1.09\n",
      "2,000 examples, moving-average loss 1.14\n",
      "2,500 examples, moving-average loss 1.24\n",
      "3,000 examples, moving-average loss 1.23\n",
      "Completed no. 1 epoch in 0:00:05\n",
      "  500 examples, moving-average loss 0.79\n",
      "1,000 examples, moving-average loss 0.88\n",
      "1,500 examples, moving-average loss 0.84\n",
      "2,000 examples, moving-average loss 0.88\n",
      "2,500 examples, moving-average loss 0.97\n",
      "3,000 examples, moving-average loss 1.04\n",
      "Completed no. 2 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.60\n",
      "1,000 examples, moving-average loss 0.67\n",
      "1,500 examples, moving-average loss 0.66\n",
      "2,000 examples, moving-average loss 0.64\n",
      "2,500 examples, moving-average loss 0.70\n",
      "3,000 examples, moving-average loss 0.79\n",
      "Completed no. 3 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.46\n",
      "1,000 examples, moving-average loss 0.50\n",
      "1,500 examples, moving-average loss 0.51\n",
      "2,000 examples, moving-average loss 0.46\n",
      "2,500 examples, moving-average loss 0.51\n",
      "3,000 examples, moving-average loss 0.55\n",
      "Completed no. 4 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.38\n",
      "1,000 examples, moving-average loss 0.41\n",
      "1,500 examples, moving-average loss 0.41\n",
      "2,000 examples, moving-average loss 0.36\n",
      "2,500 examples, moving-average loss 0.42\n",
      "3,000 examples, moving-average loss 0.42\n",
      "Completed no. 5 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.33\n",
      "1,000 examples, moving-average loss 0.36\n",
      "1,500 examples, moving-average loss 0.36\n",
      "2,000 examples, moving-average loss 0.33\n",
      "2,500 examples, moving-average loss 0.38\n",
      "3,000 examples, moving-average loss 0.36\n",
      "Completed no. 6 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.31\n",
      "1,000 examples, moving-average loss 0.34\n",
      "1,500 examples, moving-average loss 0.33\n",
      "2,000 examples, moving-average loss 0.31\n",
      "2,500 examples, moving-average loss 0.35\n",
      "3,000 examples, moving-average loss 0.34\n",
      "Completed no. 7 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.30\n",
      "1,000 examples, moving-average loss 0.32\n",
      "1,500 examples, moving-average loss 0.32\n",
      "2,000 examples, moving-average loss 0.30\n",
      "2,500 examples, moving-average loss 0.34\n",
      "3,000 examples, moving-average loss 0.33\n",
      "Completed no. 8 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.30\n",
      "1,000 examples, moving-average loss 0.32\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.30\n",
      "2,500 examples, moving-average loss 0.33\n",
      "3,000 examples, moving-average loss 0.32\n",
      "Completed no. 9 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.29\n",
      "1,000 examples, moving-average loss 0.31\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.29\n",
      "2,500 examples, moving-average loss 0.32\n",
      "3,000 examples, moving-average loss 0.31\n",
      "Completed no. 10 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.29\n",
      "1,000 examples, moving-average loss 0.31\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.29\n",
      "2,500 examples, moving-average loss 0.32\n",
      "3,000 examples, moving-average loss 0.31\n",
      "Completed no. 11 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.29\n",
      "1,000 examples, moving-average loss 0.30\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.29\n",
      "2,500 examples, moving-average loss 0.31\n",
      "3,000 examples, moving-average loss 0.31\n",
      "Completed no. 12 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.29\n",
      "1,000 examples, moving-average loss 0.30\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.29\n",
      "2,500 examples, moving-average loss 0.31\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 13 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.30\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.31\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 14 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.30\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.31\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 15 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.29\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.30\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 16 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.29\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.30\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 17 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.29\n",
      "1,500 examples, moving-average loss 0.30\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.30\n",
      "3,000 examples, moving-average loss 0.29\n",
      "Completed no. 18 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.29\n",
      "1,500 examples, moving-average loss 0.29\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.30\n",
      "3,000 examples, moving-average loss 0.29\n",
      "Completed no. 19 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.28\n",
      "1,000 examples, moving-average loss 0.29\n",
      "1,500 examples, moving-average loss 0.29\n",
      "2,000 examples, moving-average loss 0.28\n",
      "2,500 examples, moving-average loss 0.30\n",
      "3,000 examples, moving-average loss 0.29\n",
      "Completed no. 20 epoch in 0:00:02\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "batch_size = 5\n",
    "e = 175 # embedding dimension\n",
    "h = [32,64,128] # hidden dimensions\n",
    "l = 0.001 # learning rate\n",
    "num_epoch = 20 #no. of epochs\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=3,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "model_fn = classifier_model_fn\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_train.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "    ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "    \n",
    "    \n",
    "    y_conf = []\n",
    "     \n",
    "    for i in range(len(y_train)):\n",
    "        y_conf.append(2 - np.absolute(Y_weak[i]-y_train[i]))\n",
    "       \n",
    "           \n",
    "    \n",
    "    y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "    ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "    \n",
    "    for j in range(num_epoch):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_batches = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        for (bx, bns, by) in utils.multi_batch_generator(batch_size, X_n_train, ns_train, y_conf):\n",
    "            # feed NumPy arrays into the placeholder Tensors\n",
    "            feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "            batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "            # Compute some statistics\n",
    "            total_batches += 1\n",
    "            total_examples += len(bx)\n",
    "            total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "            # Compute moving average to smooth out noisy per-batch loss\n",
    "            loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "            if (total_batches % 100 == 0):\n",
    "                print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "        print(\"Completed no. {:d} epoch in {:s}\".format(j+1,utils.pretty_timedelta(since=t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.device('/gpu:0'),tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_test.shape[1]])  \n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              \n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])             \n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                              params=model_params)\n",
    "    predict_ = estimator_spec.predictions\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    t = X_n_train.shape[0]\n",
    "    \n",
    "    true_conf_score = []\n",
    "    for i in range(X_n_test.shape[0]):\n",
    "        true_conf_score.append(2 - np.absolute(Y_weak[t+i]-y_test[i]))\n",
    "    \n",
    "    \n",
    "    feed_dict = {x_ph_: X_n_test, ns_ph_: ns_test, y_ph_: true_conf_score}\n",
    "    predictions = sess.run(predict_, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_conf_score.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_conf_score = predictions['max']\n",
    "len(true_conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07563666  0.03957781  0.88478547]  test_label: 2  weak_label: 2\n",
      "[ 0.07553412  0.04512538  0.87934053]  test_label: 0  weak_label: 2\n",
      "[ 0.07556286  0.04638939  0.8780477 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07301473  0.04259972  0.88438553]  test_label: 2  weak_label: 0\n",
      "[ 0.07578142  0.04424974  0.87996882]  test_label: 0  weak_label: 1\n",
      "[ 0.07572686  0.04819401  0.87607908]  test_label: 2  weak_label: 1\n",
      "[ 0.07385767  0.0433814   0.88276094]  test_label: 0  weak_label: 1\n",
      "[ 0.07525578  0.04670056  0.87804365]  test_label: 0  weak_label: 1\n",
      "[ 0.07503267  0.04763155  0.87733585]  test_label: 0  weak_label: 1\n",
      "[ 0.07923279  0.04768939  0.87307775]  test_label: 0  weak_label: 0\n",
      "[ 0.07427606  0.04396876  0.88175517]  test_label: 1  weak_label: 1\n",
      "[ 0.08033441  0.04677994  0.87288564]  test_label: 0  weak_label: 0\n",
      "[ 0.07378038  0.04369602  0.88252366]  test_label: 0  weak_label: 0\n",
      "[ 0.07650818  0.04507896  0.8784129 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07965148  0.04890124  0.87144727]  test_label: 2  weak_label: 1\n",
      "[ 0.07593791  0.04555858  0.87850356]  test_label: 2  weak_label: 1\n",
      "[ 0.07708701  0.04770173  0.8752113 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07511034  0.04618294  0.87870669]  test_label: 0  weak_label: 1\n",
      "[ 0.07619195  0.0434826   0.88032544]  test_label: 2  weak_label: 1\n",
      "[ 0.07521345  0.04667934  0.87810719]  test_label: 1  weak_label: 1\n",
      "[ 0.07380081  0.0438199   0.88237929]  test_label: 2  weak_label: 1\n",
      "[ 0.07704537  0.04782798  0.87512666]  test_label: 2  weak_label: 0\n",
      "[ 0.07798179  0.04484703  0.87717116]  test_label: 2  weak_label: 1\n",
      "[ 0.07434826  0.05446814  0.87118363]  test_label: 2  weak_label: 2\n",
      "[ 0.07653275  0.05259542  0.87087178]  test_label: 1  weak_label: 1\n",
      "[ 0.07838479  0.04531256  0.87630266]  test_label: 1  weak_label: 1\n",
      "[ 0.07582428  0.04848183  0.87569386]  test_label: 2  weak_label: 1\n",
      "[ 0.07618681  0.04314049  0.88067269]  test_label: 1  weak_label: 1\n",
      "[ 0.07296704  0.04338268  0.8836503 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07623282  0.04564482  0.87812233]  test_label: 0  weak_label: 0\n",
      "[ 0.0775095   0.04589371  0.87659681]  test_label: 2  weak_label: 2\n",
      "[ 0.07718934  0.04531144  0.87749922]  test_label: 2  weak_label: 1\n",
      "[ 0.08071346  0.04897345  0.87031305]  test_label: 1  weak_label: 1\n",
      "[ 0.07850461  0.04682874  0.87466669]  test_label: 2  weak_label: 1\n",
      "[ 0.07737211  0.05025123  0.87237662]  test_label: 2  weak_label: 1\n",
      "[ 0.07756203  0.04690722  0.87553072]  test_label: 2  weak_label: 0\n",
      "[ 0.07832342  0.04464643  0.87703013]  test_label: 2  weak_label: 1\n",
      "[ 0.07471909  0.04567376  0.8796072 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07168581  0.04335467  0.88495952]  test_label: 2  weak_label: 1\n",
      "[ 0.07377169  0.05084207  0.8753863 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07429475  0.04663448  0.87907082]  test_label: 0  weak_label: 1\n",
      "[ 0.07560062  0.04515522  0.87924415]  test_label: 0  weak_label: 1\n",
      "[ 0.07665264  0.0469866   0.87636083]  test_label: 0  weak_label: 1\n",
      "[ 0.07822241  0.04738374  0.87439388]  test_label: 1  weak_label: 1\n",
      "[ 0.07683064  0.04283706  0.88033229]  test_label: 1  weak_label: 1\n",
      "[ 0.07865337  0.0456403   0.87570632]  test_label: 2  weak_label: 1\n",
      "[ 0.07723096  0.04487301  0.87789601]  test_label: 2  weak_label: 1\n",
      "[ 0.08049609  0.05180826  0.86769563]  test_label: 2  weak_label: 1\n",
      "[ 0.07715455  0.0450474   0.87779808]  test_label: 2  weak_label: 2\n",
      "[ 0.07594117  0.04632131  0.87773752]  test_label: 2  weak_label: 2\n",
      "[ 0.07814949  0.04796102  0.87388951]  test_label: 0  weak_label: 1\n",
      "[ 0.07688937  0.04582611  0.87728453]  test_label: 0  weak_label: 1\n",
      "[ 0.07518427  0.04392783  0.88088787]  test_label: 0  weak_label: 1\n",
      "[ 0.07077997  0.04687818  0.88234192]  test_label: 0  weak_label: 2\n",
      "[ 0.0773119   0.047641    0.87504715]  test_label: 1  weak_label: 0\n",
      "[ 0.07923803  0.04765341  0.87310857]  test_label: 0  weak_label: 1\n",
      "[ 0.07893807  0.04549214  0.87556976]  test_label: 1  weak_label: 2\n",
      "[ 0.07803378  0.04511051  0.87685567]  test_label: 0  weak_label: 0\n",
      "[ 0.07434382  0.04687998  0.87877619]  test_label: 0  weak_label: 1\n",
      "[ 0.07258634  0.0414582   0.88595545]  test_label: 2  weak_label: 2\n",
      "[ 0.07870291  0.04720299  0.87409413]  test_label: 0  weak_label: 1\n",
      "[ 0.0761965   0.04442654  0.87937695]  test_label: 1  weak_label: 0\n",
      "[ 0.07566129  0.04579449  0.87854421]  test_label: 0  weak_label: 2\n",
      "[ 0.07918665  0.04571831  0.87509501]  test_label: 2  weak_label: 0\n",
      "[ 0.07902173  0.04850537  0.87247288]  test_label: 2  weak_label: 1\n",
      "[ 0.07508021  0.04849758  0.87642217]  test_label: 0  weak_label: 1\n",
      "[ 0.07643358  0.04316828  0.88039815]  test_label: 0  weak_label: 1\n",
      "[ 0.07514742  0.04664964  0.87820286]  test_label: 2  weak_label: 0\n",
      "[ 0.07950131  0.04487584  0.87562287]  test_label: 0  weak_label: 0\n",
      "[ 0.07360074  0.04364293  0.88275629]  test_label: 0  weak_label: 1\n",
      "[ 0.07574516  0.0458598   0.87839508]  test_label: 0  weak_label: 1\n",
      "[ 0.07933037  0.04839163  0.87227803]  test_label: 2  weak_label: 1\n",
      "[ 0.0732259   0.04658004  0.88019413]  test_label: 2  weak_label: 1\n",
      "[ 0.07959815  0.04885764  0.87154424]  test_label: 2  weak_label: 1\n",
      "[ 0.07070824  0.04376523  0.88552654]  test_label: 0  weak_label: 1\n",
      "[ 0.07264384  0.04048391  0.88687223]  test_label: 0  weak_label: 2\n",
      "[ 0.07524291  0.04433025  0.88042682]  test_label: 2  weak_label: 0\n",
      "[ 0.07439736  0.05127589  0.87432683]  test_label: 0  weak_label: 0\n",
      "[ 0.0751996   0.04449923  0.88030118]  test_label: 0  weak_label: 1\n",
      "[ 0.08050532  0.04849328  0.87100136]  test_label: 2  weak_label: 1\n",
      "[ 0.0779207   0.04240327  0.87967598]  test_label: 0  weak_label: 0\n",
      "[ 0.08070083  0.04914536  0.87015378]  test_label: 0  weak_label: 1\n",
      "[ 0.08055981  0.04416572  0.87527448]  test_label: 2  weak_label: 1\n",
      "[ 0.07934696  0.04797396  0.87267905]  test_label: 0  weak_label: 1\n",
      "[ 0.07616375  0.04405132  0.87978494]  test_label: 0  weak_label: 1\n",
      "[ 0.07708293  0.04579288  0.87712419]  test_label: 0  weak_label: 2\n",
      "[ 0.07590599  0.04841752  0.87567651]  test_label: 2  weak_label: 1\n",
      "[ 0.07590239  0.04818714  0.8759104 ]  test_label: 2  weak_label: 1\n",
      "[ 0.0811545   0.04548284  0.87336266]  test_label: 1  weak_label: 0\n",
      "[ 0.07687471  0.04449704  0.87862819]  test_label: 2  weak_label: 1\n",
      "[ 0.07579219  0.04766283  0.87654495]  test_label: 2  weak_label: 1\n",
      "[ 0.07628141  0.04787881  0.87583983]  test_label: 0  weak_label: 2\n",
      "[ 0.07916837  0.04729924  0.87353235]  test_label: 0  weak_label: 0\n",
      "[ 0.07535522  0.04470225  0.87994248]  test_label: 0  weak_label: 1\n",
      "[ 0.08037244  0.04342536  0.87620217]  test_label: 0  weak_label: 1\n",
      "[ 0.07398558  0.05095771  0.87505674]  test_label: 1  weak_label: 1\n",
      "[ 0.07628006  0.04599202  0.87772793]  test_label: 2  weak_label: 1\n",
      "[ 0.08019519  0.04409197  0.87571281]  test_label: 2  weak_label: 0\n",
      "[ 0.0762185   0.04746085  0.87632072]  test_label: 0  weak_label: 1\n",
      "[ 0.07772602  0.04913038  0.87314367]  test_label: 2  weak_label: 2\n",
      "[ 0.07951532  0.05080802  0.86967665]  test_label: 2  weak_label: 0\n",
      "[ 0.07664941  0.04556569  0.87778485]  test_label: 0  weak_label: 1\n",
      "[ 0.07932664  0.04369595  0.87697738]  test_label: 0  weak_label: 1\n",
      "[ 0.07905709  0.04545828  0.87548459]  test_label: 0  weak_label: 1\n",
      "[ 0.07593232  0.04604154  0.87802613]  test_label: 1  weak_label: 1\n",
      "[ 0.07666005  0.04282663  0.88051331]  test_label: 1  weak_label: 1\n",
      "[ 0.07559938  0.04852758  0.87587303]  test_label: 2  weak_label: 1\n",
      "[ 0.07651434  0.04495689  0.87852877]  test_label: 0  weak_label: 1\n",
      "[ 0.0757972   0.04691843  0.87728435]  test_label: 2  weak_label: 0\n",
      "[ 0.07156053  0.04428296  0.88415647]  test_label: 1  weak_label: 0\n",
      "[ 0.07435828  0.04347074  0.88217098]  test_label: 0  weak_label: 1\n",
      "[ 0.07973713  0.04630847  0.87395442]  test_label: 0  weak_label: 0\n",
      "[ 0.07986331  0.04903599  0.87110066]  test_label: 2  weak_label: 1\n",
      "[ 0.07071978  0.04511855  0.88416171]  test_label: 0  weak_label: 1\n",
      "[ 0.07473803  0.04796476  0.87729722]  test_label: 2  weak_label: 1\n",
      "[ 0.07940345  0.04725259  0.873344  ]  test_label: 2  weak_label: 0\n",
      "[ 0.07832664  0.04740932  0.87426406]  test_label: 0  weak_label: 1\n",
      "[ 0.07333194  0.04380573  0.88286233]  test_label: 0  weak_label: 0\n",
      "[ 0.07673517  0.0447219   0.8785429 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07832388  0.0479487   0.87372744]  test_label: 0  weak_label: 1\n",
      "[ 0.07574087  0.04679264  0.8774665 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07982177  0.04963566  0.87054253]  test_label: 0  weak_label: 1\n",
      "[ 0.0777183   0.04708946  0.87519222]  test_label: 0  weak_label: 1\n",
      "[ 0.07570887  0.04302042  0.88127071]  test_label: 0  weak_label: 1\n",
      "[ 0.07758442  0.04686154  0.87555403]  test_label: 0  weak_label: 1\n",
      "[ 0.07524172  0.04760588  0.87715238]  test_label: 0  weak_label: 1\n",
      "[ 0.07992153  0.04816539  0.87191314]  test_label: 2  weak_label: 1\n",
      "[ 0.07469664  0.04507424  0.88022912]  test_label: 2  weak_label: 2\n",
      "[ 0.07367992  0.04535058  0.88096946]  test_label: 2  weak_label: 1\n",
      "[ 0.0787964   0.04234631  0.87885731]  test_label: 0  weak_label: 1\n",
      "[ 0.07468078  0.04690244  0.87841678]  test_label: 2  weak_label: 0\n",
      "[ 0.07792531  0.04328788  0.8787868 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07529733  0.04605721  0.87864542]  test_label: 0  weak_label: 1\n",
      "[ 0.07632336  0.04750826  0.87616843]  test_label: 0  weak_label: 2\n",
      "[ 0.07594695  0.04867041  0.87538266]  test_label: 0  weak_label: 1\n",
      "[ 0.07692187  0.04722901  0.87584913]  test_label: 0  weak_label: 1\n",
      "[ 0.07368121  0.04525525  0.88106358]  test_label: 2  weak_label: 0\n",
      "[ 0.07551193  0.04689033  0.87759769]  test_label: 0  weak_label: 0\n",
      "[ 0.07877725  0.04767228  0.87355047]  test_label: 2  weak_label: 1\n",
      "[ 0.08102378  0.04879688  0.87017936]  test_label: 0  weak_label: 1\n",
      "[ 0.07800248  0.04379953  0.87819803]  test_label: 2  weak_label: 1\n",
      "[ 0.07843017  0.04709247  0.87447739]  test_label: 0  weak_label: 1\n",
      "[ 0.07497574  0.04350836  0.88151592]  test_label: 1  weak_label: 0\n",
      "[ 0.07477881  0.0453697   0.87985152]  test_label: 2  weak_label: 1\n",
      "[ 0.07755014  0.04635958  0.87609023]  test_label: 1  weak_label: 1\n",
      "[ 0.07820403  0.04666018  0.87513578]  test_label: 2  weak_label: 1\n",
      "[ 0.07360818  0.04961742  0.87677437]  test_label: 2  weak_label: 1\n",
      "[ 0.07824665  0.04549321  0.8762601 ]  test_label: 2  weak_label: 2\n",
      "[ 0.07936488  0.04801487  0.87262022]  test_label: 2  weak_label: 1\n",
      "[ 0.07620717  0.043705    0.88008779]  test_label: 2  weak_label: 1\n",
      "[ 0.07491188  0.04281605  0.88227212]  test_label: 2  weak_label: 1\n",
      "[ 0.07686074  0.04686578  0.87627345]  test_label: 0  weak_label: 2\n",
      "[ 0.06965198  0.04686629  0.88348174]  test_label: 2  weak_label: 1\n",
      "[ 0.07449792  0.04477943  0.88072258]  test_label: 2  weak_label: 1\n",
      "[ 0.07321956  0.04776992  0.8790105 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07349478  0.04371315  0.88279206]  test_label: 2  weak_label: 1\n",
      "[ 0.07624766  0.0445078   0.87924451]  test_label: 0  weak_label: 0\n",
      "[ 0.07780157  0.04886469  0.87333375]  test_label: 1  weak_label: 1\n",
      "[ 0.07379545  0.04491179  0.88129276]  test_label: 0  weak_label: 2\n",
      "[ 0.07574244  0.04517859  0.87907904]  test_label: 2  weak_label: 1\n",
      "[ 0.07471552  0.04061812  0.88466632]  test_label: 1  weak_label: 1\n",
      "[ 0.07791764  0.0457313   0.87635112]  test_label: 0  weak_label: 1\n",
      "[ 0.07758166  0.04850188  0.87391645]  test_label: 2  weak_label: 1\n",
      "[ 0.07632788  0.04193266  0.88173944]  test_label: 0  weak_label: 1\n",
      "[ 0.07593875  0.04665793  0.87740326]  test_label: 0  weak_label: 1\n",
      "[ 0.07607388  0.04682286  0.87710327]  test_label: 1  weak_label: 1\n",
      "[ 0.07515766  0.04514367  0.87969869]  test_label: 0  weak_label: 1\n",
      "[ 0.07437339  0.04816546  0.87746119]  test_label: 2  weak_label: 0\n",
      "[ 0.075663    0.0503311   0.87400591]  test_label: 1  weak_label: 1\n",
      "[ 0.07511485  0.04404179  0.8808434 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07570571  0.04636297  0.8779313 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07961755  0.04908232  0.87130016]  test_label: 0  weak_label: 1\n",
      "[ 0.078775    0.04735545  0.8738696 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07299163  0.04509993  0.88190842]  test_label: 2  weak_label: 1\n",
      "[ 0.07733578  0.04759696  0.87506723]  test_label: 2  weak_label: 1\n",
      "[ 0.07291854  0.0478148   0.87926674]  test_label: 2  weak_label: 1\n",
      "[ 0.07628784  0.04469618  0.87901598]  test_label: 0  weak_label: 1\n",
      "[ 0.07653972  0.04681853  0.87664181]  test_label: 1  weak_label: 1\n",
      "[ 0.07677412  0.04262052  0.88060534]  test_label: 2  weak_label: 1\n",
      "[ 0.07626514  0.04569603  0.87803882]  test_label: 2  weak_label: 1\n",
      "[ 0.07556357  0.04610008  0.87833643]  test_label: 0  weak_label: 1\n",
      "[ 0.07555453  0.04625687  0.87818855]  test_label: 2  weak_label: 1\n",
      "[ 0.07309596  0.04560116  0.88130295]  test_label: 2  weak_label: 1\n",
      "[ 0.07492132  0.04710295  0.87797564]  test_label: 2  weak_label: 1\n",
      "[ 0.07658951  0.04191369  0.88149679]  test_label: 2  weak_label: 0\n",
      "[ 0.07962003  0.04887675  0.87150323]  test_label: 2  weak_label: 1\n",
      "[ 0.07609776  0.04919121  0.87471098]  test_label: 2  weak_label: 1\n",
      "[ 0.07503163  0.04468615  0.88028222]  test_label: 0  weak_label: 1\n",
      "[ 0.07531776  0.04260267  0.8820796 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07805854  0.04562004  0.87632143]  test_label: 2  weak_label: 1\n",
      "[ 0.07268083  0.04404807  0.88327116]  test_label: 0  weak_label: 0\n",
      "[ 0.0755244   0.0481228   0.87635273]  test_label: 0  weak_label: 1\n",
      "[ 0.07951146  0.0482762   0.87221235]  test_label: 2  weak_label: 1\n",
      "[ 0.07335     0.04978452  0.87686545]  test_label: 2  weak_label: 0\n",
      "[ 0.0778383   0.04896358  0.87319809]  test_label: 0  weak_label: 1\n",
      "[ 0.07290383  0.04696699  0.88012916]  test_label: 2  weak_label: 1\n",
      "[ 0.07770284  0.04459861  0.8776986 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07784211  0.04533334  0.87682456]  test_label: 0  weak_label: 1\n",
      "[ 0.07857406  0.04470421  0.87672174]  test_label: 2  weak_label: 0\n",
      "[ 0.08156145  0.04648934  0.8719492 ]  test_label: 2  weak_label: 1\n",
      "[ 0.0726552   0.0414316   0.88591325]  test_label: 1  weak_label: 1\n",
      "[ 0.08100561  0.04534123  0.87365317]  test_label: 0  weak_label: 1\n",
      "[ 0.07574482  0.04751512  0.8767401 ]  test_label: 1  weak_label: 0\n",
      "[ 0.08180318  0.04370621  0.87449062]  test_label: 0  weak_label: 1\n",
      "[ 0.07690552  0.04775556  0.87533897]  test_label: 0  weak_label: 1\n",
      "[ 0.07994301  0.05024474  0.86981219]  test_label: 2  weak_label: 1\n",
      "[ 0.07839546  0.04480499  0.87679958]  test_label: 2  weak_label: 1\n",
      "[ 0.07742566  0.05382721  0.86874712]  test_label: 2  weak_label: 1\n",
      "[ 0.07683358  0.04687869  0.87628776]  test_label: 0  weak_label: 1\n",
      "[ 0.0805607   0.04736327  0.87207609]  test_label: 0  weak_label: 0\n",
      "[ 0.07980351  0.04491107  0.87528545]  test_label: 2  weak_label: 1\n",
      "[ 0.07821012  0.05017978  0.87161016]  test_label: 2  weak_label: 1\n",
      "[ 0.07884788  0.04526782  0.87588435]  test_label: 2  weak_label: 1\n",
      "[ 0.07872097  0.04567063  0.87560844]  test_label: 0  weak_label: 1\n",
      "[ 0.0741971   0.04246787  0.88333505]  test_label: 0  weak_label: 0\n",
      "[ 0.07801385  0.04618974  0.87579638]  test_label: 2  weak_label: 1\n",
      "[ 0.07863227  0.04344543  0.8779223 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07706128  0.04103501  0.88190371]  test_label: 2  weak_label: 0\n",
      "[ 0.07319292  0.04504127  0.88176578]  test_label: 0  weak_label: 1\n",
      "[ 0.07707377  0.0490705   0.87385577]  test_label: 2  weak_label: 1\n",
      "[ 0.07572206  0.04607783  0.87820011]  test_label: 0  weak_label: 1\n",
      "[ 0.07729457  0.04474243  0.87796301]  test_label: 1  weak_label: 0\n",
      "[ 0.07159089  0.04074689  0.88766223]  test_label: 0  weak_label: 1\n",
      "[ 0.07510205  0.04575882  0.87913907]  test_label: 1  weak_label: 0\n",
      "[ 0.07664665  0.04741403  0.87593931]  test_label: 2  weak_label: 2\n",
      "[ 0.07842515  0.04319168  0.87838322]  test_label: 0  weak_label: 0\n",
      "[ 0.07589844  0.04776864  0.87633288]  test_label: 1  weak_label: 1\n",
      "[ 0.07573079  0.04499172  0.87927747]  test_label: 0  weak_label: 1\n",
      "[ 0.07632457  0.05055565  0.87311977]  test_label: 0  weak_label: 1\n",
      "[ 0.07437623  0.04685057  0.87877321]  test_label: 0  weak_label: 0\n",
      "[ 0.07554956  0.04704789  0.87740254]  test_label: 0  weak_label: 1\n",
      "[ 0.07422415  0.04379034  0.88198549]  test_label: 0  weak_label: 1\n",
      "[ 0.07962714  0.04781948  0.87255341]  test_label: 1  weak_label: 1\n",
      "[ 0.08142632  0.05077869  0.86779499]  test_label: 2  weak_label: 1\n",
      "[ 0.07597385  0.04790052  0.87612557]  test_label: 0  weak_label: 1\n",
      "[ 0.07732932  0.04387823  0.87879241]  test_label: 0  weak_label: 1\n",
      "[ 0.07560095  0.04694378  0.87745529]  test_label: 0  weak_label: 1\n",
      "[ 0.07758933  0.04818332  0.87422734]  test_label: 1  weak_label: 1\n",
      "[ 0.07497847  0.04508361  0.87993789]  test_label: 0  weak_label: 0\n",
      "[ 0.07736813  0.04268293  0.87994897]  test_label: 0  weak_label: 1\n",
      "[ 0.07926555  0.04574692  0.87498754]  test_label: 0  weak_label: 1\n",
      "[ 0.07828209  0.04574655  0.87597132]  test_label: 0  weak_label: 1\n",
      "[ 0.07863622  0.04696014  0.87440366]  test_label: 0  weak_label: 1\n",
      "[ 0.08290517  0.04491816  0.87217665]  test_label: 0  weak_label: 1\n",
      "[ 0.0759918   0.04793224  0.87607598]  test_label: 0  weak_label: 1\n",
      "[ 0.07841514  0.04835761  0.8732273 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07441047  0.04238392  0.88320559]  test_label: 0  weak_label: 1\n",
      "[ 0.07542421  0.04343876  0.88113707]  test_label: 1  weak_label: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08128089  0.04423175  0.8744874 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07728848  0.04820643  0.8745051 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07692692  0.04099018  0.88208288]  test_label: 0  weak_label: 0\n",
      "[ 0.07747971  0.04490609  0.8776142 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07465222  0.04539391  0.87995386]  test_label: 2  weak_label: 0\n",
      "[ 0.07610068  0.04681165  0.87708759]  test_label: 0  weak_label: 1\n",
      "[ 0.07548527  0.0462532   0.87826157]  test_label: 0  weak_label: 1\n",
      "[ 0.0792285   0.04691809  0.87385339]  test_label: 1  weak_label: 1\n",
      "[ 0.08244811  0.04329251  0.87425941]  test_label: 1  weak_label: 1\n",
      "[ 0.07301024  0.04385043  0.88313931]  test_label: 0  weak_label: 1\n",
      "[ 0.08317853  0.04921649  0.86760497]  test_label: 2  weak_label: 1\n",
      "[ 0.07973189  0.04700594  0.87326217]  test_label: 0  weak_label: 1\n",
      "[ 0.08100265  0.04659373  0.87240368]  test_label: 0  weak_label: 1\n",
      "[ 0.0740388   0.04453143  0.88142979]  test_label: 2  weak_label: 1\n",
      "[ 0.07134442  0.04303901  0.8856166 ]  test_label: 2  weak_label: 1\n",
      "[ 0.0756584   0.04301594  0.88132572]  test_label: 2  weak_label: 1\n",
      "[ 0.07860567  0.04642254  0.87497175]  test_label: 0  weak_label: 1\n",
      "[ 0.07382485  0.04512752  0.88104767]  test_label: 0  weak_label: 1\n",
      "[ 0.0771091   0.04103441  0.8818565 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07670054  0.04599506  0.87730438]  test_label: 0  weak_label: 1\n",
      "[ 0.07462911  0.0435534   0.88181746]  test_label: 0  weak_label: 1\n",
      "[ 0.07671382  0.0457745   0.87751168]  test_label: 0  weak_label: 1\n",
      "[ 0.0770096   0.04269851  0.88029194]  test_label: 2  weak_label: 1\n",
      "[ 0.07528272  0.04853842  0.87617886]  test_label: 2  weak_label: 1\n",
      "[ 0.08044434  0.04776273  0.87179297]  test_label: 0  weak_label: 1\n",
      "[ 0.07569718  0.04338813  0.88091469]  test_label: 0  weak_label: 1\n",
      "[ 0.07846169  0.04737731  0.87416101]  test_label: 2  weak_label: 1\n",
      "[ 0.07717089  0.04602238  0.87680674]  test_label: 2  weak_label: 1\n",
      "[ 0.07695416  0.04594283  0.87710303]  test_label: 0  weak_label: 1\n",
      "[ 0.07335555  0.04489082  0.88175362]  test_label: 0  weak_label: 1\n",
      "[ 0.07473237  0.04578625  0.87948143]  test_label: 1  weak_label: 1\n",
      "[ 0.07709173  0.04797382  0.87493443]  test_label: 2  weak_label: 1\n",
      "[ 0.07350815  0.04295408  0.88353777]  test_label: 1  weak_label: 1\n",
      "[ 0.07473487  0.04191703  0.88334817]  test_label: 0  weak_label: 1\n",
      "[ 0.07821539  0.04384762  0.87793696]  test_label: 1  weak_label: 0\n",
      "[ 0.07736348  0.04788698  0.87474948]  test_label: 1  weak_label: 1\n",
      "[ 0.07638159  0.04249952  0.88111889]  test_label: 2  weak_label: 1\n",
      "[ 0.07431192  0.04324038  0.88244772]  test_label: 1  weak_label: 1\n",
      "[ 0.07477318  0.04637808  0.87884879]  test_label: 0  weak_label: 1\n",
      "[ 0.07679658  0.05002497  0.87317848]  test_label: 2  weak_label: 1\n",
      "[ 0.07745624  0.04794754  0.87459618]  test_label: 0  weak_label: 1\n",
      "[ 0.07466729  0.04505321  0.88027954]  test_label: 0  weak_label: 1\n",
      "[ 0.07436392  0.04395846  0.88167763]  test_label: 1  weak_label: 1\n",
      "[ 0.08287329  0.05002541  0.86710125]  test_label: 2  weak_label: 1\n",
      "[ 0.07624568  0.04612369  0.87763065]  test_label: 0  weak_label: 1\n",
      "[ 0.07776499  0.04801114  0.87422383]  test_label: 0  weak_label: 1\n",
      "[ 0.07446501  0.04698632  0.87854868]  test_label: 0  weak_label: 1\n",
      "[ 0.07869709  0.04483247  0.87647039]  test_label: 2  weak_label: 1\n",
      "[ 0.07669634  0.04390257  0.87940109]  test_label: 0  weak_label: 1\n",
      "[ 0.07165326  0.04284387  0.88550287]  test_label: 2  weak_label: 1\n",
      "[ 0.07654588  0.04590973  0.87754434]  test_label: 2  weak_label: 1\n",
      "[ 0.07268243  0.04622256  0.88109505]  test_label: 2  weak_label: 1\n",
      "[ 0.07774577  0.04440494  0.87784922]  test_label: 0  weak_label: 1\n",
      "[ 0.08055209  0.04571563  0.87373233]  test_label: 0  weak_label: 1\n",
      "[ 0.07428126  0.04807444  0.87764436]  test_label: 0  weak_label: 1\n",
      "[ 0.07132068  0.04247826  0.88620102]  test_label: 2  weak_label: 1\n",
      "[ 0.08053312  0.04674691  0.87272   ]  test_label: 0  weak_label: 1\n",
      "[ 0.07765024  0.04485095  0.87749881]  test_label: 0  weak_label: 0\n",
      "[ 0.0779127   0.05116576  0.87092149]  test_label: 0  weak_label: 1\n",
      "[ 0.07385167  0.04229603  0.8838523 ]  test_label: 0  weak_label: 1\n",
      "[ 0.08470917  0.04903186  0.86625904]  test_label: 2  weak_label: 1\n",
      "[ 0.07376029  0.044285    0.88195467]  test_label: 0  weak_label: 1\n",
      "[ 0.07553063  0.04637711  0.87809229]  test_label: 2  weak_label: 1\n",
      "[ 0.07513968  0.0420368   0.88282359]  test_label: 1  weak_label: 2\n",
      "[ 0.0771344   0.04458317  0.87828243]  test_label: 0  weak_label: 1\n",
      "[ 0.07583399  0.04466478  0.87950122]  test_label: 1  weak_label: 1\n",
      "[ 0.07553048  0.04575064  0.87871885]  test_label: 0  weak_label: 1\n",
      "[ 0.07594346  0.05036115  0.87369537]  test_label: 0  weak_label: 1\n",
      "[ 0.07767275  0.04606536  0.87626195]  test_label: 2  weak_label: 1\n",
      "[ 0.07993579  0.04997453  0.87008971]  test_label: 2  weak_label: 0\n",
      "[ 0.07336088  0.05011661  0.87652254]  test_label: 1  weak_label: 1\n",
      "[ 0.07546687  0.04686623  0.87766695]  test_label: 2  weak_label: 1\n",
      "[ 0.07844336  0.05090203  0.87065464]  test_label: 0  weak_label: 1\n",
      "[ 0.07162841  0.0441309   0.88424075]  test_label: 2  weak_label: 1\n",
      "[ 0.08097504  0.04467775  0.87434721]  test_label: 0  weak_label: 1\n",
      "[ 0.07473609  0.04285906  0.8824048 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07700938  0.04396338  0.87902725]  test_label: 0  weak_label: 1\n",
      "[ 0.07658542  0.04323041  0.88018423]  test_label: 2  weak_label: 1\n",
      "[ 0.07655064  0.04671404  0.87673533]  test_label: 0  weak_label: 1\n",
      "[ 0.07542462  0.04937115  0.87520427]  test_label: 2  weak_label: 1\n",
      "[ 0.07709819  0.04531149  0.87759036]  test_label: 0  weak_label: 1\n",
      "[ 0.07476196  0.04255281  0.88268524]  test_label: 0  weak_label: 1\n",
      "[ 0.07648046  0.05016246  0.87335712]  test_label: 2  weak_label: 1\n",
      "[ 0.08142281  0.05035299  0.8682242 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07565147  0.04766015  0.87668842]  test_label: 0  weak_label: 1\n",
      "[ 0.07475507  0.04639512  0.8788498 ]  test_label: 1  weak_label: 1\n",
      "[ 0.0802222   0.04492306  0.8748548 ]  test_label: 1  weak_label: 1\n",
      "[ 0.0792616   0.04642566  0.87431276]  test_label: 2  weak_label: 1\n",
      "[ 0.08136408  0.04642275  0.87221318]  test_label: 2  weak_label: 1\n",
      "[ 0.07745097  0.04157457  0.88097447]  test_label: 0  weak_label: 1\n",
      "[ 0.08309811  0.0477135   0.86918843]  test_label: 0  weak_label: 1\n",
      "[ 0.07336856  0.03976619  0.8868652 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07559844  0.04940141  0.87500012]  test_label: 0  weak_label: 1\n",
      "[ 0.08537488  0.04498463  0.86964053]  test_label: 0  weak_label: 1\n",
      "[ 0.07610592  0.04729655  0.87659752]  test_label: 1  weak_label: 1\n",
      "[ 0.07259253  0.0454658   0.88194174]  test_label: 0  weak_label: 1\n",
      "[ 0.07805829  0.05396064  0.86798102]  test_label: 0  weak_label: 1\n",
      "[ 0.07879295  0.04874717  0.87245983]  test_label: 2  weak_label: 1\n",
      "[ 0.07533043  0.04307353  0.88159609]  test_label: 0  weak_label: 1\n",
      "[ 0.07608677  0.04399681  0.87991637]  test_label: 2  weak_label: 1\n",
      "[ 0.07476322  0.04525344  0.87998331]  test_label: 0  weak_label: 1\n",
      "[ 0.07366123  0.0428939   0.88344491]  test_label: 1  weak_label: 1\n",
      "[ 0.07500675  0.0476738   0.87731951]  test_label: 0  weak_label: 1\n",
      "[ 0.07536389  0.04458658  0.88004959]  test_label: 1  weak_label: 1\n",
      "[ 0.07707936  0.04552393  0.8773967 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07650728  0.04511271  0.87838   ]  test_label: 2  weak_label: 1\n",
      "[ 0.08034733  0.04645219  0.87320048]  test_label: 0  weak_label: 1\n",
      "[ 0.07172561  0.0474863   0.88078809]  test_label: 2  weak_label: 1\n",
      "[ 0.07696751  0.04505885  0.87797368]  test_label: 1  weak_label: 1\n",
      "[ 0.07596561  0.04667689  0.87735748]  test_label: 2  weak_label: 1\n",
      "[ 0.07724435  0.046896    0.87585968]  test_label: 2  weak_label: 1\n",
      "[ 0.07658542  0.04323041  0.88018423]  test_label: 0  weak_label: 1\n",
      "[ 0.07754288  0.04636765  0.87608945]  test_label: 0  weak_label: 1\n",
      "[ 0.07993017  0.04735791  0.8727119 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07363188  0.04481704  0.88155103]  test_label: 2  weak_label: 1\n",
      "[ 0.07589662  0.04507336  0.87902999]  test_label: 1  weak_label: 1\n",
      "[ 0.0761688   0.0497208   0.87411046]  test_label: 1  weak_label: 1\n",
      "[ 0.07596273  0.04618407  0.87785321]  test_label: 0  weak_label: 1\n",
      "[ 0.07430996  0.04699974  0.87869024]  test_label: 2  weak_label: 1\n",
      "[ 0.07555296  0.0430149   0.88143224]  test_label: 2  weak_label: 1\n",
      "[ 0.07568263  0.04657833  0.87773901]  test_label: 2  weak_label: 1\n",
      "[ 0.08006006  0.04528918  0.87465072]  test_label: 0  weak_label: 1\n",
      "[ 0.07798551  0.04555121  0.87646329]  test_label: 1  weak_label: 1\n",
      "[ 0.07638688  0.0496658   0.87394732]  test_label: 2  weak_label: 0\n",
      "[ 0.07557829  0.04560534  0.87881637]  test_label: 1  weak_label: 1\n",
      "[ 0.07778689  0.04537586  0.87683725]  test_label: 0  weak_label: 0\n",
      "[ 0.07596541  0.04652231  0.87751234]  test_label: 1  weak_label: 0\n",
      "[ 0.07392277  0.04568926  0.88038802]  test_label: 2  weak_label: 1\n",
      "[ 0.07406507  0.04229401  0.88364089]  test_label: 1  weak_label: 1\n",
      "[ 0.07562661  0.04542112  0.87895226]  test_label: 2  weak_label: 1\n",
      "[ 0.07638511  0.04504166  0.87857318]  test_label: 2  weak_label: 1\n",
      "[ 0.0730283   0.04359132  0.88338035]  test_label: 0  weak_label: 1\n",
      "[ 0.07220279  0.0418242   0.88597304]  test_label: 1  weak_label: 1\n",
      "[ 0.07909147  0.04992144  0.87098706]  test_label: 0  weak_label: 1\n",
      "[ 0.08152127  0.05274791  0.86573082]  test_label: 1  weak_label: 1\n",
      "[ 0.07759345  0.04941714  0.87298936]  test_label: 2  weak_label: 1\n",
      "[ 0.07346302  0.04725818  0.87927878]  test_label: 2  weak_label: 1\n",
      "[ 0.07425936  0.04182679  0.88391387]  test_label: 2  weak_label: 1\n",
      "[ 0.07628945  0.04504794  0.87866265]  test_label: 0  weak_label: 1\n",
      "[ 0.07424383  0.04296124  0.88279492]  test_label: 0  weak_label: 1\n",
      "[ 0.07706901  0.04263665  0.88029432]  test_label: 0  weak_label: 0\n",
      "[ 0.08027973  0.05000198  0.86971831]  test_label: 0  weak_label: 1\n",
      "[ 0.07587351  0.0470088   0.87711769]  test_label: 2  weak_label: 1\n",
      "[ 0.07895463  0.04696637  0.87407899]  test_label: 0  weak_label: 1\n",
      "[ 0.07885536  0.04754686  0.87359786]  test_label: 0  weak_label: 1\n",
      "[ 0.0787039   0.04942289  0.87187326]  test_label: 0  weak_label: 1\n",
      "[ 0.07856812  0.04519551  0.87623638]  test_label: 0  weak_label: 1\n",
      "[ 0.07777737  0.04594654  0.87627602]  test_label: 0  weak_label: 1\n",
      "[ 0.07724478  0.04575669  0.8769986 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07818677  0.04508478  0.87672848]  test_label: 2  weak_label: 1\n",
      "[ 0.07670516  0.04903865  0.87425619]  test_label: 0  weak_label: 1\n",
      "[ 0.07466535  0.04895088  0.87638378]  test_label: 0  weak_label: 2\n",
      "[ 0.07743129  0.04742443  0.8751443 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07302573  0.0501861   0.8767882 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07752165  0.04743943  0.87503892]  test_label: 0  weak_label: 1\n",
      "[ 0.07933049  0.04706726  0.87360221]  test_label: 1  weak_label: 1\n",
      "[ 0.07912956  0.04600993  0.87486053]  test_label: 2  weak_label: 1\n",
      "[ 0.0818327   0.04894164  0.86922562]  test_label: 0  weak_label: 0\n",
      "[ 0.07794632  0.04496323  0.87709045]  test_label: 2  weak_label: 1\n",
      "[ 0.08033788  0.04403287  0.87562925]  test_label: 0  weak_label: 1\n",
      "[ 0.07408522  0.04566333  0.88025147]  test_label: 2  weak_label: 1\n",
      "[ 0.07420846  0.04360942  0.88218212]  test_label: 2  weak_label: 1\n",
      "[ 0.07813621  0.05348576  0.86837798]  test_label: 0  weak_label: 1\n",
      "[ 0.0766959   0.04968704  0.87361705]  test_label: 2  weak_label: 1\n",
      "[ 0.07737423  0.048161    0.87446481]  test_label: 2  weak_label: 1\n",
      "[ 0.07246106  0.04202104  0.88551784]  test_label: 1  weak_label: 1\n",
      "[ 0.07677475  0.04781096  0.87541425]  test_label: 0  weak_label: 1\n",
      "[ 0.07756026  0.04636285  0.87607688]  test_label: 1  weak_label: 1\n",
      "[ 0.07507344  0.04176385  0.88316274]  test_label: 0  weak_label: 1\n",
      "[ 0.0787888   0.04880656  0.87240458]  test_label: 0  weak_label: 1\n",
      "[ 0.07893682  0.04469821  0.87636501]  test_label: 0  weak_label: 1\n",
      "[ 0.0787234   0.04827197  0.87300462]  test_label: 0  weak_label: 1\n",
      "[ 0.07793778  0.04630672  0.87575549]  test_label: 1  weak_label: 1\n",
      "[ 0.07942665  0.05087256  0.86970073]  test_label: 1  weak_label: 1\n",
      "[ 0.0775095   0.04589371  0.87659681]  test_label: 2  weak_label: 1\n",
      "[ 0.07516851  0.04322048  0.88161099]  test_label: 0  weak_label: 1\n",
      "[ 0.07678168  0.04929617  0.87392211]  test_label: 1  weak_label: 1\n",
      "[ 0.07567411  0.0463891   0.87793678]  test_label: 1  weak_label: 0\n",
      "[ 0.0751482   0.04631812  0.87853372]  test_label: 2  weak_label: 1\n",
      "[ 0.07991444  0.04829526  0.87179035]  test_label: 2  weak_label: 1\n",
      "[ 0.07654925  0.04307118  0.88037962]  test_label: 0  weak_label: 1\n",
      "[ 0.07651909  0.04801547  0.87546539]  test_label: 2  weak_label: 1\n",
      "[ 0.08266299  0.04910589  0.86823118]  test_label: 2  weak_label: 1\n",
      "[ 0.07887941  0.0454727   0.8756479 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07556703  0.04878603  0.87564701]  test_label: 0  weak_label: 1\n",
      "[ 0.07846291  0.04173006  0.879807  ]  test_label: 0  weak_label: 2\n",
      "[ 0.07608381  0.04286056  0.88105559]  test_label: 1  weak_label: 1\n",
      "[ 0.07510582  0.05001994  0.87487423]  test_label: 1  weak_label: 1\n",
      "[ 0.07588883  0.04668384  0.87742734]  test_label: 0  weak_label: 1\n",
      "[ 0.07825448  0.04571221  0.87603325]  test_label: 0  weak_label: 1\n",
      "[ 0.07883423  0.04348983  0.87767595]  test_label: 0  weak_label: 1\n",
      "[ 0.07708726  0.0450118   0.87790096]  test_label: 0  weak_label: 1\n",
      "[ 0.07588563  0.0451933   0.87892103]  test_label: 2  weak_label: 1\n",
      "[ 0.08089174  0.05270604  0.86640215]  test_label: 2  weak_label: 1\n",
      "[ 0.07528054  0.04475207  0.87996745]  test_label: 0  weak_label: 1\n",
      "[ 0.07480855  0.04436647  0.88082498]  test_label: 0  weak_label: 1\n",
      "[ 0.07720808  0.04641747  0.87637442]  test_label: 0  weak_label: 1\n",
      "[ 0.07216089  0.04764049  0.8801986 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07892343  0.0476677   0.87340885]  test_label: 2  weak_label: 1\n",
      "[ 0.07440951  0.04539573  0.88019472]  test_label: 0  weak_label: 1\n",
      "[ 0.0781224   0.04535061  0.87652701]  test_label: 1  weak_label: 1\n",
      "[ 0.0749259   0.0454685   0.87960565]  test_label: 0  weak_label: 1\n",
      "[ 0.07186671  0.04433906  0.88379425]  test_label: 2  weak_label: 1\n",
      "[ 0.07387399  0.04187901  0.884247  ]  test_label: 2  weak_label: 1\n",
      "[ 0.07583973  0.0451973   0.87896293]  test_label: 1  weak_label: 1\n",
      "[ 0.07780815  0.04694673  0.87524509]  test_label: 2  weak_label: 1\n",
      "[ 0.07898328  0.0458708   0.87514591]  test_label: 1  weak_label: 1\n",
      "[ 0.07842231  0.05264165  0.86893606]  test_label: 0  weak_label: 1\n",
      "[ 0.0765058   0.04805742  0.87543684]  test_label: 0  weak_label: 1\n",
      "[ 0.07703096  0.04626757  0.87670153]  test_label: 0  weak_label: 1\n",
      "[ 0.07768668  0.04395188  0.8783614 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07693766  0.0451954   0.87786698]  test_label: 0  weak_label: 1\n",
      "[ 0.07802249  0.04503931  0.87693816]  test_label: 0  weak_label: 1\n",
      "[ 0.07531191  0.04251778  0.88217032]  test_label: 2  weak_label: 1\n",
      "[ 0.07810459  0.05114793  0.87074751]  test_label: 1  weak_label: 1\n",
      "[ 0.07726195  0.04846031  0.87427771]  test_label: 2  weak_label: 1\n",
      "[ 0.08100045  0.04665519  0.87234432]  test_label: 0  weak_label: 1\n",
      "[ 0.07509623  0.04512167  0.87978208]  test_label: 2  weak_label: 1\n",
      "[ 0.07699504  0.04849284  0.87451214]  test_label: 0  weak_label: 1\n",
      "[ 0.07582739  0.04848427  0.87568831]  test_label: 1  weak_label: 1\n",
      "[ 0.07767967  0.04649992  0.8758204 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07890021  0.04528423  0.87581557]  test_label: 2  weak_label: 1\n",
      "[ 0.08087335  0.04837131  0.87075537]  test_label: 2  weak_label: 1\n",
      "[ 0.07798503  0.05044334  0.8715716 ]  test_label: 1  weak_label: 1\n",
      "[ 0.08021632  0.04613795  0.87364572]  test_label: 2  weak_label: 1\n",
      "[ 0.07907872  0.04772175  0.87319952]  test_label: 0  weak_label: 1\n",
      "[ 0.07559303  0.05145853  0.87294847]  test_label: 0  weak_label: 1\n",
      "[ 0.07556479  0.04791836  0.87651682]  test_label: 0  weak_label: 1\n",
      "[ 0.07744905  0.04600162  0.87654936]  test_label: 0  weak_label: 1\n",
      "[ 0.07876815  0.04533038  0.87590146]  test_label: 2  weak_label: 1\n",
      "[ 0.07612213  0.04275556  0.88112235]  test_label: 0  weak_label: 1\n",
      "[ 0.07326287  0.04845006  0.87828714]  test_label: 0  weak_label: 1\n",
      "[ 0.07698415  0.04437201  0.87864387]  test_label: 0  weak_label: 1\n",
      "[ 0.07736354  0.0462423   0.87639415]  test_label: 0  weak_label: 1\n",
      "[ 0.07562226  0.05007821  0.87429959]  test_label: 0  weak_label: 1\n",
      "[ 0.07735433  0.0510318   0.87161386]  test_label: 1  weak_label: 1\n",
      "[ 0.07514299  0.04603606  0.87882096]  test_label: 2  weak_label: 1\n",
      "[ 0.07718423  0.04618023  0.87663555]  test_label: 1  weak_label: 1\n",
      "[ 0.08038263  0.04774899  0.87186831]  test_label: 0  weak_label: 1\n",
      "[ 0.07939029  0.04741294  0.87319684]  test_label: 2  weak_label: 1\n",
      "[ 0.07591886  0.04976326  0.87431788]  test_label: 2  weak_label: 1\n",
      "[ 0.07728963  0.04542165  0.87728876]  test_label: 0  weak_label: 1\n",
      "[ 0.07868952  0.0442422   0.87706828]  test_label: 2  weak_label: 1\n",
      "[ 0.07603227  0.04640635  0.87756133]  test_label: 1  weak_label: 1\n",
      "[ 0.07615232  0.04539266  0.87845504]  test_label: 0  weak_label: 1\n",
      "[ 0.07881119  0.0441557   0.87703311]  test_label: 2  weak_label: 1\n",
      "[ 0.07393285  0.04521779  0.8808493 ]  test_label: 0  weak_label: 1\n",
      "[ 0.073425    0.04324306  0.88333195]  test_label: 0  weak_label: 0\n",
      "[ 0.07554968  0.04626413  0.87818617]  test_label: 2  weak_label: 1\n",
      "[ 0.07711006  0.04770578  0.87518418]  test_label: 0  weak_label: 1\n",
      "[ 0.07886294  0.04437697  0.87676018]  test_label: 2  weak_label: 1\n",
      "[ 0.07932672  0.04973122  0.87094206]  test_label: 0  weak_label: 1\n",
      "[ 0.07429516  0.04115002  0.8845548 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07851012  0.04802784  0.87346202]  test_label: 0  weak_label: 1\n",
      "[ 0.07695194  0.04433892  0.87870914]  test_label: 0  weak_label: 1\n",
      "[ 0.07819914  0.04735896  0.87444186]  test_label: 0  weak_label: 1\n",
      "[ 0.07638894  0.0457219   0.8778891 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07897373  0.04455035  0.87647593]  test_label: 0  weak_label: 1\n",
      "[ 0.07644279  0.05172697  0.87183028]  test_label: 0  weak_label: 1\n",
      "[ 0.07610197  0.04609014  0.87780792]  test_label: 0  weak_label: 1\n",
      "[ 0.07657456  0.04475961  0.87866586]  test_label: 2  weak_label: 1\n",
      "[ 0.07941785  0.04493598  0.87564617]  test_label: 2  weak_label: 1\n",
      "[ 0.08505902  0.04844136  0.8664996 ]  test_label: 2  weak_label: 1\n",
      "[ 0.08016535  0.04454529  0.87528938]  test_label: 1  weak_label: 1\n",
      "[ 0.07428756  0.04638387  0.87932855]  test_label: 1  weak_label: 1\n",
      "[ 0.07391847  0.04662013  0.87946141]  test_label: 1  weak_label: 1\n",
      "[ 0.07576278  0.0490558   0.87518144]  test_label: 1  weak_label: 1\n",
      "[ 0.07414652  0.04477699  0.88107651]  test_label: 2  weak_label: 1\n",
      "[ 0.07938324  0.04502131  0.87559545]  test_label: 0  weak_label: 1\n",
      "[ 0.07967439  0.04685387  0.8734718 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07864744  0.04735123  0.87400132]  test_label: 0  weak_label: 1\n",
      "[ 0.07403141  0.04585284  0.88011569]  test_label: 0  weak_label: 1\n",
      "[ 0.0748652   0.04455242  0.88058233]  test_label: 1  weak_label: 1\n",
      "[ 0.07585092  0.04619365  0.87795544]  test_label: 2  weak_label: 1\n",
      "[ 0.07638703  0.04317874  0.88043422]  test_label: 0  weak_label: 1\n",
      "[ 0.07676033  0.04634054  0.87689912]  test_label: 1  weak_label: 1\n",
      "[ 0.08010321  0.04802434  0.87187243]  test_label: 2  weak_label: 1\n",
      "[ 0.07830685  0.04473905  0.87695408]  test_label: 0  weak_label: 1\n",
      "[ 0.07591054  0.04635646  0.87773293]  test_label: 1  weak_label: 1\n",
      "[ 0.07977685  0.04449354  0.87572962]  test_label: 2  weak_label: 1\n",
      "[ 0.07103728  0.04108702  0.88787574]  test_label: 0  weak_label: 1\n",
      "[ 0.08045163  0.04923941  0.870309  ]  test_label: 2  weak_label: 0\n",
      "[ 0.07404444  0.04742796  0.87852752]  test_label: 2  weak_label: 1\n",
      "[ 0.07920386  0.04902563  0.8717705 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07758363  0.04867047  0.87374592]  test_label: 1  weak_label: 1\n",
      "[ 0.07591639  0.04653522  0.8775484 ]  test_label: 0  weak_label: 1\n",
      "[ 0.0745752   0.04312289  0.88230193]  test_label: 0  weak_label: 1\n",
      "[ 0.07455727  0.04271558  0.88272715]  test_label: 2  weak_label: 1\n",
      "[ 0.07576635  0.04919835  0.87503529]  test_label: 0  weak_label: 1\n",
      "[ 0.07742587  0.04761283  0.87496126]  test_label: 0  weak_label: 1\n",
      "[ 0.07580533  0.04816714  0.87602746]  test_label: 2  weak_label: 1\n",
      "[ 0.07358594  0.04368458  0.88272947]  test_label: 1  weak_label: 1\n",
      "[ 0.07719932  0.0472577   0.875543  ]  test_label: 0  weak_label: 1\n",
      "[ 0.0825328   0.05106258  0.86640459]  test_label: 2  weak_label: 1\n",
      "[ 0.07768764  0.04865084  0.87366152]  test_label: 0  weak_label: 1\n",
      "[ 0.07878183  0.04805584  0.87316239]  test_label: 0  weak_label: 2\n",
      "[ 0.07660604  0.05127031  0.87212366]  test_label: 1  weak_label: 1\n",
      "[ 0.07488822  0.04568182  0.87942994]  test_label: 2  weak_label: 1\n",
      "[ 0.07135308  0.04229511  0.88635182]  test_label: 0  weak_label: 1\n",
      "[ 0.07572183  0.04650478  0.87777334]  test_label: 0  weak_label: 1\n",
      "[ 0.07552574  0.04482453  0.8796497 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07427309  0.04546953  0.88025737]  test_label: 0  weak_label: 1\n",
      "[ 0.07767367  0.04550479  0.87682158]  test_label: 0  weak_label: 0\n",
      "[ 0.07666043  0.04544196  0.87789756]  test_label: 0  weak_label: 1\n",
      "[ 0.07825565  0.04795785  0.87378651]  test_label: 2  weak_label: 1\n",
      "[ 0.07784391  0.05299954  0.86915654]  test_label: 1  weak_label: 1\n",
      "[ 0.07695783  0.04221193  0.88083023]  test_label: 2  weak_label: 1\n",
      "[ 0.07561734  0.0487688   0.87561393]  test_label: 2  weak_label: 1\n",
      "[ 0.0760342   0.04818776  0.87577802]  test_label: 2  weak_label: 1\n",
      "[ 0.07674694  0.04939373  0.87385941]  test_label: 0  weak_label: 1\n",
      "[ 0.07458911  0.04652691  0.87888396]  test_label: 2  weak_label: 1\n",
      "[ 0.07864461  0.04500438  0.876351  ]  test_label: 1  weak_label: 1\n",
      "[ 0.07096104  0.04329053  0.88574845]  test_label: 1  weak_label: 1\n",
      "[ 0.0802924   0.04599262  0.87371492]  test_label: 0  weak_label: 1\n",
      "[ 0.07724874  0.04669036  0.8760609 ]  test_label: 1  weak_label: 2\n",
      "[ 0.07531422  0.04862457  0.87606126]  test_label: 2  weak_label: 1\n",
      "[ 0.07900718  0.04868409  0.87230867]  test_label: 2  weak_label: 1\n",
      "[ 0.07570896  0.04637081  0.87792027]  test_label: 2  weak_label: 1\n",
      "[ 0.07906951  0.04788697  0.87304354]  test_label: 2  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07506081  0.0492946   0.87564462]  test_label: 1  weak_label: 1\n",
      "[ 0.0750986   0.0461377   0.87876368]  test_label: 1  weak_label: 1\n",
      "[ 0.07576087  0.04607601  0.8781631 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07734816  0.04551235  0.87713951]  test_label: 2  weak_label: 1\n",
      "[ 0.07864197  0.05053021  0.87082785]  test_label: 0  weak_label: 1\n",
      "[ 0.08006641  0.04636746  0.87356609]  test_label: 0  weak_label: 1\n",
      "[ 0.07468905  0.04515138  0.88015956]  test_label: 0  weak_label: 1\n",
      "[ 0.0807399   0.04849006  0.87076998]  test_label: 2  weak_label: 1\n",
      "[ 0.07645342  0.0447925   0.87875402]  test_label: 0  weak_label: 1\n",
      "[ 0.07454965  0.04597234  0.87947798]  test_label: 0  weak_label: 1\n",
      "[ 0.07159203  0.04195868  0.88644928]  test_label: 2  weak_label: 1\n",
      "[ 0.07789113  0.04536664  0.87674218]  test_label: 0  weak_label: 1\n",
      "[ 0.07656067  0.04524121  0.87819809]  test_label: 2  weak_label: 1\n",
      "[ 0.07752067  0.04588843  0.87659097]  test_label: 0  weak_label: 1\n",
      "[ 0.07643469  0.04455657  0.87900871]  test_label: 2  weak_label: 1\n",
      "[ 0.07759292  0.0489222   0.87348485]  test_label: 2  weak_label: 1\n",
      "[ 0.07712206  0.04635061  0.87652731]  test_label: 0  weak_label: 1\n",
      "[ 0.07613067  0.04823842  0.87563092]  test_label: 2  weak_label: 1\n",
      "[ 0.07399342  0.04103211  0.88497454]  test_label: 2  weak_label: 1\n",
      "[ 0.07491484  0.04462371  0.88046145]  test_label: 0  weak_label: 1\n",
      "[ 0.07112709  0.04601481  0.8828581 ]  test_label: 0  weak_label: 1\n",
      "[ 0.08227006  0.05083806  0.86689192]  test_label: 2  weak_label: 1\n",
      "[ 0.0771675  0.0506059  0.8722266]  test_label: 0  weak_label: 1\n",
      "[ 0.07829382  0.04906117  0.87264496]  test_label: 0  weak_label: 1\n",
      "[ 0.08036997  0.04573105  0.87389898]  test_label: 2  weak_label: 1\n",
      "[ 0.076447    0.04498175  0.87857127]  test_label: 1  weak_label: 1\n",
      "[ 0.0741801  0.0503911  0.8754288]  test_label: 2  weak_label: 0\n",
      "[ 0.07550959  0.04298511  0.88150531]  test_label: 0  weak_label: 1\n",
      "[ 0.07560071  0.04207341  0.88232583]  test_label: 0  weak_label: 1\n",
      "[ 0.07643902  0.04753926  0.87602174]  test_label: 1  weak_label: 1\n",
      "[ 0.07844589  0.04598722  0.87556684]  test_label: 0  weak_label: 1\n",
      "[ 0.08003035  0.04693701  0.87303263]  test_label: 2  weak_label: 1\n",
      "[ 0.07811256  0.04342303  0.87846446]  test_label: 0  weak_label: 1\n",
      "[ 0.07626728  0.04390609  0.87982661]  test_label: 2  weak_label: 1\n",
      "[ 0.07812059  0.04644406  0.87543535]  test_label: 2  weak_label: 1\n",
      "[ 0.07434263  0.04333843  0.88231891]  test_label: 0  weak_label: 0\n",
      "[ 0.07562265  0.04754711  0.87683016]  test_label: 2  weak_label: 1\n",
      "[ 0.07966624  0.04749933  0.8728345 ]  test_label: 0  weak_label: 1\n",
      "[ 0.0797412   0.05092977  0.86932904]  test_label: 0  weak_label: 1\n",
      "[ 0.07519418  0.04296956  0.8818363 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07666225  0.04666167  0.87667602]  test_label: 0  weak_label: 1\n",
      "[ 0.07496239  0.04828773  0.87674987]  test_label: 2  weak_label: 1\n",
      "[ 0.07527148  0.0436257   0.8811028 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07802767  0.04874715  0.87322515]  test_label: 1  weak_label: 1\n",
      "[ 0.0772357   0.05083388  0.87193042]  test_label: 2  weak_label: 1\n",
      "[ 0.07804307  0.04983793  0.87211895]  test_label: 0  weak_label: 1\n",
      "[ 0.07333759  0.04157592  0.88508648]  test_label: 2  weak_label: 1\n",
      "[ 0.07621093  0.04522604  0.87856305]  test_label: 0  weak_label: 1\n",
      "[ 0.08127678  0.04654203  0.87218118]  test_label: 2  weak_label: 1\n",
      "[ 0.07857948  0.04426299  0.87715751]  test_label: 2  weak_label: 1\n",
      "[ 0.07683505  0.04350494  0.87966007]  test_label: 1  weak_label: 1\n",
      "[ 0.07626827  0.04654092  0.87719077]  test_label: 0  weak_label: 1\n",
      "[ 0.0743826   0.04869156  0.87692589]  test_label: 0  weak_label: 1\n",
      "[ 0.07723489  0.04735617  0.87540895]  test_label: 0  weak_label: 1\n",
      "[ 0.07745759  0.04978608  0.8727563 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07934092  0.04736933  0.8732897 ]  test_label: 0  weak_label: 2\n",
      "[ 0.0757542   0.04203162  0.88221419]  test_label: 0  weak_label: 1\n",
      "[ 0.07526799  0.04392118  0.8808108 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07899962  0.04529794  0.8757025 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07943378  0.04501786  0.87554836]  test_label: 0  weak_label: 1\n",
      "[ 0.07137947  0.04270604  0.88591444]  test_label: 0  weak_label: 1\n",
      "[ 0.07541209  0.0520325   0.87255538]  test_label: 0  weak_label: 1\n",
      "[ 0.0799339   0.04850707  0.87155908]  test_label: 0  weak_label: 1\n",
      "[ 0.07678433  0.04538394  0.8778317 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07943865  0.04290732  0.87765396]  test_label: 0  weak_label: 1\n",
      "[ 0.07538864  0.04939614  0.87521523]  test_label: 2  weak_label: 1\n",
      "[ 0.07656999  0.04700752  0.87642252]  test_label: 0  weak_label: 1\n",
      "[ 0.07829742  0.04920042  0.87250215]  test_label: 0  weak_label: 1\n",
      "[ 0.07914059  0.045058    0.87580138]  test_label: 2  weak_label: 1\n",
      "[ 0.07876926  0.04576593  0.87546486]  test_label: 1  weak_label: 1\n",
      "[ 0.07621893  0.04582476  0.87795627]  test_label: 0  weak_label: 1\n",
      "[ 0.07850488  0.04568256  0.87581253]  test_label: 2  weak_label: 0\n",
      "[ 0.07450621  0.04758753  0.87790632]  test_label: 2  weak_label: 1\n",
      "[ 0.07607675  0.04518284  0.87874043]  test_label: 0  weak_label: 1\n",
      "[ 0.07535826  0.04547792  0.8791638 ]  test_label: 0  weak_label: 1\n",
      "[ 0.08079403  0.04567482  0.87353116]  test_label: 2  weak_label: 2\n",
      "[ 0.07483966  0.04523794  0.87992239]  test_label: 0  weak_label: 1\n",
      "[ 0.07729645  0.04646886  0.87623465]  test_label: 1  weak_label: 1\n",
      "[ 0.07873079  0.05012028  0.87114888]  test_label: 0  weak_label: 1\n",
      "[ 0.07422767  0.04804456  0.87772781]  test_label: 0  weak_label: 1\n",
      "[ 0.07422397  0.04332102  0.88245505]  test_label: 0  weak_label: 1\n",
      "[ 0.07220659  0.04440709  0.88338631]  test_label: 0  weak_label: 1\n",
      "[ 0.07715721  0.04661077  0.87623203]  test_label: 2  weak_label: 1\n",
      "[ 0.07695047  0.04436836  0.87868112]  test_label: 2  weak_label: 1\n",
      "[ 0.07624568  0.04612369  0.87763065]  test_label: 0  weak_label: 1\n",
      "[ 0.07581078  0.04578205  0.87840724]  test_label: 1  weak_label: 1\n",
      "[ 0.07493314  0.04685604  0.87821078]  test_label: 0  weak_label: 1\n",
      "[ 0.07530203  0.04150946  0.88318849]  test_label: 2  weak_label: 1\n",
      "[ 0.07840541  0.04490237  0.87669218]  test_label: 0  weak_label: 1\n",
      "[ 0.07353901  0.0425365   0.88392448]  test_label: 1  weak_label: 1\n",
      "[ 0.07687347  0.04631861  0.87680793]  test_label: 1  weak_label: 1\n",
      "[ 0.07641767  0.0457412   0.87784117]  test_label: 2  weak_label: 1\n",
      "[ 0.07862381  0.04754507  0.87383115]  test_label: 2  weak_label: 1\n",
      "[ 0.07700589  0.04509802  0.87789607]  test_label: 2  weak_label: 0\n",
      "[ 0.07787609  0.04596245  0.87616146]  test_label: 2  weak_label: 1\n",
      "[ 0.08053185  0.04675522  0.87271291]  test_label: 2  weak_label: 1\n",
      "[ 0.07603685  0.04906181  0.87490129]  test_label: 0  weak_label: 1\n",
      "[ 0.07472347  0.04280508  0.8824715 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07709612  0.04261431  0.88028949]  test_label: 2  weak_label: 1\n",
      "[ 0.07836314  0.04765395  0.87398297]  test_label: 0  weak_label: 1\n",
      "[ 0.07674656  0.04062883  0.88262457]  test_label: 2  weak_label: 1\n",
      "[ 0.07680713  0.04944872  0.87374419]  test_label: 0  weak_label: 1\n",
      "[ 0.07620943  0.04511525  0.87867534]  test_label: 0  weak_label: 1\n",
      "[ 0.07714596  0.04551992  0.87733412]  test_label: 2  weak_label: 1\n",
      "[ 0.07866337  0.04373793  0.8775987 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07672602  0.04760756  0.87566644]  test_label: 0  weak_label: 2\n",
      "[ 0.07524459  0.04492173  0.8798337 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07828774  0.04454554  0.87716669]  test_label: 0  weak_label: 1\n",
      "[ 0.07409567  0.04735481  0.87854952]  test_label: 0  weak_label: 1\n",
      "[ 0.08067597  0.04861647  0.87070751]  test_label: 0  weak_label: 1\n",
      "[ 0.07772882  0.04492301  0.87734812]  test_label: 0  weak_label: 1\n",
      "[ 0.07986816  0.04561227  0.87451959]  test_label: 0  weak_label: 1\n",
      "[ 0.07401644  0.04614861  0.87983495]  test_label: 2  weak_label: 1\n",
      "[ 0.07577162  0.04636964  0.8778587 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07435168  0.0441362   0.88151217]  test_label: 2  weak_label: 1\n",
      "[ 0.07615741  0.04197571  0.88186687]  test_label: 0  weak_label: 1\n",
      "[ 0.07762769  0.04289781  0.87947452]  test_label: 2  weak_label: 1\n",
      "[ 0.0765683   0.04601362  0.87741804]  test_label: 0  weak_label: 1\n",
      "[ 0.07911073  0.04634557  0.87454367]  test_label: 2  weak_label: 1\n",
      "[ 0.08080538  0.04485562  0.87433904]  test_label: 2  weak_label: 1\n",
      "[ 0.07785007  0.05180875  0.87034124]  test_label: 0  weak_label: 1\n",
      "[ 0.07262619  0.04778813  0.87958568]  test_label: 2  weak_label: 1\n",
      "[ 0.07826743  0.0465992   0.87513334]  test_label: 2  weak_label: 1\n",
      "[ 0.07891843  0.0458492   0.8752324 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07971939  0.0403657   0.87991488]  test_label: 1  weak_label: 1\n",
      "[ 0.07699707  0.04248255  0.88052034]  test_label: 0  weak_label: 1\n",
      "[ 0.07802681  0.04725828  0.87471491]  test_label: 0  weak_label: 1\n",
      "[ 0.07814605  0.04680844  0.87504548]  test_label: 2  weak_label: 1\n",
      "[ 0.07809485  0.04687892  0.87502623]  test_label: 2  weak_label: 1\n",
      "[ 0.07617464  0.04163274  0.88219261]  test_label: 1  weak_label: 1\n",
      "[ 0.0732118   0.04285025  0.8839379 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07485455  0.04799783  0.87714761]  test_label: 2  weak_label: 1\n",
      "[ 0.07483549  0.04492415  0.88024038]  test_label: 0  weak_label: 1\n",
      "[ 0.08242627  0.04638993  0.87118381]  test_label: 2  weak_label: 0\n",
      "[ 0.07443782  0.04519667  0.88036543]  test_label: 1  weak_label: 1\n",
      "[ 0.07384819  0.04314253  0.88300931]  test_label: 0  weak_label: 1\n",
      "[ 0.07762221  0.04374739  0.8786304 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07767149  0.04754137  0.87478715]  test_label: 2  weak_label: 1\n",
      "[ 0.07212807  0.04399785  0.88387406]  test_label: 2  weak_label: 1\n",
      "[ 0.07763669  0.04803446  0.87432885]  test_label: 0  weak_label: 1\n",
      "[ 0.07576013  0.0462537   0.87798613]  test_label: 0  weak_label: 1\n",
      "[ 0.0767669   0.04468234  0.87855071]  test_label: 2  weak_label: 1\n",
      "[ 0.07674596  0.04038276  0.88287121]  test_label: 0  weak_label: 1\n",
      "[ 0.07318916  0.04181491  0.88499588]  test_label: 0  weak_label: 1\n",
      "[ 0.07690703  0.05026585  0.87282711]  test_label: 1  weak_label: 1\n",
      "[ 0.07251585  0.04039732  0.88708681]  test_label: 2  weak_label: 1\n",
      "[ 0.07649831  0.04306493  0.88043678]  test_label: 0  weak_label: 2\n",
      "[ 0.07576587  0.04565265  0.87858146]  test_label: 2  weak_label: 1\n",
      "[ 0.07686864  0.04280516  0.88032621]  test_label: 0  weak_label: 1\n",
      "[ 0.07934228  0.04504719  0.87561053]  test_label: 0  weak_label: 1\n",
      "[ 0.07534461  0.04398543  0.88066989]  test_label: 1  weak_label: 1\n",
      "[ 0.07795864  0.04465517  0.87738621]  test_label: 0  weak_label: 1\n",
      "[ 0.0785425   0.04886831  0.87258917]  test_label: 0  weak_label: 0\n",
      "[ 0.07346924  0.04235494  0.8841759 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07800591  0.04966163  0.87233245]  test_label: 2  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.0773377   0.04552537  0.87713695]  test_label: 0  weak_label: 1\n",
      "[ 0.07769004  0.04935799  0.87295192]  test_label: 1  weak_label: 1\n",
      "[ 0.07939433  0.04555202  0.87505364]  test_label: 2  weak_label: 1\n",
      "[ 0.07350338  0.0439159   0.88258076]  test_label: 0  weak_label: 1\n",
      "[ 0.0771735   0.04898045  0.87384611]  test_label: 2  weak_label: 1\n",
      "[ 0.07889624  0.04381067  0.87729305]  test_label: 2  weak_label: 1\n",
      "[ 0.07063881  0.04430347  0.88505769]  test_label: 0  weak_label: 1\n",
      "[ 0.08401738  0.04866079  0.86732179]  test_label: 0  weak_label: 1\n",
      "[ 0.07520972  0.04344755  0.88134277]  test_label: 1  weak_label: 1\n",
      "[ 0.07711944  0.04394905  0.87893146]  test_label: 2  weak_label: 1\n",
      "[ 0.07867444  0.05051434  0.87081122]  test_label: 1  weak_label: 1\n",
      "[ 0.07410834  0.04599859  0.879893  ]  test_label: 0  weak_label: 1\n",
      "[ 0.0795842   0.04603009  0.87438571]  test_label: 0  weak_label: 2\n",
      "[ 0.08211089  0.04164486  0.87624425]  test_label: 2  weak_label: 1\n",
      "[ 0.07401855  0.04436091  0.88162053]  test_label: 2  weak_label: 1\n",
      "[ 0.07471105  0.04454637  0.88074255]  test_label: 1  weak_label: 1\n",
      "[ 0.07413099  0.04525004  0.88061893]  test_label: 0  weak_label: 1\n",
      "[ 0.07847963  0.04419169  0.87732869]  test_label: 0  weak_label: 1\n",
      "[ 0.07910317  0.04970731  0.87118953]  test_label: 1  weak_label: 1\n",
      "[ 0.07771758  0.04647535  0.87580705]  test_label: 0  weak_label: 1\n",
      "[ 0.08050905  0.04726727  0.87222373]  test_label: 1  weak_label: 0\n",
      "[ 0.07670978  0.0446899   0.87860036]  test_label: 0  weak_label: 1\n",
      "[ 0.07729071  0.04688822  0.87582105]  test_label: 0  weak_label: 1\n",
      "[ 0.07819692  0.04578586  0.87601727]  test_label: 2  weak_label: 1\n",
      "[ 0.07695628  0.04812931  0.87491447]  test_label: 1  weak_label: 1\n",
      "[ 0.07554291  0.04505532  0.87940174]  test_label: 0  weak_label: 0\n",
      "[ 0.07560632  0.042363    0.88203073]  test_label: 0  weak_label: 0\n",
      "[ 0.07660625  0.04372959  0.87966418]  test_label: 0  weak_label: 1\n",
      "[ 0.07484155  0.04456815  0.88059032]  test_label: 1  weak_label: 1\n",
      "[ 0.08132615  0.04786848  0.87080544]  test_label: 1  weak_label: 1\n",
      "[ 0.07969255  0.04644583  0.87386155]  test_label: 0  weak_label: 1\n",
      "[ 0.07837532  0.04561683  0.8760078 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07697146  0.04591844  0.87711006]  test_label: 2  weak_label: 1\n",
      "[ 0.07938552  0.05060153  0.870013  ]  test_label: 2  weak_label: 1\n",
      "[ 0.07713318  0.04582283  0.87704396]  test_label: 0  weak_label: 0\n",
      "[ 0.07652122  0.04610322  0.87737554]  test_label: 0  weak_label: 1\n",
      "[ 0.07625912  0.04943617  0.87430477]  test_label: 2  weak_label: 1\n",
      "[ 0.07223082  0.04296844  0.88480073]  test_label: 2  weak_label: 1\n",
      "[ 0.07618618  0.04362748  0.88018632]  test_label: 2  weak_label: 1\n",
      "[ 0.07368773  0.04709548  0.87921679]  test_label: 2  weak_label: 1\n",
      "[ 0.07663271  0.04710268  0.87626457]  test_label: 0  weak_label: 2\n",
      "[ 0.07075463  0.04507446  0.88417095]  test_label: 0  weak_label: 1\n",
      "[ 0.07311619  0.05059813  0.87628573]  test_label: 0  weak_label: 1\n",
      "[ 0.07495061  0.04669619  0.87835324]  test_label: 0  weak_label: 1\n",
      "[ 0.07795969  0.04443199  0.87760836]  test_label: 0  weak_label: 1\n",
      "[ 0.07657193  0.04442902  0.87899905]  test_label: 0  weak_label: 1\n",
      "[ 0.07751401  0.0464546   0.8760314 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07659598  0.0453935   0.87801051]  test_label: 2  weak_label: 1\n",
      "[ 0.07567349  0.04313998  0.88118649]  test_label: 0  weak_label: 1\n",
      "[ 0.07316793  0.04605478  0.88077724]  test_label: 0  weak_label: 1\n",
      "[ 0.07115001  0.04872154  0.88012844]  test_label: 2  weak_label: 1\n",
      "[ 0.08211326  0.04908605  0.8688007 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07722853  0.04709746  0.87567395]  test_label: 2  weak_label: 1\n",
      "[ 0.07537443  0.04511141  0.87951416]  test_label: 0  weak_label: 1\n",
      "[ 0.07824948  0.04533363  0.87641692]  test_label: 0  weak_label: 1\n",
      "[ 0.08070622  0.05174936  0.86754441]  test_label: 2  weak_label: 1\n",
      "[ 0.08280854  0.04847725  0.86871415]  test_label: 1  weak_label: 1\n",
      "[ 0.077       0.04173327  0.88126671]  test_label: 2  weak_label: 1\n",
      "[ 0.07584067  0.04687667  0.87728262]  test_label: 0  weak_label: 0\n",
      "[ 0.07303602  0.044375    0.88258904]  test_label: 2  weak_label: 1\n",
      "[ 0.07502847  0.04959941  0.87537211]  test_label: 0  weak_label: 0\n",
      "[ 0.07555938  0.0465566   0.87788409]  test_label: 2  weak_label: 1\n",
      "[ 0.07705536  0.04536398  0.8775807 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07784319  0.04727674  0.87488008]  test_label: 0  weak_label: 1\n",
      "[ 0.07469729  0.04210403  0.88319868]  test_label: 1  weak_label: 1\n",
      "[ 0.08112087  0.04588643  0.87299269]  test_label: 2  weak_label: 1\n",
      "[ 0.07441898  0.0462655   0.87931556]  test_label: 2  weak_label: 1\n",
      "[ 0.08160461  0.04771661  0.87067878]  test_label: 2  weak_label: 1\n",
      "[ 0.07652266  0.04484851  0.87862885]  test_label: 0  weak_label: 1\n",
      "[ 0.0753775   0.04858064  0.87604183]  test_label: 0  weak_label: 1\n",
      "[ 0.07624599  0.04671096  0.87704307]  test_label: 2  weak_label: 1\n",
      "[ 0.07831206  0.04198754  0.87970036]  test_label: 2  weak_label: 1\n",
      "[ 0.07554688  0.04401445  0.88043863]  test_label: 2  weak_label: 1\n",
      "[ 0.07545134  0.04377434  0.88077432]  test_label: 0  weak_label: 1\n",
      "[ 0.07596131  0.04190902  0.88212961]  test_label: 0  weak_label: 1\n",
      "[ 0.07655395  0.04611962  0.87732637]  test_label: 0  weak_label: 1\n",
      "[ 0.07610794  0.04392026  0.87997186]  test_label: 2  weak_label: 1\n",
      "[ 0.08026843  0.04864805  0.8710835 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07862176  0.04469511  0.87668312]  test_label: 0  weak_label: 1\n",
      "[ 0.07435037  0.04608526  0.8795644 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07716796  0.04492959  0.87790245]  test_label: 2  weak_label: 1\n",
      "[ 0.07589177  0.04662732  0.87748092]  test_label: 1  weak_label: 1\n",
      "[ 0.07765646  0.05331438  0.86902916]  test_label: 0  weak_label: 0\n",
      "[ 0.07545453  0.04528887  0.87925655]  test_label: 2  weak_label: 1\n",
      "[ 0.07557505  0.04963339  0.87479156]  test_label: 2  weak_label: 1\n",
      "[ 0.07706815  0.04823247  0.87469935]  test_label: 0  weak_label: 0\n",
      "[ 0.07527829  0.04497093  0.87975085]  test_label: 0  weak_label: 1\n",
      "[ 0.07889538  0.05073673  0.87036788]  test_label: 0  weak_label: 1\n",
      "[ 0.08022775  0.04684095  0.8729313 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07621625  0.04303076  0.88075292]  test_label: 2  weak_label: 1\n",
      "[ 0.07869524  0.04866032  0.87264442]  test_label: 2  weak_label: 1\n",
      "[ 0.07553262  0.04763812  0.87682927]  test_label: 0  weak_label: 1\n",
      "[ 0.07666367  0.04479018  0.87854612]  test_label: 2  weak_label: 1\n",
      "[ 0.08094238  0.04865164  0.87040597]  test_label: 2  weak_label: 1\n",
      "[ 0.07679243  0.04346011  0.87974739]  test_label: 0  weak_label: 0\n",
      "[ 0.07722501  0.04459025  0.8781848 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07863971  0.04621356  0.87514675]  test_label: 0  weak_label: 1\n",
      "[ 0.07768729  0.04817858  0.87413412]  test_label: 0  weak_label: 0\n",
      "[ 0.07430872  0.04824811  0.87744319]  test_label: 0  weak_label: 1\n",
      "[ 0.07876648  0.0489449   0.87228864]  test_label: 0  weak_label: 1\n",
      "[ 0.07610572  0.04780137  0.87609291]  test_label: 2  weak_label: 1\n",
      "[ 0.07756843  0.04553546  0.87689608]  test_label: 1  weak_label: 1\n",
      "[ 0.07794949  0.04474893  0.87730163]  test_label: 0  weak_label: 0\n",
      "[ 0.0792909   0.04027167  0.88043743]  test_label: 2  weak_label: 1\n",
      "[ 0.08025363  0.04907267  0.87067366]  test_label: 2  weak_label: 1\n",
      "[ 0.07363337  0.04468872  0.88167793]  test_label: 0  weak_label: 0\n",
      "[ 0.07850898  0.04563729  0.87585372]  test_label: 1  weak_label: 0\n",
      "[ 0.074526    0.04701327  0.87846076]  test_label: 2  weak_label: 1\n",
      "[ 0.07764582  0.04597548  0.87637866]  test_label: 2  weak_label: 1\n",
      "[ 0.07635336  0.04668216  0.87696445]  test_label: 2  weak_label: 1\n",
      "[ 0.08013393  0.0474963   0.87236977]  test_label: 2  weak_label: 1\n",
      "[ 0.07268952  0.0467707   0.88053972]  test_label: 2  weak_label: 1\n",
      "[ 0.07906965  0.04670263  0.8742277 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07814802  0.04390807  0.87794387]  test_label: 0  weak_label: 1\n",
      "[ 0.07822512  0.04864715  0.87312776]  test_label: 2  weak_label: 1\n",
      "[ 0.07572697  0.04056613  0.88370687]  test_label: 2  weak_label: 1\n",
      "[ 0.07468377  0.04513765  0.88017857]  test_label: 0  weak_label: 1\n",
      "[ 0.07413646  0.04618005  0.87968349]  test_label: 0  weak_label: 1\n",
      "[ 0.07562492  0.04735339  0.87702167]  test_label: 1  weak_label: 1\n",
      "[ 0.07917182  0.04902238  0.87180585]  test_label: 2  weak_label: 1\n",
      "[ 0.07553849  0.04496493  0.87949651]  test_label: 2  weak_label: 1\n",
      "[ 0.07165699  0.0481361   0.88020694]  test_label: 0  weak_label: 1\n",
      "[ 0.07898069  0.05350459  0.86751473]  test_label: 0  weak_label: 1\n",
      "[ 0.07868773  0.0446871   0.87662512]  test_label: 2  weak_label: 1\n",
      "[ 0.07715404  0.05064427  0.87220168]  test_label: 1  weak_label: 0\n",
      "[ 0.07205765  0.04543785  0.88250452]  test_label: 2  weak_label: 1\n",
      "[ 0.07656507  0.04597141  0.87746352]  test_label: 0  weak_label: 1\n",
      "[ 0.07604686  0.04848877  0.87546438]  test_label: 2  weak_label: 1\n",
      "[ 0.07519493  0.04403131  0.88077372]  test_label: 2  weak_label: 1\n",
      "[ 0.07342825  0.04933762  0.87723416]  test_label: 0  weak_label: 1\n",
      "[ 0.07553951  0.04703354  0.87742698]  test_label: 0  weak_label: 1\n",
      "[ 0.07331705  0.04571204  0.88097084]  test_label: 1  weak_label: 1\n",
      "[ 0.07678884  0.04730874  0.87590235]  test_label: 0  weak_label: 1\n",
      "[ 0.07785301  0.04404828  0.87809873]  test_label: 2  weak_label: 1\n",
      "[ 0.07125348  0.0468358   0.88191074]  test_label: 2  weak_label: 1\n",
      "[ 0.07429055  0.04318529  0.88252419]  test_label: 2  weak_label: 1\n",
      "[ 0.07432516  0.04617707  0.87949771]  test_label: 2  weak_label: 1\n",
      "[ 0.08158638  0.0470902   0.87132341]  test_label: 0  weak_label: 1\n",
      "[ 0.07846311  0.04503778  0.87649906]  test_label: 2  weak_label: 1\n",
      "[ 0.07220659  0.04440709  0.88338631]  test_label: 0  weak_label: 1\n",
      "[ 0.07166703  0.03955657  0.88877642]  test_label: 0  weak_label: 1\n",
      "[ 0.07525987  0.04850937  0.87623072]  test_label: 2  weak_label: 1\n",
      "[ 0.07593424  0.04253996  0.88152575]  test_label: 0  weak_label: 1\n",
      "[ 0.07527185  0.04678693  0.87794119]  test_label: 1  weak_label: 1\n"
     ]
    }
   ],
   "source": [
    "conf_prob = predictions['proba']\n",
    "\n",
    "t = X_n_train.shape[0]\n",
    "\n",
    "for i in range(X_n_test.shape[0]):\n",
    "    print(conf_prob[i],\" test_label:\",y_test[i],\" weak_label:\",Y_weak[t+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18591224018475749"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = [pred_conf_score == true_conf_score]\n",
    "sum(sum(num_correct))/len(pred_conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpkjugavbd\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpkjugavbd', '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=classifier_model_fn, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=5, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_train, \"ns\": ns_train}, y=y_conf,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_test = np.reshape(ns_test,newshape=(len(ns_test),))\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_test, \"ns\": ns_test}, y=true_conf_score,\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.62941\n",
      "INFO:tensorflow:global_step/sec: 270.605\n",
      "INFO:tensorflow:step = 101, loss = 1.02966 (0.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.073\n",
      "INFO:tensorflow:step = 201, loss = 1.38245 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.566\n",
      "INFO:tensorflow:step = 301, loss = 1.02012 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.692\n",
      "INFO:tensorflow:step = 401, loss = 0.861883 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.556\n",
      "INFO:tensorflow:step = 501, loss = 0.857907 (0.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.533\n",
      "INFO:tensorflow:step = 601, loss = 0.845381 (0.338 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 692 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.5416.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-692\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:34\n",
      "INFO:tensorflow:Saving dict for global step 692: accuracy = 0.756351, cross_entropy_loss = 0.715738, global_step = 692, loss = 0.998516\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-692\n",
      "INFO:tensorflow:Saving checkpoints for 693 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 693, loss = 1.13666\n",
      "INFO:tensorflow:global_step/sec: 274.858\n",
      "INFO:tensorflow:step = 793, loss = 0.641486 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.165\n",
      "INFO:tensorflow:step = 893, loss = 1.02681 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.976\n",
      "INFO:tensorflow:step = 993, loss = 0.927427 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.854\n",
      "INFO:tensorflow:step = 1093, loss = 0.737368 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.612\n",
      "INFO:tensorflow:step = 1193, loss = 0.681536 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 283.496\n",
      "INFO:tensorflow:step = 1293, loss = 0.757045 (0.353 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1384 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.01275.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-1384\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:39\n",
      "INFO:tensorflow:Saving dict for global step 1384: accuracy = 0.715935, cross_entropy_loss = 0.756684, global_step = 1384, loss = 1.0392\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-1384\n",
      "INFO:tensorflow:Saving checkpoints for 1385 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 1385, loss = 0.820478\n",
      "INFO:tensorflow:global_step/sec: 279.466\n",
      "INFO:tensorflow:step = 1485, loss = 0.471141 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.071\n",
      "INFO:tensorflow:step = 1585, loss = 0.832396 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.504\n",
      "INFO:tensorflow:step = 1685, loss = 0.789039 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.679\n",
      "INFO:tensorflow:step = 1785, loss = 0.53577 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.966\n",
      "INFO:tensorflow:step = 1885, loss = 0.513356 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.337\n",
      "INFO:tensorflow:step = 1985, loss = 0.577731 (0.345 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2076 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.545761.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-2076\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:43\n",
      "INFO:tensorflow:Saving dict for global step 2076: accuracy = 0.682448, cross_entropy_loss = 0.859716, global_step = 2076, loss = 1.14231\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-2076\n",
      "INFO:tensorflow:Saving checkpoints for 2077 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 2077, loss = 0.550478\n",
      "INFO:tensorflow:global_step/sec: 282.149\n",
      "INFO:tensorflow:step = 2177, loss = 0.352839 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.012\n",
      "INFO:tensorflow:step = 2277, loss = 0.673996 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.516\n",
      "INFO:tensorflow:step = 2377, loss = 0.592835 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.66\n",
      "INFO:tensorflow:step = 2477, loss = 0.370235 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.64\n",
      "INFO:tensorflow:step = 2577, loss = 0.421749 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 304.357\n",
      "INFO:tensorflow:step = 2677, loss = 0.403687 (0.329 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2768 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.376362.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-2768\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:47\n",
      "INFO:tensorflow:Saving dict for global step 2768: accuracy = 0.672055, cross_entropy_loss = 0.985636, global_step = 2768, loss = 1.26818\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-2768\n",
      "INFO:tensorflow:Saving checkpoints for 2769 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 2769, loss = 0.388858\n",
      "INFO:tensorflow:global_step/sec: 282.356\n",
      "INFO:tensorflow:step = 2869, loss = 0.3171 (0.356 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.513\n",
      "INFO:tensorflow:step = 2969, loss = 0.480813 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.419\n",
      "INFO:tensorflow:step = 3069, loss = 0.427093 (0.338 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.688\n",
      "INFO:tensorflow:step = 3169, loss = 0.315593 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.44\n",
      "INFO:tensorflow:step = 3269, loss = 0.35357 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.958\n",
      "INFO:tensorflow:step = 3369, loss = 0.328711 (0.332 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3460 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.336627.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-3460\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:51\n",
      "INFO:tensorflow:Saving dict for global step 3460: accuracy = 0.661663, cross_entropy_loss = 1.0994, global_step = 3460, loss = 1.38168\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-3460\n",
      "INFO:tensorflow:Saving checkpoints for 3461 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 3461, loss = 0.331873\n",
      "INFO:tensorflow:global_step/sec: 282.597\n",
      "INFO:tensorflow:step = 3561, loss = 0.302665 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.182\n",
      "INFO:tensorflow:step = 3661, loss = 0.380712 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.926\n",
      "INFO:tensorflow:step = 3761, loss = 0.354319 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.182\n",
      "INFO:tensorflow:step = 3861, loss = 0.299416 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.918\n",
      "INFO:tensorflow:step = 3961, loss = 0.320196 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.621\n",
      "INFO:tensorflow:step = 4061, loss = 0.307794 (0.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4152 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.318446.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:32:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-4152\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:32:55\n",
      "INFO:tensorflow:Saving dict for global step 4152: accuracy = 0.650115, cross_entropy_loss = 1.18731, global_step = 4152, loss = 1.46912\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-4152\n",
      "INFO:tensorflow:Saving checkpoints for 4153 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 4153, loss = 0.312133\n",
      "INFO:tensorflow:global_step/sec: 270.845\n",
      "INFO:tensorflow:step = 4253, loss = 0.294422 (0.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.622\n",
      "INFO:tensorflow:step = 4353, loss = 0.339645 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.852\n",
      "INFO:tensorflow:step = 4453, loss = 0.327314 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.216\n",
      "INFO:tensorflow:step = 4553, loss = 0.292463 (0.341 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 293.733\n",
      "INFO:tensorflow:step = 4653, loss = 0.3061 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.143\n",
      "INFO:tensorflow:step = 4753, loss = 0.29863 (0.338 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4844 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.308115.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-4844\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:00\n",
      "INFO:tensorflow:Saving dict for global step 4844: accuracy = 0.640878, cross_entropy_loss = 1.2532, global_step = 4844, loss = 1.53444\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-4844\n",
      "INFO:tensorflow:Saving checkpoints for 4845 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 4845, loss = 0.302555\n",
      "INFO:tensorflow:global_step/sec: 288.691\n",
      "INFO:tensorflow:step = 4945, loss = 0.289585 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 301.002\n",
      "INFO:tensorflow:step = 5045, loss = 0.320955 (0.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.665\n",
      "INFO:tensorflow:step = 5145, loss = 0.314705 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.033\n",
      "INFO:tensorflow:step = 5245, loss = 0.288671 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 295.013\n",
      "INFO:tensorflow:step = 5345, loss = 0.298721 (0.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.599\n",
      "INFO:tensorflow:step = 5445, loss = 0.293504 (0.337 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5536 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.301895.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:04\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-5536\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:04\n",
      "INFO:tensorflow:Saving dict for global step 5536: accuracy = 0.639723, cross_entropy_loss = 1.30392, global_step = 5536, loss = 1.58453\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-5536\n",
      "INFO:tensorflow:Saving checkpoints for 5537 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 5537, loss = 0.29696\n",
      "INFO:tensorflow:global_step/sec: 282.479\n",
      "INFO:tensorflow:step = 5637, loss = 0.286468 (0.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.601\n",
      "INFO:tensorflow:step = 5737, loss = 0.310969 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.823\n",
      "INFO:tensorflow:step = 5837, loss = 0.307162 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.35\n",
      "INFO:tensorflow:step = 5937, loss = 0.286241 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.063\n",
      "INFO:tensorflow:step = 6037, loss = 0.294196 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.374\n",
      "INFO:tensorflow:step = 6137, loss = 0.290178 (0.334 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6228 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.297593.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-6228\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:08\n",
      "INFO:tensorflow:Saving dict for global step 6228: accuracy = 0.635104, cross_entropy_loss = 1.34431, global_step = 6228, loss = 1.62424\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-6228\n",
      "INFO:tensorflow:Saving checkpoints for 6229 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 6229, loss = 0.293216\n",
      "INFO:tensorflow:global_step/sec: 268.44\n",
      "INFO:tensorflow:step = 6329, loss = 0.284269 (0.374 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.562\n",
      "INFO:tensorflow:step = 6429, loss = 0.304834 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.128\n",
      "INFO:tensorflow:step = 6529, loss = 0.301812 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.036\n",
      "INFO:tensorflow:step = 6629, loss = 0.284456 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.64\n",
      "INFO:tensorflow:step = 6729, loss = 0.291053 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 288.74\n",
      "INFO:tensorflow:step = 6829, loss = 0.28776 (0.346 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6920 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.294193.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:12\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-6920\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:12\n",
      "INFO:tensorflow:Saving dict for global step 6920: accuracy = 0.635104, cross_entropy_loss = 1.3772, global_step = 6920, loss = 1.65643\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-6920\n",
      "INFO:tensorflow:Saving checkpoints for 6921 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 6921, loss = 0.290422\n",
      "INFO:tensorflow:global_step/sec: 274.834\n",
      "INFO:tensorflow:step = 7021, loss = 0.282602 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.081\n",
      "INFO:tensorflow:step = 7121, loss = 0.300557 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.904\n",
      "INFO:tensorflow:step = 7221, loss = 0.297745 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.466\n",
      "INFO:tensorflow:step = 7321, loss = 0.283016 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.197\n",
      "INFO:tensorflow:step = 7421, loss = 0.288659 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.823\n",
      "INFO:tensorflow:step = 7521, loss = 0.285854 (0.345 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7612 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.291317.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-7612\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:17\n",
      "INFO:tensorflow:Saving dict for global step 7612: accuracy = 0.63164, cross_entropy_loss = 1.40442, global_step = 7612, loss = 1.68294\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-7612\n",
      "INFO:tensorflow:Saving checkpoints for 7613 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 7613, loss = 0.28818\n",
      "INFO:tensorflow:global_step/sec: 275.581\n",
      "INFO:tensorflow:step = 7713, loss = 0.281252 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.389\n",
      "INFO:tensorflow:step = 7813, loss = 0.297251 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.047\n",
      "INFO:tensorflow:step = 7913, loss = 0.294501 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 286.665\n",
      "INFO:tensorflow:step = 8013, loss = 0.281786 (0.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.033\n",
      "INFO:tensorflow:step = 8113, loss = 0.286729 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.399\n",
      "INFO:tensorflow:step = 8213, loss = 0.284257 (0.340 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8304 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.288807.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-8304\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:21\n",
      "INFO:tensorflow:Saving dict for global step 8304: accuracy = 0.63164, cross_entropy_loss = 1.42744, global_step = 8304, loss = 1.70525\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-8304\n",
      "INFO:tensorflow:Saving checkpoints for 8305 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 8305, loss = 0.286292\n",
      "INFO:tensorflow:global_step/sec: 276.005\n",
      "INFO:tensorflow:step = 8405, loss = 0.280093 (0.364 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.172\n",
      "INFO:tensorflow:step = 8505, loss = 0.294514 (0.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.126\n",
      "INFO:tensorflow:step = 8605, loss = 0.291778 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.298\n",
      "INFO:tensorflow:step = 8705, loss = 0.280689 (0.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.766\n",
      "INFO:tensorflow:step = 8805, loss = 0.285105 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.943\n",
      "INFO:tensorflow:step = 8905, loss = 0.282862 (0.339 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8996 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28659.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-8996\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:26\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 8996: accuracy = 0.635104, cross_entropy_loss = 1.44721, global_step = 8996, loss = 1.72429\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-8996\n",
      "INFO:tensorflow:Saving checkpoints for 8997 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 8997, loss = 0.284645\n",
      "INFO:tensorflow:global_step/sec: 286.299\n",
      "INFO:tensorflow:step = 9097, loss = 0.279055 (0.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.956\n",
      "INFO:tensorflow:step = 9197, loss = 0.29215 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 302.575\n",
      "INFO:tensorflow:step = 9297, loss = 0.289483 (0.330 sec)\n",
      "INFO:tensorflow:global_step/sec: 305.642\n",
      "INFO:tensorflow:step = 9397, loss = 0.279636 (0.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 307.739\n",
      "INFO:tensorflow:step = 9497, loss = 0.283694 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 313.278\n",
      "INFO:tensorflow:step = 9597, loss = 0.281607 (0.319 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9688 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284747.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-9688\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:30\n",
      "INFO:tensorflow:Saving dict for global step 9688: accuracy = 0.636259, cross_entropy_loss = 1.464, global_step = 9688, loss = 1.74037\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-9688\n",
      "INFO:tensorflow:Saving checkpoints for 9689 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 9689, loss = 0.283183\n",
      "INFO:tensorflow:global_step/sec: 279.383\n",
      "INFO:tensorflow:step = 9789, loss = 0.278103 (0.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.81\n",
      "INFO:tensorflow:step = 9889, loss = 0.29013 (0.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.249\n",
      "INFO:tensorflow:step = 9989, loss = 0.287634 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 298.008\n",
      "INFO:tensorflow:step = 10089, loss = 0.27864 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 292.093\n",
      "INFO:tensorflow:step = 10189, loss = 0.282405 (0.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.396\n",
      "INFO:tensorflow:step = 10289, loss = 0.280457 (0.346 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10380 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283149.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-10380\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:34\n",
      "INFO:tensorflow:Saving dict for global step 10380: accuracy = 0.636259, cross_entropy_loss = 1.47874, global_step = 10380, loss = 1.75438\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-10380\n",
      "INFO:tensorflow:Saving checkpoints for 10381 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 10381, loss = 0.281863\n",
      "INFO:tensorflow:global_step/sec: 274.79\n",
      "INFO:tensorflow:step = 10481, loss = 0.277205 (0.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.544\n",
      "INFO:tensorflow:step = 10581, loss = 0.288349 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.243\n",
      "INFO:tensorflow:step = 10681, loss = 0.286015 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.418\n",
      "INFO:tensorflow:step = 10781, loss = 0.2777 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.602\n",
      "INFO:tensorflow:step = 10881, loss = 0.281207 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.766\n",
      "INFO:tensorflow:step = 10981, loss = 0.279384 (0.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11072 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281713.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:38\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-11072\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:38\n",
      "INFO:tensorflow:Saving dict for global step 11072: accuracy = 0.638568, cross_entropy_loss = 1.49194, global_step = 11072, loss = 1.76687\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-11072\n",
      "INFO:tensorflow:Saving checkpoints for 11073 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 11073, loss = 0.280649\n",
      "INFO:tensorflow:global_step/sec: 281.339\n",
      "INFO:tensorflow:step = 11173, loss = 0.276346 (0.357 sec)\n",
      "INFO:tensorflow:global_step/sec: 299.659\n",
      "INFO:tensorflow:step = 11273, loss = 0.286733 (0.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.44\n",
      "INFO:tensorflow:step = 11373, loss = 0.284545 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.259\n",
      "INFO:tensorflow:step = 11473, loss = 0.276799 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 294.203\n",
      "INFO:tensorflow:step = 11573, loss = 0.280082 (0.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 300.12\n",
      "INFO:tensorflow:step = 11673, loss = 0.278372 (0.333 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11764 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280404.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-11764\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:43\n",
      "INFO:tensorflow:Saving dict for global step 11764: accuracy = 0.639723, cross_entropy_loss = 1.5039, global_step = 11764, loss = 1.77811\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-11764\n",
      "INFO:tensorflow:Saving checkpoints for 11765 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 11765, loss = 0.279518\n",
      "INFO:tensorflow:global_step/sec: 290.125\n",
      "INFO:tensorflow:step = 11865, loss = 0.275517 (0.346 sec)\n",
      "INFO:tensorflow:global_step/sec: 308.086\n",
      "INFO:tensorflow:step = 11965, loss = 0.285249 (0.325 sec)\n",
      "INFO:tensorflow:global_step/sec: 310.67\n",
      "INFO:tensorflow:step = 12065, loss = 0.28319 (0.322 sec)\n",
      "INFO:tensorflow:global_step/sec: 309.571\n",
      "INFO:tensorflow:step = 12165, loss = 0.275932 (0.324 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.126\n",
      "INFO:tensorflow:step = 12265, loss = 0.279018 (0.336 sec)\n",
      "INFO:tensorflow:global_step/sec: 297.787\n",
      "INFO:tensorflow:step = 12365, loss = 0.277408 (0.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12456 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.279196.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-12456\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:46\n",
      "INFO:tensorflow:Saving dict for global step 12456: accuracy = 0.642032, cross_entropy_loss = 1.51482, global_step = 12456, loss = 1.78833\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-12456\n",
      "INFO:tensorflow:Saving checkpoints for 12457 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 12457, loss = 0.278453\n",
      "INFO:tensorflow:global_step/sec: 278.373\n",
      "INFO:tensorflow:step = 12557, loss = 0.274711 (0.360 sec)\n",
      "INFO:tensorflow:global_step/sec: 296.507\n",
      "INFO:tensorflow:step = 12657, loss = 0.28387 (0.337 sec)\n",
      "INFO:tensorflow:global_step/sec: 293.262\n",
      "INFO:tensorflow:step = 12757, loss = 0.281925 (0.341 sec)\n",
      "INFO:tensorflow:global_step/sec: 291.877\n",
      "INFO:tensorflow:step = 12857, loss = 0.275091 (0.343 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.698\n",
      "INFO:tensorflow:step = 12957, loss = 0.278003 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 290.593\n",
      "INFO:tensorflow:step = 13057, loss = 0.276484 (0.344 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13148 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278068.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13148\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:51\n",
      "INFO:tensorflow:Saving dict for global step 13148: accuracy = 0.642032, cross_entropy_loss = 1.52487, global_step = 13148, loss = 1.79767\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13148\n",
      "INFO:tensorflow:Saving checkpoints for 13149 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:step = 13149, loss = 0.277441\n",
      "INFO:tensorflow:global_step/sec: 273.054\n",
      "INFO:tensorflow:step = 13249, loss = 0.273925 (0.368 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 288.401\n",
      "INFO:tensorflow:step = 13349, loss = 0.282579 (0.347 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.616\n",
      "INFO:tensorflow:step = 13449, loss = 0.280735 (0.345 sec)\n",
      "INFO:tensorflow:global_step/sec: 287.688\n",
      "INFO:tensorflow:step = 13549, loss = 0.274273 (0.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 284.641\n",
      "INFO:tensorflow:step = 13649, loss = 0.27703 (0.352 sec)\n",
      "INFO:tensorflow:global_step/sec: 289.482\n",
      "INFO:tensorflow:step = 13749, loss = 0.275591 (0.345 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13840 into /tmp/tmpkjugavbd/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.277007.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:33:54\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13840\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:33:55\n",
      "INFO:tensorflow:Saving dict for global step 13840: accuracy = 0.642032, cross_entropy_loss = 1.53416, global_step = 13840, loss = 1.80626\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:34:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13840\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:34:08\n",
      "INFO:tensorflow:Saving dict for global step 13840: accuracy = 0.642032, cross_entropy_loss = 1.53416, global_step = 13840, loss = 1.80626\n",
      "Accuracy on test set: 64.20%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.64203233,\n",
       " 'cross_entropy_loss': 1.5341592,\n",
       " 'global_step': 13840,\n",
       " 'loss': 1.8062626}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer_conf(h_, labels_, num_classes, conf):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        #c = tf.constant(conf,dtype = tf.float32)\n",
    "        loss_ = tf.reduce_mean(tf.cast(conf,dtype = tf.float32) * tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn_conf(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer_conf(h_, labels, params['num_classes'],conf=features['conf'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "        \n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_conf_score = []\n",
    "for i in range(U_n_train.shape[0]):\n",
    "    true_conf_score.append(2)\n",
    "    \n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train},\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_train = model.predict(input_fn=train_input_fn)  \n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test},\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_test = model.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13840\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpkjugavbd/model.ckpt-13840\n"
     ]
    }
   ],
   "source": [
    "predictions_train =list(predictions_train)\n",
    "predictions_test =list(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u_train = []\n",
    "\n",
    "for i in range(len(predictions_train)):\n",
    "    conf_score_u_train.append(predictions_train[i]['max'])\n",
    "    \n",
    "conf_score_u_test = []\n",
    "\n",
    "for i in range(len(predictions_test)):\n",
    "    conf_score_u_test.append(predictions_test[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpqkbx6lwt\n",
      "INFO:tensorflow:Using config: {'_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_model_dir': '/tmp/tmpqkbx6lwt', '_keep_checkpoint_max': 5, '_tf_random_seed': 1, '_keep_checkpoint_every_n_hours': 10000, '_session_config': None, '_save_summary_steps': 100}\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=3,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "\n",
    "\n",
    "model1 = tf.estimator.Estimator(model_fn=classifier_model_fn_conf, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the U dataset\n",
    "train_params = dict(batch_size=250, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "y_u_train = np.reshape(y_u_train,newshape=(len(y_u_train),))\n",
    "conf_score_u_train = np.reshape(conf_score_u_train,newshape=(U_n_train.shape[0],))\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train,\"conf\": conf_score_u_train}, y=y_u_train,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "y_u_test = np.reshape(y_u_test,newshape=(len(y_u_test),))\n",
    "conf_score_u_test = np.reshape(conf_score_u_test,newshape=(U_n_test.shape[0],))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test,\"conf\": conf_score_u_test}, y=y_u_test,\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 1, loss = 1.71607\n",
      "INFO:tensorflow:global_step/sec: 11.6391\n",
      "INFO:tensorflow:step = 101, loss = 0.621098 (8.593 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6516\n",
      "INFO:tensorflow:step = 201, loss = 0.511374 (8.582 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6336\n",
      "INFO:tensorflow:step = 301, loss = 0.52165 (8.596 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 363 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.481827.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:34:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-363\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:34:58\n",
      "INFO:tensorflow:Saving dict for global step 363: accuracy = 0.952251, cross_entropy_loss = 0.202962, global_step = 363, loss = 0.492254\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-363\n",
      "INFO:tensorflow:Saving checkpoints for 364 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 364, loss = 0.656251\n",
      "INFO:tensorflow:global_step/sec: 11.9356\n",
      "INFO:tensorflow:step = 464, loss = 0.467591 (8.380 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8997\n",
      "INFO:tensorflow:step = 564, loss = 0.434366 (8.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8401\n",
      "INFO:tensorflow:step = 664, loss = 0.447888 (8.446 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 726 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.425247.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:35:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-726\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:35:33\n",
      "INFO:tensorflow:Saving dict for global step 726: accuracy = 0.96143, cross_entropy_loss = 0.151217, global_step = 726, loss = 0.440694\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-726\n",
      "INFO:tensorflow:Saving checkpoints for 727 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 727, loss = 0.582618\n",
      "INFO:tensorflow:global_step/sec: 11.8894\n",
      "INFO:tensorflow:step = 827, loss = 0.4106 (8.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3431\n",
      "INFO:tensorflow:step = 927, loss = 0.396195 (8.816 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.556\n",
      "INFO:tensorflow:step = 1027, loss = 0.404667 (8.654 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1089 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.392462.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:36:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1089\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:36:10\n",
      "INFO:tensorflow:Saving dict for global step 1089: accuracy = 0.966726, cross_entropy_loss = 0.123642, global_step = 1089, loss = 0.412997\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1089\n",
      "INFO:tensorflow:Saving checkpoints for 1090 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 1090, loss = 0.533962\n",
      "INFO:tensorflow:global_step/sec: 11.7204\n",
      "INFO:tensorflow:step = 1190, loss = 0.378239 (8.534 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.9056\n",
      "INFO:tensorflow:step = 1290, loss = 0.371554 (8.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4958\n",
      "INFO:tensorflow:step = 1390, loss = 0.375643 (8.699 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1452 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.369334.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:36:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1452\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:36:46\n",
      "INFO:tensorflow:Saving dict for global step 1452: accuracy = 0.971492, cross_entropy_loss = 0.106587, global_step = 1452, loss = 0.395715\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1452\n",
      "INFO:tensorflow:Saving checkpoints for 1453 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 1453, loss = 0.500065\n",
      "INFO:tensorflow:global_step/sec: 11.6137\n",
      "INFO:tensorflow:step = 1553, loss = 0.357997 (8.612 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6479\n",
      "INFO:tensorflow:step = 1653, loss = 0.353396 (8.585 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3996\n",
      "INFO:tensorflow:step = 1753, loss = 0.355988 (8.772 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1815 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.352175.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:37:19\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1815\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:37:22\n",
      "INFO:tensorflow:Saving dict for global step 1815: accuracy = 0.974757, cross_entropy_loss = 0.095185, global_step = 1815, loss = 0.383995\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-1815\n",
      "INFO:tensorflow:Saving checkpoints for 1816 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 1816, loss = 0.475789\n",
      "INFO:tensorflow:global_step/sec: 11.6695\n",
      "INFO:tensorflow:step = 1916, loss = 0.344618 (8.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5529\n",
      "INFO:tensorflow:step = 2016, loss = 0.33957 (8.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.461\n",
      "INFO:tensorflow:step = 2116, loss = 0.342331 (8.726 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2178 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.339393.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:37:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2178\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:37:58\n",
      "INFO:tensorflow:Saving dict for global step 2178: accuracy = 0.97767, cross_entropy_loss = 0.0872457, global_step = 2178, loss = 0.375671\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2178\n",
      "INFO:tensorflow:Saving checkpoints for 2179 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 2179, loss = 0.457095\n",
      "INFO:tensorflow:global_step/sec: 11.7997\n",
      "INFO:tensorflow:step = 2279, loss = 0.335016 (8.476 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7922\n",
      "INFO:tensorflow:step = 2379, loss = 0.329165 (8.480 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.8456\n",
      "INFO:tensorflow:step = 2479, loss = 0.332346 (8.442 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2541 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.330012.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:38:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2541\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:38:34\n",
      "INFO:tensorflow:Saving dict for global step 2541: accuracy = 0.97895, cross_entropy_loss = 0.0816362, global_step = 2541, loss = 0.369641\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2541\n",
      "INFO:tensorflow:Saving checkpoints for 2542 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 2542, loss = 0.442134\n",
      "INFO:tensorflow:global_step/sec: 11.7319\n",
      "INFO:tensorflow:step = 2642, loss = 0.327804 (8.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4539\n",
      "INFO:tensorflow:step = 2742, loss = 0.32119 (8.731 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3676\n",
      "INFO:tensorflow:step = 2842, loss = 0.324566 (8.797 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2904 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.322807.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:39:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2904\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:39:11\n",
      "INFO:tensorflow:Saving dict for global step 2904: accuracy = 0.980141, cross_entropy_loss = 0.0775568, global_step = 2904, loss = 0.365112\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-2904\n",
      "INFO:tensorflow:Saving checkpoints for 2905 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 2905, loss = 0.429575\n",
      "INFO:tensorflow:global_step/sec: 12.4227\n",
      "INFO:tensorflow:step = 3005, loss = 0.32191 (8.051 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.3761\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:step = 3105, loss = 0.314914 (8.081 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2234\n",
      "INFO:tensorflow:step = 3205, loss = 0.318273 (8.181 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3267 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.317086.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:39:42\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3267\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:39:46\n",
      "INFO:tensorflow:Saving dict for global step 3267: accuracy = 0.981112, cross_entropy_loss = 0.0745269, global_step = 3267, loss = 0.361617\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3267\n",
      "INFO:tensorflow:Saving checkpoints for 3268 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 3268, loss = 0.418702\n",
      "INFO:tensorflow:global_step/sec: 11.4856\n",
      "INFO:tensorflow:step = 3368, loss = 0.316922 (8.708 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2539\n",
      "INFO:tensorflow:step = 3468, loss = 0.309829 (8.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6783\n",
      "INFO:tensorflow:step = 3568, loss = 0.313016 (8.563 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3630 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.312368.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:40:18\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3630\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:40:22\n",
      "INFO:tensorflow:Saving dict for global step 3630: accuracy = 0.98173, cross_entropy_loss = 0.0722562, global_step = 3630, loss = 0.358868\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3630\n",
      "INFO:tensorflow:Saving checkpoints for 3631 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 3631, loss = 0.409031\n",
      "INFO:tensorflow:global_step/sec: 11.3485\n",
      "INFO:tensorflow:step = 3731, loss = 0.312461 (8.813 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.401\n",
      "INFO:tensorflow:step = 3831, loss = 0.305557 (8.771 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.512\n",
      "INFO:tensorflow:step = 3931, loss = 0.308533 (8.686 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3993 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.308323.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:40:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3993\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:40:59\n",
      "INFO:tensorflow:Saving dict for global step 3993: accuracy = 0.982083, cross_entropy_loss = 0.0705395, global_step = 3993, loss = 0.356663\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-3993\n",
      "INFO:tensorflow:Saving checkpoints for 3994 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 3994, loss = 0.400211\n",
      "INFO:tensorflow:global_step/sec: 11.4092\n",
      "INFO:tensorflow:step = 4094, loss = 0.308391 (8.766 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5544\n",
      "INFO:tensorflow:step = 4194, loss = 0.301872 (8.655 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7341\n",
      "INFO:tensorflow:step = 4294, loss = 0.304658 (8.522 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4356 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.304749.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:41:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-4356\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:41:35\n",
      "INFO:tensorflow:Saving dict for global step 4356: accuracy = 0.982613, cross_entropy_loss = 0.0692425, global_step = 4356, loss = 0.354871\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-4356\n",
      "INFO:tensorflow:Saving checkpoints for 4357 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 4357, loss = 0.391999\n",
      "INFO:tensorflow:global_step/sec: 11.6173\n",
      "INFO:tensorflow:step = 4457, loss = 0.304711 (8.609 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.779\n",
      "INFO:tensorflow:step = 4557, loss = 0.298654 (8.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.7585\n",
      "INFO:tensorflow:step = 4657, loss = 0.301278 (8.504 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4719 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.30153.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:42:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-4719\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:42:11\n",
      "INFO:tensorflow:Saving dict for global step 4719: accuracy = 0.982921, cross_entropy_loss = 0.0682763, global_step = 4719, loss = 0.353405\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-4719\n",
      "INFO:tensorflow:Saving checkpoints for 4720 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 4720, loss = 0.384239\n",
      "INFO:tensorflow:global_step/sec: 12.0789\n",
      "INFO:tensorflow:step = 4820, loss = 0.301396 (8.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.9682\n",
      "INFO:tensorflow:step = 4920, loss = 0.295827 (8.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.425\n",
      "INFO:tensorflow:step = 5020, loss = 0.298319 (8.753 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5082 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.298609.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:42:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5082\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:42:47\n",
      "INFO:tensorflow:Saving dict for global step 5082: accuracy = 0.983319, cross_entropy_loss = 0.0675632, global_step = 5082, loss = 0.35219\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5082\n",
      "INFO:tensorflow:Saving checkpoints for 5083 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 5083, loss = 0.376887\n",
      "INFO:tensorflow:global_step/sec: 11.3779\n",
      "INFO:tensorflow:step = 5183, loss = 0.298477 (8.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3192\n",
      "INFO:tensorflow:step = 5283, loss = 0.293376 (8.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2531\n",
      "INFO:tensorflow:step = 5383, loss = 0.295711 (8.887 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5445 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.295957.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:43:20\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5445\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:43:24\n",
      "INFO:tensorflow:Saving dict for global step 5445: accuracy = 0.983495, cross_entropy_loss = 0.0670544, global_step = 5445, loss = 0.351179\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5445\n",
      "INFO:tensorflow:Saving checkpoints for 5446 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 5446, loss = 0.369909\n",
      "INFO:tensorflow:global_step/sec: 11.5104\n",
      "INFO:tensorflow:step = 5546, loss = 0.295936 (8.689 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.6668\n",
      "INFO:tensorflow:step = 5646, loss = 0.291263 (8.571 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5872\n",
      "INFO:tensorflow:step = 5746, loss = 0.293394 (8.630 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5808 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.293537.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:43:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5808\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:44:00\n",
      "INFO:tensorflow:Saving dict for global step 5808: accuracy = 0.983583, cross_entropy_loss = 0.0667153, global_step = 5808, loss = 0.350339\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-5808\n",
      "INFO:tensorflow:Saving checkpoints for 5809 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 5809, loss = 0.363248\n",
      "INFO:tensorflow:global_step/sec: 11.363\n",
      "INFO:tensorflow:step = 5909, loss = 0.293691 (8.802 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.4513\n",
      "INFO:tensorflow:step = 6009, loss = 0.289427 (8.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.3954\n",
      "INFO:tensorflow:step = 6109, loss = 0.291323 (8.776 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6171 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.291322.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:44:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6171\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:44:37\n",
      "INFO:tensorflow:Saving dict for global step 6171: accuracy = 0.983672, cross_entropy_loss = 0.0665147, global_step = 6171, loss = 0.349638\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6171\n",
      "INFO:tensorflow:Saving checkpoints for 6172 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 6172, loss = 0.356843\n",
      "INFO:tensorflow:global_step/sec: 12.2306\n",
      "INFO:tensorflow:step = 6272, loss = 0.291685 (8.178 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2119\n",
      "INFO:tensorflow:step = 6372, loss = 0.287811 (8.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4524\n",
      "INFO:tensorflow:step = 6472, loss = 0.289463 (8.030 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6534 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.289304.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:45:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6534\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:45:11\n",
      "INFO:tensorflow:Saving dict for global step 6534: accuracy = 0.983848, cross_entropy_loss = 0.0664287, global_step = 6534, loss = 0.349053\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6534\n",
      "INFO:tensorflow:Saving checkpoints for 6535 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 6535, loss = 0.350682\n",
      "INFO:tensorflow:global_step/sec: 12.2737\n",
      "INFO:tensorflow:step = 6635, loss = 0.289877 (8.149 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.2391\n",
      "INFO:tensorflow:step = 6735, loss = 0.28637 (8.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.953\n",
      "INFO:tensorflow:step = 6835, loss = 0.28778 (8.366 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6897 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.287474.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:45:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6897\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:45:46\n",
      "INFO:tensorflow:Saving dict for global step 6897: accuracy = 0.983936, cross_entropy_loss = 0.0664379, global_step = 6897, loss = 0.348565\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-6897\n",
      "INFO:tensorflow:Saving checkpoints for 6898 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:step = 6898, loss = 0.344802\n",
      "INFO:tensorflow:global_step/sec: 11.5742\n",
      "INFO:tensorflow:step = 6998, loss = 0.288235 (8.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.5022\n",
      "INFO:tensorflow:step = 7098, loss = 0.285069 (8.694 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2234\n",
      "INFO:tensorflow:step = 7198, loss = 0.286248 (8.910 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7260 into /tmp/tmpqkbx6lwt/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.285815.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:46:19\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-7260\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:46:22\n",
      "INFO:tensorflow:Saving dict for global step 7260: accuracy = 0.984157, cross_entropy_loss = 0.0665251, global_step = 7260, loss = 0.348155\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "        model1.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model1.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u = list(conf_score_u_train) + list(conf_score_u_test)\n",
    "conf_score_u = np.reshape(conf_score_u,newshape = (len(conf_score_u),))\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_sent, \"ns\":u_ns , \"conf\": conf_score_u},\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions = model1.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-7260\n"
     ]
    }
   ],
   "source": [
    "predictions = list(predictions)\n",
    "pred_label_U = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    pred_label_U.append(predictions[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-12-21:53:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpqkbx6lwt/model.ckpt-7260\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-12-21:53:34\n",
      "INFO:tensorflow:Saving dict for global step 7260: accuracy = 0.984157, cross_entropy_loss = 0.0665251, global_step = 7260, loss = 0.348155\n",
      "Accuracy on test set: 98.42%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.98415709,\n",
       " 'cross_entropy_loss': 0.066525094,\n",
       " 'global_step': 7260,\n",
       " 'loss': 0.34815535}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model1.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U = pd.DataFrame(data = pred_label_U, columns = ['pred_labels'])\n",
    "pred_label_U.to_csv(\"predictions_12April.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U = pd.read_csv(\"predictions_12April.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     113299\n",
       "pred_labels    113299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0        1\n",
       "pred_labels    1704\n",
       "dtype: int64"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         1\n",
       "pred_labels    92233\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         1\n",
       "pred_labels    19362\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_0 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 0:\n",
    "        pred_label_U_0.append(1)\n",
    "    else:\n",
    "        pred_label_U_0.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_1 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 1:\n",
    "        pred_label_U_1.append(1)\n",
    "    else:\n",
    "        pred_label_U_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_2 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 2:\n",
    "        pred_label_U_2.append(1)\n",
    "    else:\n",
    "        pred_label_U_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_0'] = pred_label_U_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_1'] = pred_label_U_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_2'] = pred_label_U_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = U_sent.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "input_x = tf.placeholder(tf.float32,shape=[None,feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = tf.placeholder(tf.float32,shape=[None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32, name=\"hold_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our model\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(name = 'W_embed', shape=[V,embed_dim],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_)\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        \n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation,    \n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                             , bias_initializer=tf.zeros_initializer(), name = 'Hidden_%d'%i)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.nn.dropout(h_, keep_prob=dropout_rate)\n",
    "            \n",
    "        #if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(inputs = h_, rate=dropout_rate, training = is_training)\n",
    "        \n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "        embedding_size, filter_heights, filter_widths,num_filters,channels_in,channels_out, init_scale,l2_reg_lambda):\n",
    "\n",
    "        assert(len(filter_widths)==num_filters)\n",
    "        assert(len(filter_heights)==num_filters)\n",
    "        assert(len(channels_in)==num_filters)\n",
    "        assert(len(channels_out)==num_filters)\n",
    "        assert(channels_in[0]==1)\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding_CNN\"):\n",
    "            W_embedding = tf.get_variable(name = 'W_embedding', shape=[vocab_size,embedding_size],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "            self.xs = tf.nn.embedding_lookup(W_embedding, self.input_x)\n",
    "            self.xs_expanded = tf.reshape(self.xs,[-1,sequence_length,embedding_size,1])\n",
    "            \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        i = 1\n",
    "        x = self.xs_expanded\n",
    "        for filter_h, filter_w, c_in, c_out in zip(filter_heights,filter_widths,channels_in,channels_out):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % i):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_h, filter_w, c_in , c_out]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[c_out]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\",name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h,ksize=[1,embedding_size-filter_w+1, 1, 1],strides=[1, 1, 1, 1],padding=\"SAME\",name=\"pool\")\n",
    "                x = pooled\n",
    "                i = i + 1\n",
    "                pooled_outputs.append(pooled)\n",
    "        \n",
    "        length = len(pooled_outputs)\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = c_out*sequence_length*embedding_size\n",
    "        self.h_pool_flat = tf.reshape(pooled_outputs[length-1], [-1, num_filters_total])\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final padded sentences\n",
    "X_sent_test, ns = utils.pad_np_array(X_ids, max_len=max_len2, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label = pd.DataFrame(data = Y_label, columns = ['labels'])\n",
    "label_0 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 0:\n",
    "        label_0.append(1)\n",
    "    else:\n",
    "        label_0.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 1:\n",
    "        label_1.append(1)\n",
    "    else:\n",
    "        label_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 2:\n",
    "        label_2.append(1)\n",
    "    else:\n",
    "        label_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label['label_0'] = label_0\n",
    "Y_label['label_1'] = label_1\n",
    "Y_label['label_2'] = label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label_0  label_1  label_2\n",
       "0          1        0        0\n",
       "1          1        0        0\n",
       "2          1        0        0\n",
       "3          1        0        0\n",
       "4          1        0        0\n",
       "5          1        0        0\n",
       "6          1        0        0\n",
       "7          1        0        0\n",
       "8          1        0        0\n",
       "9          1        0        0\n",
       "10         1        0        0\n",
       "11         1        0        0\n",
       "12         1        0        0\n",
       "13         1        0        0\n",
       "14         1        0        0\n",
       "15         1        0        0\n",
       "16         1        0        0\n",
       "17         1        0        0\n",
       "18         1        0        0\n",
       "19         1        0        0\n",
       "20         1        0        0\n",
       "21         1        0        0\n",
       "22         1        0        0\n",
       "23         1        0        0\n",
       "24         1        0        0\n",
       "25         1        0        0\n",
       "26         1        0        0\n",
       "27         1        0        0\n",
       "28         1        0        0\n",
       "29         1        0        0\n",
       "..       ...      ...      ...\n",
       "570        1        0        0\n",
       "571        1        0        0\n",
       "572        1        0        0\n",
       "573        1        0        0\n",
       "574        1        0        0\n",
       "575        1        0        0\n",
       "576        1        0        0\n",
       "577        1        0        0\n",
       "578        1        0        0\n",
       "579        1        0        0\n",
       "580        1        0        0\n",
       "581        1        0        0\n",
       "582        1        0        0\n",
       "583        1        0        0\n",
       "584        1        0        0\n",
       "585        1        0        0\n",
       "586        1        0        0\n",
       "587        1        0        0\n",
       "588        1        0        0\n",
       "589        1        0        0\n",
       "590        1        0        0\n",
       "591        1        0        0\n",
       "592        1        0        0\n",
       "593        1        0        0\n",
       "594        1        0        0\n",
       "595        1        0        0\n",
       "596        1        0        0\n",
       "597        1        0        0\n",
       "598        1        0        0\n",
       "599        1        0        0\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_label.iloc[0:600][['label_0','label_1','label_2']]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ramya_girish/political_bias/political_bias/runs/1523578017\n",
      "\n",
      "2018-04-13T00:07:01.329226: step 1, loss 1.50804, acc 0.29\n",
      "2018-04-13T00:07:03.043461: step 2, loss 83.3398, acc 0.79\n",
      "2018-04-13T00:07:04.741172: step 3, loss 98.4132, acc 0.75\n",
      "2018-04-13T00:07:06.427617: step 4, loss 40.413, acc 0.86\n",
      "2018-04-13T00:07:08.115939: step 5, loss 33.7409, acc 0.81\n",
      "2018-04-13T00:07:09.785575: step 6, loss 24.1075, acc 0.78\n",
      "2018-04-13T00:07:11.454745: step 7, loss 8.88641, acc 0.82\n",
      "2018-04-13T00:07:13.117280: step 8, loss 24.996, acc 0.17\n",
      "2018-04-13T00:07:14.784913: step 9, loss 21.0143, acc 0.22\n",
      "2018-04-13T00:07:16.442222: step 10, loss 6.84779, acc 0.16\n",
      "2018-04-13T00:07:18.093108: step 11, loss 3.32468, acc 0.85\n",
      "2018-04-13T00:07:19.735124: step 12, loss 6.08347, acc 0.76\n",
      "2018-04-13T00:07:21.383001: step 13, loss 5.67003, acc 0.79\n",
      "2018-04-13T00:07:23.012329: step 14, loss 3.4453, acc 0.87\n",
      "2018-04-13T00:07:24.631812: step 15, loss 4.89934, acc 0.74\n",
      "2018-04-13T00:07:26.252500: step 16, loss 3.63381, acc 0.75\n",
      "2018-04-13T00:07:27.869495: step 17, loss 2.73599, acc 0.8\n",
      "2018-04-13T00:07:29.475996: step 18, loss 2.28246, acc 0.83\n",
      "2018-04-13T00:07:31.092389: step 19, loss 2.14242, acc 0.82\n",
      "2018-04-13T00:07:32.700293: step 20, loss 1.96504, acc 0.84\n",
      "2018-04-13T00:07:34.315481: step 21, loss 1.9734, acc 0.85\n",
      "2018-04-13T00:07:35.924941: step 22, loss 2.199, acc 0.77\n",
      "2018-04-13T00:07:37.523794: step 23, loss 2.29021, acc 0.84\n",
      "2018-04-13T00:07:39.129798: step 24, loss 2.39851, acc 0.77\n",
      "2018-04-13T00:07:40.738043: step 25, loss 2.40991, acc 0.74\n",
      "2018-04-13T00:07:42.333201: step 26, loss 2.38671, acc 0.79\n",
      "2018-04-13T00:07:43.928231: step 27, loss 2.34333, acc 0.84\n",
      "2018-04-13T00:07:45.525619: step 28, loss 2.31496, acc 0.82\n",
      "2018-04-13T00:07:47.123640: step 29, loss 2.27804, acc 0.82\n",
      "2018-04-13T00:07:48.726142: step 30, loss 2.20772, acc 0.89\n",
      "2018-04-13T00:07:50.323769: step 31, loss 2.16752, acc 0.84\n",
      "2018-04-13T00:07:51.917812: step 32, loss 2.11728, acc 0.83\n",
      "2018-04-13T00:07:53.513120: step 33, loss 2.03931, acc 0.85\n",
      "2018-04-13T00:07:55.114779: step 34, loss 2.07141, acc 0.73\n",
      "2018-04-13T00:07:56.723924: step 35, loss 1.90089, acc 0.85\n",
      "2018-04-13T00:07:58.324757: step 36, loss 1.86244, acc 0.81\n",
      "2018-04-13T00:07:59.920943: step 37, loss 1.86331, acc 0.79\n",
      "2018-04-13T00:08:01.529703: step 38, loss 1.69988, acc 0.86\n",
      "2018-04-13T00:08:03.136826: step 39, loss 1.74097, acc 0.83\n",
      "2018-04-13T00:08:04.739167: step 40, loss 1.77644, acc 0.79\n",
      "2018-04-13T00:08:06.343066: step 41, loss 1.72118, acc 0.81\n",
      "2018-04-13T00:08:07.940275: step 42, loss 1.57925, acc 0.88\n",
      "2018-04-13T00:08:09.546234: step 43, loss 1.64925, acc 0.82\n",
      "2018-04-13T00:08:11.151896: step 44, loss 1.78057, acc 0.8\n",
      "2018-04-13T00:08:12.760191: step 45, loss 1.64436, acc 0.82\n",
      "2018-04-13T00:08:14.361415: step 46, loss 1.57049, acc 0.85\n",
      "2018-04-13T00:08:15.966366: step 47, loss 1.60315, acc 0.84\n",
      "2018-04-13T00:08:17.560181: step 48, loss 1.58791, acc 0.81\n",
      "2018-04-13T00:08:19.165989: step 49, loss 1.5436, acc 0.83\n",
      "2018-04-13T00:08:20.772272: step 50, loss 1.49709, acc 0.84\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:08:27.612085: step 50, loss 1.51692, acc 0.797178\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-50\n",
      "\n",
      "2018-04-13T00:08:29.365861: step 51, loss 1.36835, acc 0.86\n",
      "2018-04-13T00:08:30.962482: step 52, loss 1.38888, acc 0.87\n",
      "2018-04-13T00:08:32.557160: step 53, loss 1.34877, acc 0.85\n",
      "2018-04-13T00:08:34.160130: step 54, loss 1.40082, acc 0.84\n",
      "2018-04-13T00:08:35.767181: step 55, loss 1.4726, acc 0.78\n",
      "2018-04-13T00:08:37.364105: step 56, loss 1.37947, acc 0.79\n",
      "2018-04-13T00:08:38.967002: step 57, loss 1.33339, acc 0.81\n",
      "2018-04-13T00:08:40.573597: step 58, loss 1.31352, acc 0.83\n",
      "2018-04-13T00:08:42.179234: step 59, loss 1.2337, acc 0.86\n",
      "2018-04-13T00:08:43.782986: step 60, loss 1.22941, acc 0.85\n",
      "2018-04-13T00:08:45.376467: step 61, loss 1.2623, acc 0.83\n",
      "2018-04-13T00:08:46.982687: step 62, loss 1.32513, acc 0.79\n",
      "2018-04-13T00:08:48.583039: step 63, loss 1.30068, acc 0.78\n",
      "2018-04-13T00:08:50.182046: step 64, loss 1.20166, acc 0.87\n",
      "2018-04-13T00:08:51.783480: step 65, loss 1.223, acc 0.81\n",
      "2018-04-13T00:08:53.395908: step 66, loss 1.06064, acc 0.92\n",
      "2018-04-13T00:08:54.997554: step 67, loss 1.33556, acc 0.74\n",
      "2018-04-13T00:08:56.600019: step 68, loss 1.24493, acc 0.77\n",
      "2018-04-13T00:08:58.203077: step 69, loss 1.19982, acc 0.82\n",
      "2018-04-13T00:08:59.800579: step 70, loss 1.13997, acc 0.83\n",
      "2018-04-13T00:09:01.410959: step 71, loss 1.08223, acc 0.84\n",
      "2018-04-13T00:09:03.010777: step 72, loss 1.09059, acc 0.83\n",
      "2018-04-13T00:09:04.616339: step 73, loss 1.11146, acc 0.81\n",
      "2018-04-13T00:09:06.234699: step 74, loss 1.0475, acc 0.84\n",
      "2018-04-13T00:09:07.840546: step 75, loss 1.08882, acc 0.81\n",
      "2018-04-13T00:09:09.452961: step 76, loss 0.995876, acc 0.86\n",
      "2018-04-13T00:09:11.053320: step 77, loss 0.973924, acc 0.85\n",
      "2018-04-13T00:09:12.655486: step 78, loss 1.03901, acc 0.82\n",
      "2018-04-13T00:09:14.261644: step 79, loss 1.0373, acc 0.81\n",
      "2018-04-13T00:09:15.866138: step 80, loss 0.984154, acc 0.84\n",
      "2018-04-13T00:09:17.472434: step 81, loss 0.994065, acc 0.85\n",
      "2018-04-13T00:09:19.079750: step 82, loss 1.00645, acc 0.8\n",
      "2018-04-13T00:09:20.685352: step 83, loss 1.05397, acc 0.79\n",
      "2018-04-13T00:09:22.289723: step 84, loss 1.01077, acc 0.81\n",
      "2018-04-13T00:09:23.899220: step 85, loss 0.931045, acc 0.84\n",
      "2018-04-13T00:09:25.505390: step 86, loss 0.989363, acc 0.8\n",
      "2018-04-13T00:09:27.105202: step 87, loss 1.02937, acc 0.78\n",
      "2018-04-13T00:09:28.711292: step 88, loss 0.820262, acc 0.88\n",
      "2018-04-13T00:09:30.315686: step 89, loss 1.00202, acc 0.8\n",
      "2018-04-13T00:09:31.919506: step 90, loss 0.855154, acc 0.88\n",
      "2018-04-13T00:09:33.516329: step 91, loss 0.843655, acc 0.85\n",
      "2018-04-13T00:09:35.124897: step 92, loss 0.975744, acc 0.79\n",
      "2018-04-13T00:09:36.740758: step 93, loss 0.896601, acc 0.82\n",
      "2018-04-13T00:09:38.346294: step 94, loss 0.977823, acc 0.78\n",
      "2018-04-13T00:09:39.952894: step 95, loss 0.929526, acc 0.79\n",
      "2018-04-13T00:09:41.556458: step 96, loss 0.82013, acc 0.84\n",
      "2018-04-13T00:09:43.160345: step 97, loss 1.06405, acc 0.71\n",
      "2018-04-13T00:09:44.758980: step 98, loss 0.846035, acc 0.86\n",
      "2018-04-13T00:09:46.368679: step 99, loss 0.810805, acc 0.85\n",
      "2018-04-13T00:09:47.975422: step 100, loss 0.79941, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:09:53.446398: step 100, loss 0.876503, acc 0.797178\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-100\n",
      "\n",
      "2018-04-13T00:09:55.199339: step 101, loss 0.788195, acc 0.84\n",
      "2018-04-13T00:09:56.796165: step 102, loss 0.806258, acc 0.84\n",
      "2018-04-13T00:09:58.401623: step 103, loss 0.814209, acc 0.83\n",
      "2018-04-13T00:10:00.003061: step 104, loss 0.96101, acc 0.76\n",
      "2018-04-13T00:10:01.599392: step 105, loss 0.70025, acc 0.89\n",
      "2018-04-13T00:10:03.194605: step 106, loss 0.7181, acc 0.86\n",
      "2018-04-13T00:10:04.790888: step 107, loss 0.940352, acc 0.77\n",
      "2018-04-13T00:10:06.394548: step 108, loss 0.766136, acc 0.88\n",
      "2018-04-13T00:10:07.994532: step 109, loss 0.762881, acc 0.85\n",
      "2018-04-13T00:10:09.604308: step 110, loss 0.830128, acc 0.79\n",
      "2018-04-13T00:10:11.211327: step 111, loss 0.891759, acc 0.75\n",
      "2018-04-13T00:10:12.816102: step 112, loss 0.66417, acc 0.89\n",
      "2018-04-13T00:10:14.425135: step 113, loss 0.814296, acc 0.81\n",
      "2018-04-13T00:10:16.028399: step 114, loss 0.7263, acc 0.86\n",
      "2018-04-13T00:10:17.625973: step 115, loss 0.680962, acc 0.87\n",
      "2018-04-13T00:10:19.228083: step 116, loss 0.774624, acc 0.81\n",
      "2018-04-13T00:10:20.825673: step 117, loss 0.824557, acc 0.78\n",
      "2018-04-13T00:10:22.417192: step 118, loss 0.723626, acc 0.85\n",
      "2018-04-13T00:10:24.022968: step 119, loss 0.750401, acc 0.82\n",
      "2018-04-13T00:10:25.625701: step 120, loss 0.768667, acc 0.83\n",
      "2018-04-13T00:10:27.227085: step 121, loss 0.741008, acc 0.82\n",
      "2018-04-13T00:10:28.830033: step 122, loss 0.789569, acc 0.8\n",
      "2018-04-13T00:10:30.432347: step 123, loss 0.662704, acc 0.85\n",
      "2018-04-13T00:10:32.042899: step 124, loss 0.826184, acc 0.76\n",
      "2018-04-13T00:10:33.644176: step 125, loss 0.719988, acc 0.81\n",
      "2018-04-13T00:10:35.240163: step 126, loss 0.710346, acc 0.83\n",
      "2018-04-13T00:10:36.841440: step 127, loss 0.749549, acc 0.83\n",
      "2018-04-13T00:10:38.438135: step 128, loss 0.751267, acc 0.83\n",
      "2018-04-13T00:10:40.033575: step 129, loss 0.657103, acc 0.84\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:10:41.629501: step 130, loss 0.632039, acc 0.87\n",
      "2018-04-13T00:10:43.226497: step 131, loss 0.631576, acc 0.85\n",
      "2018-04-13T00:10:44.825001: step 132, loss 0.662599, acc 0.83\n",
      "2018-04-13T00:10:46.427511: step 133, loss 0.622234, acc 0.85\n",
      "2018-04-13T00:10:48.031641: step 134, loss 0.767864, acc 0.81\n",
      "2018-04-13T00:10:49.638317: step 135, loss 0.612674, acc 0.85\n",
      "2018-04-13T00:10:51.235963: step 136, loss 0.806828, acc 0.76\n",
      "2018-04-13T00:10:52.838960: step 137, loss 0.665158, acc 0.82\n",
      "2018-04-13T00:10:54.435650: step 138, loss 0.680604, acc 0.82\n",
      "2018-04-13T00:10:56.032030: step 139, loss 0.663089, acc 0.83\n",
      "2018-04-13T00:10:57.641961: step 140, loss 0.792895, acc 0.76\n",
      "2018-04-13T00:10:59.244556: step 141, loss 0.694758, acc 0.81\n",
      "2018-04-13T00:11:00.845533: step 142, loss 0.619723, acc 0.86\n",
      "2018-04-13T00:11:02.446489: step 143, loss 0.625378, acc 0.86\n",
      "2018-04-13T00:11:04.045816: step 144, loss 0.660026, acc 0.83\n",
      "2018-04-13T00:11:05.655631: step 145, loss 0.595868, acc 0.85\n",
      "2018-04-13T00:11:07.263565: step 146, loss 0.661443, acc 0.84\n",
      "2018-04-13T00:11:08.864306: step 147, loss 0.717802, acc 0.78\n",
      "2018-04-13T00:11:10.467939: step 148, loss 0.669044, acc 0.84\n",
      "2018-04-13T00:11:12.076719: step 149, loss 0.779702, acc 0.76\n",
      "2018-04-13T00:11:13.672545: step 150, loss 0.578417, acc 0.85\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:11:19.134968: step 150, loss 0.693962, acc 0.797178\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-150\n",
      "\n",
      "2018-04-13T00:11:20.890123: step 151, loss 0.64145, acc 0.84\n",
      "2018-04-13T00:11:22.491691: step 152, loss 0.697799, acc 0.82\n",
      "2018-04-13T00:11:24.098092: step 153, loss 0.700934, acc 0.8\n",
      "2018-04-13T00:11:25.699130: step 154, loss 0.641667, acc 0.85\n",
      "2018-04-13T00:11:27.300871: step 155, loss 0.663448, acc 0.82\n",
      "2018-04-13T00:11:28.903308: step 156, loss 0.486521, acc 0.9\n",
      "2018-04-13T00:11:30.501529: step 157, loss 0.587067, acc 0.84\n",
      "2018-04-13T00:11:32.098534: step 158, loss 0.705999, acc 0.78\n",
      "2018-04-13T00:11:33.695015: step 159, loss 0.781342, acc 0.77\n",
      "2018-04-13T00:11:35.296663: step 160, loss 0.633702, acc 0.81\n",
      "2018-04-13T00:11:36.898248: step 161, loss 0.77901, acc 0.75\n",
      "2018-04-13T00:11:38.498713: step 162, loss 0.734547, acc 0.79\n",
      "2018-04-13T00:11:40.104293: step 163, loss 0.518755, acc 0.9\n",
      "2018-04-13T00:11:41.697291: step 164, loss 0.575644, acc 0.85\n",
      "2018-04-13T00:11:43.290890: step 165, loss 0.669296, acc 0.79\n",
      "2018-04-13T00:11:44.897344: step 166, loss 0.718776, acc 0.75\n",
      "2018-04-13T00:11:46.500738: step 167, loss 0.58038, acc 0.85\n",
      "2018-04-13T00:11:48.103846: step 168, loss 0.670195, acc 0.8\n",
      "2018-04-13T00:11:49.702265: step 169, loss 0.647663, acc 0.8\n",
      "2018-04-13T00:11:51.309696: step 170, loss 0.649018, acc 0.81\n",
      "2018-04-13T00:11:52.901829: step 171, loss 0.675969, acc 0.81\n",
      "2018-04-13T00:11:54.498123: step 172, loss 0.701118, acc 0.78\n",
      "2018-04-13T00:11:56.096263: step 173, loss 0.491392, acc 0.88\n",
      "2018-04-13T00:11:57.695118: step 174, loss 0.5504, acc 0.84\n",
      "2018-04-13T00:11:59.295170: step 175, loss 0.607702, acc 0.83\n",
      "2018-04-13T00:12:00.899498: step 176, loss 0.670801, acc 0.78\n",
      "2018-04-13T00:12:02.497577: step 177, loss 0.591338, acc 0.83\n",
      "2018-04-13T00:12:04.095332: step 178, loss 0.674045, acc 0.79\n",
      "2018-04-13T00:12:05.693357: step 179, loss 0.600294, acc 0.82\n",
      "2018-04-13T00:12:07.292332: step 180, loss 0.573493, acc 0.82\n",
      "2018-04-13T00:12:08.888145: step 181, loss 0.635296, acc 0.77\n",
      "2018-04-13T00:12:10.490650: step 182, loss 0.53669, acc 0.84\n",
      "2018-04-13T00:12:12.088161: step 183, loss 0.440686, acc 0.93\n",
      "2018-04-13T00:12:13.680780: step 184, loss 0.535919, acc 0.86\n",
      "2018-04-13T00:12:15.288916: step 185, loss 0.629836, acc 0.83\n",
      "2018-04-13T00:12:16.887546: step 186, loss 0.700899, acc 0.79\n",
      "2018-04-13T00:12:18.488903: step 187, loss 0.560743, acc 0.82\n",
      "2018-04-13T00:12:20.086871: step 188, loss 0.651412, acc 0.8\n",
      "2018-04-13T00:12:21.692850: step 189, loss 0.603723, acc 0.81\n",
      "2018-04-13T00:12:23.293584: step 190, loss 0.507472, acc 0.85\n",
      "2018-04-13T00:12:24.890362: step 191, loss 0.454318, acc 0.88\n",
      "2018-04-13T00:12:26.495172: step 192, loss 0.566563, acc 0.83\n",
      "2018-04-13T00:12:28.096256: step 193, loss 0.682742, acc 0.8\n",
      "2018-04-13T00:12:29.702045: step 194, loss 0.651988, acc 0.77\n",
      "2018-04-13T00:12:31.304634: step 195, loss 0.524855, acc 0.85\n",
      "2018-04-13T00:12:32.903469: step 196, loss 0.606493, acc 0.78\n",
      "2018-04-13T00:12:34.507173: step 197, loss 0.572555, acc 0.83\n",
      "2018-04-13T00:12:36.110633: step 198, loss 0.604685, acc 0.82\n",
      "2018-04-13T00:12:37.707893: step 199, loss 0.681535, acc 0.76\n",
      "2018-04-13T00:12:39.303381: step 200, loss 0.556357, acc 0.83\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:12:44.718826: step 200, loss 0.601218, acc 0.797178\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-200\n",
      "\n",
      "2018-04-13T00:12:46.461635: step 201, loss 0.574955, acc 0.82\n",
      "2018-04-13T00:12:48.057123: step 202, loss 0.548765, acc 0.83\n",
      "2018-04-13T00:12:49.650670: step 203, loss 0.636518, acc 0.82\n",
      "2018-04-13T00:12:51.249324: step 204, loss 0.546169, acc 0.82\n",
      "2018-04-13T00:12:52.843795: step 205, loss 0.575517, acc 0.81\n",
      "2018-04-13T00:12:54.440338: step 206, loss 0.625027, acc 0.79\n",
      "2018-04-13T00:12:56.047531: step 207, loss 0.569038, acc 0.81\n",
      "2018-04-13T00:12:57.651075: step 208, loss 0.565176, acc 0.81\n",
      "2018-04-13T00:12:59.251542: step 209, loss 0.423352, acc 0.89\n",
      "2018-04-13T00:13:00.856555: step 210, loss 0.401197, acc 0.9\n",
      "2018-04-13T00:13:02.457686: step 211, loss 0.552526, acc 0.84\n",
      "2018-04-13T00:13:04.062933: step 212, loss 0.5548, acc 0.8\n",
      "2018-04-13T00:13:05.669115: step 213, loss 0.544632, acc 0.84\n",
      "2018-04-13T00:13:07.277206: step 214, loss 0.688928, acc 0.77\n",
      "2018-04-13T00:13:08.878716: step 215, loss 0.686557, acc 0.75\n",
      "2018-04-13T00:13:10.478925: step 216, loss 0.602515, acc 0.8\n",
      "2018-04-13T00:13:12.079180: step 217, loss 0.447397, acc 0.87\n",
      "2018-04-13T00:13:13.676381: step 218, loss 0.509125, acc 0.83\n",
      "2018-04-13T00:13:15.281337: step 219, loss 0.595175, acc 0.8\n",
      "2018-04-13T00:13:16.879207: step 220, loss 0.63998, acc 0.77\n",
      "2018-04-13T00:13:18.475090: step 221, loss 0.462086, acc 0.86\n",
      "2018-04-13T00:13:20.070726: step 222, loss 0.521771, acc 0.82\n",
      "2018-04-13T00:13:21.668151: step 223, loss 0.522894, acc 0.82\n",
      "2018-04-13T00:13:23.267236: step 224, loss 0.52064, acc 0.84\n",
      "2018-04-13T00:13:24.865983: step 225, loss 0.484, acc 0.84\n",
      "2018-04-13T00:13:26.465273: step 226, loss 0.532739, acc 0.83\n",
      "2018-04-13T00:13:28.073253: step 227, loss 0.686749, acc 0.75\n",
      "2018-04-13T00:13:29.679533: step 228, loss 0.523492, acc 0.84\n",
      "2018-04-13T00:13:31.274378: step 229, loss 0.509613, acc 0.82\n",
      "2018-04-13T00:13:32.879768: step 230, loss 0.534214, acc 0.8\n",
      "2018-04-13T00:13:34.479061: step 231, loss 0.579542, acc 0.79\n",
      "2018-04-13T00:13:36.088217: step 232, loss 0.496278, acc 0.86\n",
      "2018-04-13T00:13:37.684269: step 233, loss 0.497466, acc 0.83\n",
      "2018-04-13T00:13:39.289357: step 234, loss 0.597204, acc 0.79\n",
      "2018-04-13T00:13:40.888508: step 235, loss 0.51207, acc 0.77\n",
      "2018-04-13T00:13:42.488965: step 236, loss 0.461146, acc 0.84\n",
      "2018-04-13T00:13:44.096509: step 237, loss 0.638905, acc 0.77\n",
      "2018-04-13T00:13:45.697019: step 238, loss 0.461936, acc 0.87\n",
      "2018-04-13T00:13:47.293402: step 239, loss 0.424279, acc 0.87\n",
      "2018-04-13T00:13:48.891793: step 240, loss 0.556709, acc 0.79\n",
      "2018-04-13T00:13:50.489752: step 241, loss 0.473269, acc 0.83\n",
      "2018-04-13T00:13:52.087599: step 242, loss 0.417052, acc 0.85\n",
      "2018-04-13T00:13:53.682790: step 243, loss 0.454534, acc 0.83\n",
      "2018-04-13T00:13:55.283661: step 244, loss 0.529069, acc 0.81\n",
      "2018-04-13T00:13:56.882436: step 245, loss 0.541434, acc 0.81\n",
      "2018-04-13T00:13:58.489006: step 246, loss 0.474079, acc 0.81\n",
      "2018-04-13T00:14:00.089997: step 247, loss 0.542822, acc 0.76\n",
      "2018-04-13T00:14:01.692056: step 248, loss 0.52928, acc 0.81\n",
      "2018-04-13T00:14:03.291380: step 249, loss 0.432975, acc 0.87\n",
      "2018-04-13T00:14:04.893231: step 250, loss 0.463054, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:14:10.344074: step 250, loss 0.47509, acc 0.805996\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-250\n",
      "\n",
      "2018-04-13T00:14:12.087386: step 251, loss 0.441419, acc 0.87\n",
      "2018-04-13T00:14:13.685196: step 252, loss 0.537442, acc 0.75\n",
      "2018-04-13T00:14:15.283430: step 253, loss 0.391078, acc 0.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:14:16.885317: step 254, loss 0.460036, acc 0.8\n",
      "2018-04-13T00:14:18.486482: step 255, loss 0.49623, acc 0.79\n",
      "2018-04-13T00:14:20.089166: step 256, loss 0.413141, acc 0.83\n",
      "2018-04-13T00:14:21.683267: step 257, loss 0.320049, acc 0.85\n",
      "2018-04-13T00:14:23.285045: step 258, loss 0.644897, acc 0.8\n",
      "2018-04-13T00:14:24.883013: step 259, loss 0.38191, acc 0.94\n",
      "2018-04-13T00:14:26.485775: step 260, loss 0.519313, acc 0.88\n",
      "2018-04-13T00:14:28.082535: step 261, loss 0.44033, acc 0.91\n",
      "2018-04-13T00:14:29.677802: step 262, loss 0.472724, acc 0.85\n",
      "2018-04-13T00:14:31.276330: step 263, loss 0.392568, acc 0.87\n",
      "2018-04-13T00:14:32.879531: step 264, loss 0.39722, acc 0.85\n",
      "2018-04-13T00:14:34.482127: step 265, loss 0.442471, acc 0.84\n",
      "2018-04-13T00:14:36.083143: step 266, loss 0.485596, acc 0.85\n",
      "2018-04-13T00:14:37.688607: step 267, loss 0.380023, acc 0.86\n",
      "2018-04-13T00:14:39.289338: step 268, loss 0.503239, acc 0.85\n",
      "2018-04-13T00:14:40.889563: step 269, loss 0.399654, acc 0.89\n",
      "2018-04-13T00:14:42.492479: step 270, loss 0.358223, acc 0.92\n",
      "2018-04-13T00:14:44.099453: step 271, loss 0.359147, acc 0.88\n",
      "2018-04-13T00:14:45.706272: step 272, loss 0.379805, acc 0.93\n",
      "2018-04-13T00:14:47.313463: step 273, loss 0.319578, acc 0.96\n",
      "2018-04-13T00:14:48.915047: step 274, loss 0.466066, acc 0.91\n",
      "2018-04-13T00:14:50.518070: step 275, loss 0.353619, acc 0.92\n",
      "2018-04-13T00:14:52.123963: step 276, loss 0.360998, acc 0.92\n",
      "2018-04-13T00:14:53.731214: step 277, loss 0.420727, acc 0.89\n",
      "2018-04-13T00:14:55.329708: step 278, loss 0.483645, acc 0.89\n",
      "2018-04-13T00:14:56.930878: step 279, loss 0.280694, acc 0.94\n",
      "2018-04-13T00:14:58.527546: step 280, loss 0.396524, acc 0.92\n",
      "2018-04-13T00:15:00.126665: step 281, loss 0.326883, acc 0.95\n",
      "2018-04-13T00:15:01.730471: step 282, loss 0.307934, acc 0.96\n",
      "2018-04-13T00:15:03.325146: step 283, loss 0.361209, acc 0.93\n",
      "2018-04-13T00:15:04.924665: step 284, loss 0.339148, acc 0.94\n",
      "2018-04-13T00:15:06.531134: step 285, loss 0.350643, acc 0.91\n",
      "2018-04-13T00:15:08.137494: step 286, loss 0.384744, acc 0.9\n",
      "2018-04-13T00:15:09.743404: step 287, loss 0.403031, acc 0.89\n",
      "2018-04-13T00:15:11.338087: step 288, loss 0.349443, acc 0.94\n",
      "2018-04-13T00:15:12.944291: step 289, loss 0.26547, acc 0.95\n",
      "2018-04-13T00:15:14.545746: step 290, loss 0.211014, acc 0.98\n",
      "2018-04-13T00:15:16.147889: step 291, loss 0.264135, acc 0.95\n",
      "2018-04-13T00:15:17.753386: step 292, loss 0.290131, acc 0.94\n",
      "2018-04-13T00:15:19.361177: step 293, loss 0.2025, acc 0.98\n",
      "2018-04-13T00:15:20.964376: step 294, loss 0.379105, acc 0.93\n",
      "2018-04-13T00:15:22.565721: step 295, loss 0.220976, acc 0.96\n",
      "2018-04-13T00:15:24.176138: step 296, loss 0.298864, acc 0.95\n",
      "2018-04-13T00:15:25.780810: step 297, loss 0.30736, acc 0.94\n",
      "2018-04-13T00:15:27.388647: step 298, loss 0.29513, acc 0.95\n",
      "2018-04-13T00:15:28.991735: step 299, loss 0.273194, acc 0.95\n",
      "2018-04-13T00:15:30.591455: step 300, loss 0.226582, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:15:36.066214: step 300, loss 0.292272, acc 0.936508\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-300\n",
      "\n",
      "2018-04-13T00:15:37.838806: step 301, loss 0.243753, acc 0.96\n",
      "2018-04-13T00:15:39.443456: step 302, loss 0.288893, acc 0.95\n",
      "2018-04-13T00:15:41.042154: step 303, loss 0.458856, acc 0.88\n",
      "2018-04-13T00:15:42.647983: step 304, loss 0.280434, acc 0.95\n",
      "2018-04-13T00:15:44.255114: step 305, loss 0.289617, acc 0.97\n",
      "2018-04-13T00:15:45.860941: step 306, loss 0.288564, acc 0.95\n",
      "2018-04-13T00:15:47.466601: step 307, loss 0.352493, acc 0.91\n",
      "2018-04-13T00:15:49.066510: step 308, loss 0.303649, acc 0.96\n",
      "2018-04-13T00:15:50.669181: step 309, loss 0.239754, acc 0.94\n",
      "2018-04-13T00:15:52.270881: step 310, loss 0.274844, acc 0.93\n",
      "2018-04-13T00:15:53.875095: step 311, loss 0.283752, acc 0.94\n",
      "2018-04-13T00:15:55.478972: step 312, loss 0.435736, acc 0.88\n",
      "2018-04-13T00:15:57.081818: step 313, loss 0.21308, acc 0.96\n",
      "2018-04-13T00:15:58.684239: step 314, loss 0.226567, acc 0.96\n",
      "2018-04-13T00:16:00.282539: step 315, loss 0.213981, acc 0.95\n",
      "2018-04-13T00:16:01.892804: step 316, loss 0.260599, acc 0.94\n",
      "2018-04-13T00:16:03.495748: step 317, loss 0.286372, acc 0.93\n",
      "2018-04-13T00:16:05.096751: step 318, loss 0.255811, acc 0.95\n",
      "2018-04-13T00:16:06.705175: step 319, loss 0.320341, acc 0.92\n",
      "2018-04-13T00:16:08.304687: step 320, loss 0.305532, acc 0.94\n",
      "2018-04-13T00:16:09.911287: step 321, loss 0.261513, acc 0.95\n",
      "2018-04-13T00:16:11.517555: step 322, loss 0.271197, acc 0.97\n",
      "2018-04-13T00:16:13.120419: step 323, loss 0.263504, acc 0.97\n",
      "2018-04-13T00:16:14.722905: step 324, loss 0.166911, acc 0.97\n",
      "2018-04-13T00:16:16.326187: step 325, loss 0.310409, acc 0.91\n",
      "2018-04-13T00:16:17.933441: step 326, loss 0.215422, acc 0.95\n",
      "2018-04-13T00:16:19.536566: step 327, loss 0.328763, acc 0.93\n",
      "2018-04-13T00:16:21.140934: step 328, loss 0.393448, acc 0.89\n",
      "2018-04-13T00:16:22.745581: step 329, loss 0.248248, acc 0.94\n",
      "2018-04-13T00:16:24.348678: step 330, loss 0.279841, acc 0.92\n",
      "2018-04-13T00:16:25.952323: step 331, loss 0.25131, acc 0.95\n",
      "2018-04-13T00:16:27.549909: step 332, loss 0.323728, acc 0.94\n",
      "2018-04-13T00:16:29.154103: step 333, loss 0.212146, acc 0.97\n",
      "2018-04-13T00:16:30.754379: step 334, loss 0.162809, acc 0.98\n",
      "2018-04-13T00:16:32.353132: step 335, loss 0.318057, acc 0.94\n",
      "2018-04-13T00:16:33.961173: step 336, loss 0.240824, acc 0.96\n",
      "2018-04-13T00:16:35.563811: step 337, loss 0.21065, acc 0.96\n",
      "2018-04-13T00:16:37.166510: step 338, loss 0.149873, acc 0.98\n",
      "2018-04-13T00:16:38.768779: step 339, loss 0.182187, acc 0.95\n",
      "2018-04-13T00:16:40.369791: step 340, loss 0.128665, acc 0.97\n",
      "2018-04-13T00:16:41.971033: step 341, loss 0.248092, acc 0.94\n",
      "2018-04-13T00:16:43.574365: step 342, loss 0.351142, acc 0.93\n",
      "2018-04-13T00:16:45.176624: step 343, loss 0.177902, acc 0.96\n",
      "2018-04-13T00:16:46.777149: step 344, loss 0.316795, acc 0.94\n",
      "2018-04-13T00:16:48.381014: step 345, loss 0.216701, acc 0.94\n",
      "2018-04-13T00:16:49.981346: step 346, loss 0.196271, acc 0.97\n",
      "2018-04-13T00:16:51.589859: step 347, loss 0.210905, acc 0.97\n",
      "2018-04-13T00:16:53.194069: step 348, loss 0.246345, acc 0.96\n",
      "2018-04-13T00:16:54.801362: step 349, loss 0.221429, acc 0.97\n",
      "2018-04-13T00:16:56.407760: step 350, loss 0.257803, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:17:01.863406: step 350, loss 0.222685, acc 0.957672\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-350\n",
      "\n",
      "2018-04-13T00:17:03.640429: step 351, loss 0.306189, acc 0.92\n",
      "2018-04-13T00:17:05.236078: step 352, loss 0.206204, acc 0.95\n",
      "2018-04-13T00:17:06.841689: step 353, loss 0.260922, acc 0.95\n",
      "2018-04-13T00:17:08.443698: step 354, loss 0.235351, acc 0.95\n",
      "2018-04-13T00:17:10.047501: step 355, loss 0.303788, acc 0.94\n",
      "2018-04-13T00:17:11.649395: step 356, loss 0.116001, acc 0.99\n",
      "2018-04-13T00:17:13.262395: step 357, loss 0.231142, acc 0.96\n",
      "2018-04-13T00:17:14.865827: step 358, loss 0.205104, acc 0.95\n",
      "2018-04-13T00:17:16.469298: step 359, loss 0.252156, acc 0.94\n",
      "2018-04-13T00:17:18.073347: step 360, loss 0.156257, acc 0.97\n",
      "2018-04-13T00:17:19.680186: step 361, loss 0.188117, acc 0.97\n",
      "2018-04-13T00:17:21.289363: step 362, loss 0.197198, acc 0.96\n",
      "2018-04-13T00:17:22.895227: step 363, loss 0.203962, acc 0.97\n",
      "2018-04-13T00:17:24.505860: step 364, loss 0.289083, acc 0.95\n",
      "2018-04-13T00:17:26.110981: step 365, loss 0.235695, acc 0.96\n",
      "2018-04-13T00:17:27.717841: step 366, loss 0.187089, acc 0.97\n",
      "2018-04-13T00:17:29.324088: step 367, loss 0.222965, acc 0.97\n",
      "2018-04-13T00:17:30.930920: step 368, loss 0.24037, acc 0.96\n",
      "2018-04-13T00:17:32.534761: step 369, loss 0.276047, acc 0.94\n",
      "2018-04-13T00:17:34.145511: step 370, loss 0.12566, acc 0.99\n",
      "2018-04-13T00:17:35.749888: step 371, loss 0.170239, acc 0.98\n",
      "2018-04-13T00:17:37.356884: step 372, loss 0.202306, acc 0.95\n",
      "2018-04-13T00:17:38.962310: step 373, loss 0.342235, acc 0.91\n",
      "2018-04-13T00:17:40.561615: step 374, loss 0.266928, acc 0.94\n",
      "2018-04-13T00:17:42.168121: step 375, loss 0.30144, acc 0.93\n",
      "2018-04-13T00:17:43.770819: step 376, loss 0.257906, acc 0.95\n",
      "2018-04-13T00:17:45.378458: step 377, loss 0.178745, acc 0.97\n",
      "2018-04-13T00:17:46.983019: step 378, loss 0.196769, acc 0.95\n",
      "2018-04-13T00:17:48.588001: step 379, loss 0.223447, acc 0.95\n",
      "2018-04-13T00:17:50.192738: step 380, loss 0.173614, acc 0.98\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:17:51.801490: step 381, loss 0.323364, acc 0.93\n",
      "2018-04-13T00:17:53.408777: step 382, loss 0.195271, acc 0.97\n",
      "2018-04-13T00:17:55.015744: step 383, loss 0.120112, acc 0.98\n",
      "2018-04-13T00:17:56.620571: step 384, loss 0.232283, acc 0.95\n",
      "2018-04-13T00:17:58.223746: step 385, loss 0.201995, acc 0.96\n",
      "2018-04-13T00:17:59.832415: step 386, loss 0.0933379, acc 0.99\n",
      "2018-04-13T00:18:01.430577: step 387, loss 0.277124, acc 0.93\n",
      "2018-04-13T00:18:03.034560: step 388, loss 0.151955, acc 0.97\n",
      "2018-04-13T00:18:04.643852: step 389, loss 0.226367, acc 0.95\n",
      "2018-04-13T00:18:06.254788: step 390, loss 0.192059, acc 0.97\n",
      "2018-04-13T00:18:07.861914: step 391, loss 0.108359, acc 0.97\n",
      "2018-04-13T00:18:09.472471: step 392, loss 0.235684, acc 0.95\n",
      "2018-04-13T00:18:11.078333: step 393, loss 0.271035, acc 0.94\n",
      "2018-04-13T00:18:12.692949: step 394, loss 0.15365, acc 0.98\n",
      "2018-04-13T00:18:14.292065: step 395, loss 0.234125, acc 0.95\n",
      "2018-04-13T00:18:15.899488: step 396, loss 0.2495, acc 0.95\n",
      "2018-04-13T00:18:17.498769: step 397, loss 0.169563, acc 0.97\n",
      "2018-04-13T00:18:19.103544: step 398, loss 0.160482, acc 0.98\n",
      "2018-04-13T00:18:20.711541: step 399, loss 0.268733, acc 0.93\n",
      "2018-04-13T00:18:22.317924: step 400, loss 0.267688, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:18:27.794036: step 400, loss 0.169273, acc 0.964727\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-400\n",
      "\n",
      "2018-04-13T00:18:29.565156: step 401, loss 0.262192, acc 0.95\n",
      "2018-04-13T00:18:31.172771: step 402, loss 0.178588, acc 0.97\n",
      "2018-04-13T00:18:32.776279: step 403, loss 0.21794, acc 0.95\n",
      "2018-04-13T00:18:34.383784: step 404, loss 0.178539, acc 0.96\n",
      "2018-04-13T00:18:35.992777: step 405, loss 0.191706, acc 0.96\n",
      "2018-04-13T00:18:37.593805: step 406, loss 0.192744, acc 0.96\n",
      "2018-04-13T00:18:39.205590: step 407, loss 0.197295, acc 0.96\n",
      "2018-04-13T00:18:40.812198: step 408, loss 0.345656, acc 0.9\n",
      "2018-04-13T00:18:42.415177: step 409, loss 0.15461, acc 0.98\n",
      "2018-04-13T00:18:44.023182: step 410, loss 0.20484, acc 0.95\n",
      "2018-04-13T00:18:45.627676: step 411, loss 0.225034, acc 0.95\n",
      "2018-04-13T00:18:47.228414: step 412, loss 0.261279, acc 0.94\n",
      "2018-04-13T00:18:48.834218: step 413, loss 0.198316, acc 0.96\n",
      "2018-04-13T00:18:50.440257: step 414, loss 0.119859, acc 0.99\n",
      "2018-04-13T00:18:52.049354: step 415, loss 0.179171, acc 0.96\n",
      "2018-04-13T00:18:53.652744: step 416, loss 0.237163, acc 0.94\n",
      "2018-04-13T00:18:55.261767: step 417, loss 0.121956, acc 0.98\n",
      "2018-04-13T00:18:56.870273: step 418, loss 0.167405, acc 0.98\n",
      "2018-04-13T00:18:58.470711: step 419, loss 0.185174, acc 0.97\n",
      "2018-04-13T00:19:00.074119: step 420, loss 0.121595, acc 0.99\n",
      "2018-04-13T00:19:01.681666: step 421, loss 0.162812, acc 0.96\n",
      "2018-04-13T00:19:03.286957: step 422, loss 0.213296, acc 0.95\n",
      "2018-04-13T00:19:04.889631: step 423, loss 0.225744, acc 0.95\n",
      "2018-04-13T00:19:06.503002: step 424, loss 0.173589, acc 0.95\n",
      "2018-04-13T00:19:08.109415: step 425, loss 0.147635, acc 0.97\n",
      "2018-04-13T00:19:09.711198: step 426, loss 0.261886, acc 0.94\n",
      "2018-04-13T00:19:11.316882: step 427, loss 0.166022, acc 0.96\n",
      "2018-04-13T00:19:12.928321: step 428, loss 0.188761, acc 0.97\n",
      "2018-04-13T00:19:14.532904: step 429, loss 0.24603, acc 0.93\n",
      "2018-04-13T00:19:16.136711: step 430, loss 0.23463, acc 0.94\n",
      "2018-04-13T00:19:17.742108: step 431, loss 0.166211, acc 0.96\n",
      "2018-04-13T00:19:19.347811: step 432, loss 0.245467, acc 0.94\n",
      "2018-04-13T00:19:20.952317: step 433, loss 0.252657, acc 0.95\n",
      "2018-04-13T00:19:22.561713: step 434, loss 0.165291, acc 0.97\n",
      "2018-04-13T00:19:24.160287: step 435, loss 0.175318, acc 0.96\n",
      "2018-04-13T00:19:25.767393: step 436, loss 0.162928, acc 0.96\n",
      "2018-04-13T00:19:27.374378: step 437, loss 0.265198, acc 0.93\n",
      "2018-04-13T00:19:28.983062: step 438, loss 0.183791, acc 0.95\n",
      "2018-04-13T00:19:30.587739: step 439, loss 0.123628, acc 0.98\n",
      "2018-04-13T00:19:32.195800: step 440, loss 0.0943148, acc 0.99\n",
      "2018-04-13T00:19:33.801269: step 441, loss 0.128912, acc 0.96\n",
      "2018-04-13T00:19:35.403917: step 442, loss 0.21194, acc 0.96\n",
      "2018-04-13T00:19:37.011321: step 443, loss 0.257158, acc 0.95\n",
      "2018-04-13T00:19:38.621437: step 444, loss 0.1631, acc 0.97\n",
      "2018-04-13T00:19:40.222345: step 445, loss 0.0846089, acc 0.99\n",
      "2018-04-13T00:19:41.832462: step 446, loss 0.118826, acc 0.98\n",
      "2018-04-13T00:19:43.441231: step 447, loss 0.273794, acc 0.93\n",
      "2018-04-13T00:19:45.050472: step 448, loss 0.162758, acc 0.97\n",
      "2018-04-13T00:19:46.655729: step 449, loss 0.145229, acc 0.96\n",
      "2018-04-13T00:19:48.255964: step 450, loss 0.142828, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:19:53.732347: step 450, loss 0.137027, acc 0.977072\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-450\n",
      "\n",
      "2018-04-13T00:19:55.513047: step 451, loss 0.107812, acc 0.98\n",
      "2018-04-13T00:19:57.127015: step 452, loss 0.199723, acc 0.96\n",
      "2018-04-13T00:19:58.729927: step 453, loss 0.0991236, acc 0.99\n",
      "2018-04-13T00:20:00.339352: step 454, loss 0.146044, acc 0.98\n",
      "2018-04-13T00:20:01.943885: step 455, loss 0.160574, acc 0.96\n",
      "2018-04-13T00:20:03.550776: step 456, loss 0.0953103, acc 0.98\n",
      "2018-04-13T00:20:05.154689: step 457, loss 0.231873, acc 0.95\n",
      "2018-04-13T00:20:06.760015: step 458, loss 0.124896, acc 0.98\n",
      "2018-04-13T00:20:08.370400: step 459, loss 0.309458, acc 0.92\n",
      "2018-04-13T00:20:09.979407: step 460, loss 0.236553, acc 0.92\n",
      "2018-04-13T00:20:11.589050: step 461, loss 0.141864, acc 0.96\n",
      "2018-04-13T00:20:13.194197: step 462, loss 0.212615, acc 0.94\n",
      "2018-04-13T00:20:14.798286: step 463, loss 0.171288, acc 0.96\n",
      "2018-04-13T00:20:16.402052: step 464, loss 0.104978, acc 0.99\n",
      "2018-04-13T00:20:18.011935: step 465, loss 0.181102, acc 0.98\n",
      "2018-04-13T00:20:19.621103: step 466, loss 0.0912741, acc 0.99\n",
      "2018-04-13T00:20:21.232105: step 467, loss 0.119131, acc 0.98\n",
      "2018-04-13T00:20:22.832676: step 468, loss 0.143317, acc 0.97\n",
      "2018-04-13T00:20:24.439082: step 469, loss 0.114435, acc 0.97\n",
      "2018-04-13T00:20:26.045580: step 470, loss 0.151202, acc 0.96\n",
      "2018-04-13T00:20:27.648589: step 471, loss 0.315752, acc 0.92\n",
      "2018-04-13T00:20:29.248676: step 472, loss 0.212754, acc 0.95\n",
      "2018-04-13T00:20:30.854269: step 473, loss 0.0644907, acc 1\n",
      "2018-04-13T00:20:32.456573: step 474, loss 0.124739, acc 0.98\n",
      "2018-04-13T00:20:34.062815: step 475, loss 0.155944, acc 0.96\n",
      "2018-04-13T00:20:35.667421: step 476, loss 0.095564, acc 0.98\n",
      "2018-04-13T00:20:37.273355: step 477, loss 0.150533, acc 0.97\n",
      "2018-04-13T00:20:38.885156: step 478, loss 0.134925, acc 0.98\n",
      "2018-04-13T00:20:40.491863: step 479, loss 0.0761242, acc 0.99\n",
      "2018-04-13T00:20:42.106127: step 480, loss 0.112551, acc 0.99\n",
      "2018-04-13T00:20:43.714645: step 481, loss 0.181459, acc 0.96\n",
      "2018-04-13T00:20:45.320615: step 482, loss 0.145533, acc 0.96\n",
      "2018-04-13T00:20:46.928653: step 483, loss 0.100303, acc 0.99\n",
      "2018-04-13T00:20:48.543502: step 484, loss 0.163865, acc 0.97\n",
      "2018-04-13T00:20:50.143957: step 485, loss 0.128406, acc 0.98\n",
      "2018-04-13T00:20:51.755869: step 486, loss 0.216035, acc 0.95\n",
      "2018-04-13T00:20:53.361605: step 487, loss 0.191133, acc 0.96\n",
      "2018-04-13T00:20:54.969231: step 488, loss 0.252257, acc 0.95\n",
      "2018-04-13T00:20:56.576656: step 489, loss 0.180169, acc 0.95\n",
      "2018-04-13T00:20:58.183191: step 490, loss 0.136944, acc 0.97\n",
      "2018-04-13T00:20:59.794309: step 491, loss 0.284247, acc 0.93\n",
      "2018-04-13T00:21:01.403552: step 492, loss 0.19395, acc 0.95\n",
      "2018-04-13T00:21:03.010241: step 493, loss 0.153331, acc 0.97\n",
      "2018-04-13T00:21:04.622164: step 494, loss 0.190061, acc 0.96\n",
      "2018-04-13T00:21:06.233173: step 495, loss 0.155215, acc 0.96\n",
      "2018-04-13T00:21:07.839157: step 496, loss 0.243486, acc 0.93\n",
      "2018-04-13T00:21:09.452683: step 497, loss 0.195313, acc 0.96\n",
      "2018-04-13T00:21:11.064895: step 498, loss 0.166964, acc 0.96\n",
      "2018-04-13T00:21:12.671140: step 499, loss 0.159706, acc 0.98\n",
      "2018-04-13T00:21:14.277066: step 500, loss 0.152706, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:21:19.690871: step 500, loss 0.124103, acc 0.978836\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-500\n",
      "\n",
      "2018-04-13T00:21:21.470378: step 501, loss 0.135796, acc 0.97\n",
      "2018-04-13T00:21:23.077404: step 502, loss 0.0761437, acc 0.99\n",
      "2018-04-13T00:21:24.677470: step 503, loss 0.217848, acc 0.95\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:21:26.284532: step 504, loss 0.123894, acc 0.98\n",
      "2018-04-13T00:21:27.895295: step 505, loss 0.15587, acc 0.98\n",
      "2018-04-13T00:21:29.502985: step 506, loss 0.127394, acc 0.97\n",
      "2018-04-13T00:21:31.108085: step 507, loss 0.139911, acc 0.96\n",
      "2018-04-13T00:21:32.714056: step 508, loss 0.144783, acc 0.98\n",
      "2018-04-13T00:21:34.316849: step 509, loss 0.107785, acc 0.99\n",
      "2018-04-13T00:21:35.926287: step 510, loss 0.12533, acc 0.98\n",
      "2018-04-13T00:21:37.547350: step 511, loss 0.125451, acc 0.98\n",
      "2018-04-13T00:21:39.161353: step 512, loss 0.0902624, acc 0.98\n",
      "2018-04-13T00:21:40.767879: step 513, loss 0.2449, acc 0.94\n",
      "2018-04-13T00:21:42.385610: step 514, loss 0.149169, acc 0.98\n",
      "2018-04-13T00:21:44.006678: step 515, loss 0.106997, acc 0.98\n",
      "2018-04-13T00:21:45.610836: step 516, loss 0.219084, acc 0.95\n",
      "2018-04-13T00:21:47.222246: step 517, loss 0.0590859, acc 1\n",
      "2018-04-13T00:21:48.825277: step 518, loss 0.195921, acc 0.94\n",
      "2018-04-13T00:21:50.434242: step 519, loss 0.161369, acc 0.96\n",
      "2018-04-13T00:21:52.036906: step 520, loss 0.136484, acc 0.97\n",
      "2018-04-13T00:21:53.644956: step 521, loss 0.16176, acc 0.96\n",
      "2018-04-13T00:21:55.254985: step 522, loss 0.253987, acc 0.93\n",
      "2018-04-13T00:21:56.865597: step 523, loss 0.180728, acc 0.95\n",
      "2018-04-13T00:21:58.478699: step 524, loss 0.186648, acc 0.97\n",
      "2018-04-13T00:22:00.094527: step 525, loss 0.135864, acc 0.97\n",
      "2018-04-13T00:22:01.697688: step 526, loss 0.206968, acc 0.94\n",
      "2018-04-13T00:22:03.310850: step 527, loss 0.214509, acc 0.95\n",
      "2018-04-13T00:22:04.922385: step 528, loss 0.16189, acc 0.96\n",
      "2018-04-13T00:22:06.528031: step 529, loss 0.0946875, acc 0.99\n",
      "2018-04-13T00:22:08.131952: step 530, loss 0.184547, acc 0.96\n",
      "2018-04-13T00:22:09.734754: step 531, loss 0.170217, acc 0.96\n",
      "2018-04-13T00:22:11.341643: step 532, loss 0.0951529, acc 0.99\n",
      "2018-04-13T00:22:12.955976: step 533, loss 0.179852, acc 0.96\n",
      "2018-04-13T00:22:14.571933: step 534, loss 0.147248, acc 0.96\n",
      "2018-04-13T00:22:16.185074: step 535, loss 0.148929, acc 0.97\n",
      "2018-04-13T00:22:17.794566: step 536, loss 0.0715277, acc 0.99\n",
      "2018-04-13T00:22:19.402109: step 537, loss 0.248821, acc 0.94\n",
      "2018-04-13T00:22:21.011856: step 538, loss 0.101314, acc 0.98\n",
      "2018-04-13T00:22:22.626540: step 539, loss 0.0558949, acc 1\n",
      "2018-04-13T00:22:24.232974: step 540, loss 0.151665, acc 0.96\n",
      "2018-04-13T00:22:25.839737: step 541, loss 0.105724, acc 0.97\n",
      "2018-04-13T00:22:27.454337: step 542, loss 0.0608047, acc 1\n",
      "2018-04-13T00:22:29.075110: step 543, loss 0.100051, acc 0.97\n",
      "2018-04-13T00:22:30.686092: step 544, loss 0.26152, acc 0.94\n",
      "2018-04-13T00:22:32.301777: step 545, loss 0.164773, acc 0.98\n",
      "2018-04-13T00:22:33.917075: step 546, loss 0.27174, acc 0.93\n",
      "2018-04-13T00:22:35.525550: step 547, loss 0.251534, acc 0.93\n",
      "2018-04-13T00:22:37.135273: step 548, loss 0.182368, acc 0.96\n",
      "2018-04-13T00:22:38.740789: step 549, loss 0.1267, acc 0.98\n",
      "2018-04-13T00:22:40.349483: step 550, loss 0.24456, acc 0.96\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:22:45.790537: step 550, loss 0.118716, acc 0.982363\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-550\n",
      "\n",
      "2018-04-13T00:22:47.567788: step 551, loss 0.127754, acc 0.96\n",
      "2018-04-13T00:22:49.173031: step 552, loss 0.118362, acc 0.97\n",
      "2018-04-13T00:22:50.782483: step 553, loss 0.0976149, acc 0.98\n",
      "2018-04-13T00:22:52.391042: step 554, loss 0.107698, acc 0.99\n",
      "2018-04-13T00:22:53.992168: step 555, loss 0.190756, acc 0.95\n",
      "2018-04-13T00:22:55.599598: step 556, loss 0.0929628, acc 0.98\n",
      "2018-04-13T00:22:57.213595: step 557, loss 0.1767, acc 0.94\n",
      "2018-04-13T00:22:58.818170: step 558, loss 0.0902179, acc 0.99\n",
      "2018-04-13T00:23:00.427023: step 559, loss 0.0980929, acc 0.98\n",
      "2018-04-13T00:23:02.031679: step 560, loss 0.116735, acc 0.99\n",
      "2018-04-13T00:23:03.643819: step 561, loss 0.178564, acc 0.95\n",
      "2018-04-13T00:23:05.255986: step 562, loss 0.105494, acc 0.98\n",
      "2018-04-13T00:23:06.857766: step 563, loss 0.131125, acc 0.96\n",
      "2018-04-13T00:23:08.467710: step 564, loss 0.0560468, acc 1\n",
      "2018-04-13T00:23:10.073469: step 565, loss 0.108297, acc 0.98\n",
      "2018-04-13T00:23:11.684831: step 566, loss 0.102595, acc 0.98\n",
      "2018-04-13T00:23:13.300464: step 567, loss 0.0928875, acc 0.98\n",
      "2018-04-13T00:23:14.909296: step 568, loss 0.110948, acc 0.98\n",
      "2018-04-13T00:23:16.522686: step 569, loss 0.136488, acc 0.95\n",
      "2018-04-13T00:23:18.136658: step 570, loss 0.125353, acc 0.96\n",
      "2018-04-13T00:23:19.748775: step 571, loss 0.0643066, acc 0.99\n",
      "2018-04-13T00:23:21.357316: step 572, loss 0.273667, acc 0.94\n",
      "2018-04-13T00:23:22.971230: step 573, loss 0.141802, acc 0.97\n",
      "2018-04-13T00:23:24.589437: step 574, loss 0.0979122, acc 0.98\n",
      "2018-04-13T00:23:26.200052: step 575, loss 0.178706, acc 0.96\n",
      "2018-04-13T00:23:27.809123: step 576, loss 0.132982, acc 0.99\n",
      "2018-04-13T00:23:29.414420: step 577, loss 0.138278, acc 0.97\n",
      "2018-04-13T00:23:31.018314: step 578, loss 0.178737, acc 0.97\n",
      "2018-04-13T00:23:32.631019: step 579, loss 0.17857, acc 0.96\n",
      "2018-04-13T00:23:34.243681: step 580, loss 0.133379, acc 0.98\n",
      "2018-04-13T00:23:35.847213: step 581, loss 0.075636, acc 0.98\n",
      "2018-04-13T00:23:37.452754: step 582, loss 0.144173, acc 0.97\n",
      "2018-04-13T00:23:39.059582: step 583, loss 0.10872, acc 0.97\n",
      "2018-04-13T00:23:40.669581: step 584, loss 0.169066, acc 0.97\n",
      "2018-04-13T00:23:42.281726: step 585, loss 0.13789, acc 0.97\n",
      "2018-04-13T00:23:43.889515: step 586, loss 0.0985001, acc 0.99\n",
      "2018-04-13T00:23:45.498448: step 587, loss 0.111982, acc 0.97\n",
      "2018-04-13T00:23:47.107349: step 588, loss 0.271719, acc 0.93\n",
      "2018-04-13T00:23:48.718120: step 589, loss 0.0985195, acc 0.98\n",
      "2018-04-13T00:23:50.324895: step 590, loss 0.114398, acc 0.98\n",
      "2018-04-13T00:23:51.939014: step 591, loss 0.176143, acc 0.96\n",
      "2018-04-13T00:23:53.554557: step 592, loss 0.231659, acc 0.93\n",
      "2018-04-13T00:23:55.167493: step 593, loss 0.0973064, acc 0.98\n",
      "2018-04-13T00:23:56.778690: step 594, loss 0.142244, acc 0.98\n",
      "2018-04-13T00:23:58.389782: step 595, loss 0.0698819, acc 1\n",
      "2018-04-13T00:24:00.000592: step 596, loss 0.118372, acc 0.98\n",
      "2018-04-13T00:24:01.617181: step 597, loss 0.168099, acc 0.95\n",
      "2018-04-13T00:24:03.225780: step 598, loss 0.115276, acc 0.97\n",
      "2018-04-13T00:24:04.832282: step 599, loss 0.245729, acc 0.95\n",
      "2018-04-13T00:24:06.441849: step 600, loss 0.138087, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:24:11.913345: step 600, loss 0.0996649, acc 0.982363\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-600\n",
      "\n",
      "2018-04-13T00:24:13.683104: step 601, loss 0.104644, acc 0.98\n",
      "2018-04-13T00:24:15.297469: step 602, loss 0.0806936, acc 0.98\n",
      "2018-04-13T00:24:16.893974: step 603, loss 0.0688545, acc 0.99\n",
      "2018-04-13T00:24:18.504238: step 604, loss 0.162991, acc 0.96\n",
      "2018-04-13T00:24:20.116702: step 605, loss 0.282939, acc 0.92\n",
      "2018-04-13T00:24:21.724131: step 606, loss 0.052733, acc 1\n",
      "2018-04-13T00:24:23.329046: step 607, loss 0.0830674, acc 0.99\n",
      "2018-04-13T00:24:24.934023: step 608, loss 0.106297, acc 0.98\n",
      "2018-04-13T00:24:26.544352: step 609, loss 0.149716, acc 0.96\n",
      "2018-04-13T00:24:28.153440: step 610, loss 0.0762176, acc 0.98\n",
      "2018-04-13T00:24:29.766860: step 611, loss 0.099322, acc 0.97\n",
      "2018-04-13T00:24:31.378438: step 612, loss 0.143019, acc 0.95\n",
      "2018-04-13T00:24:32.994835: step 613, loss 0.0728527, acc 0.99\n",
      "2018-04-13T00:24:34.607028: step 614, loss 0.0651994, acc 0.99\n",
      "2018-04-13T00:24:36.216334: step 615, loss 0.140881, acc 0.95\n",
      "2018-04-13T00:24:37.823316: step 616, loss 0.196989, acc 0.94\n",
      "2018-04-13T00:24:39.430531: step 617, loss 0.0996449, acc 0.97\n",
      "2018-04-13T00:24:41.037514: step 618, loss 0.0911311, acc 0.97\n",
      "2018-04-13T00:24:42.640042: step 619, loss 0.128184, acc 0.98\n",
      "2018-04-13T00:24:44.245447: step 620, loss 0.0850657, acc 0.98\n",
      "2018-04-13T00:24:45.851415: step 621, loss 0.0987381, acc 0.98\n",
      "2018-04-13T00:24:47.457479: step 622, loss 0.0449496, acc 1\n",
      "2018-04-13T00:24:49.068020: step 623, loss 0.0510447, acc 1\n",
      "2018-04-13T00:24:50.685970: step 624, loss 0.171125, acc 0.96\n",
      "2018-04-13T00:24:52.297551: step 625, loss 0.139956, acc 0.96\n",
      "2018-04-13T00:24:53.906249: step 626, loss 0.116604, acc 0.97\n",
      "2018-04-13T00:24:55.520558: step 627, loss 0.0841485, acc 0.98\n",
      "2018-04-13T00:24:57.129397: step 628, loss 0.0811378, acc 0.99\n",
      "2018-04-13T00:24:58.737853: step 629, loss 0.176223, acc 0.96\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:25:00.345802: step 630, loss 0.102727, acc 0.99\n",
      "2018-04-13T00:25:01.955538: step 631, loss 0.0875015, acc 0.99\n",
      "2018-04-13T00:25:03.564065: step 632, loss 0.0559749, acc 0.99\n",
      "2018-04-13T00:25:05.167712: step 633, loss 0.14655, acc 0.96\n",
      "2018-04-13T00:25:06.775014: step 634, loss 0.173235, acc 0.96\n",
      "2018-04-13T00:25:08.376722: step 635, loss 0.113918, acc 0.97\n",
      "2018-04-13T00:25:09.989864: step 636, loss 0.0604975, acc 1\n",
      "2018-04-13T00:25:11.605763: step 637, loss 0.121602, acc 0.97\n",
      "2018-04-13T00:25:13.216408: step 638, loss 0.0897456, acc 0.98\n",
      "2018-04-13T00:25:14.835716: step 639, loss 0.114762, acc 0.98\n",
      "2018-04-13T00:25:16.443036: step 640, loss 0.0521811, acc 1\n",
      "2018-04-13T00:25:18.054780: step 641, loss 0.125519, acc 0.97\n",
      "2018-04-13T00:25:19.669037: step 642, loss 0.18074, acc 0.96\n",
      "2018-04-13T00:25:21.276144: step 643, loss 0.171845, acc 0.96\n",
      "2018-04-13T00:25:22.889385: step 644, loss 0.210064, acc 0.92\n",
      "2018-04-13T00:25:24.507231: step 645, loss 0.112387, acc 0.97\n",
      "2018-04-13T00:25:26.115976: step 646, loss 0.284365, acc 0.93\n",
      "2018-04-13T00:25:27.733427: step 647, loss 0.132865, acc 0.97\n",
      "2018-04-13T00:25:29.342929: step 648, loss 0.163495, acc 0.96\n",
      "2018-04-13T00:25:30.958950: step 649, loss 0.0997571, acc 0.99\n",
      "2018-04-13T00:25:32.573062: step 650, loss 0.129184, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:25:38.024454: step 650, loss 0.100862, acc 0.984127\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-650\n",
      "\n",
      "2018-04-13T00:25:39.801257: step 651, loss 0.179239, acc 0.97\n",
      "2018-04-13T00:25:41.415284: step 652, loss 0.144182, acc 0.98\n",
      "2018-04-13T00:25:43.017524: step 653, loss 0.143264, acc 0.96\n",
      "2018-04-13T00:25:44.623328: step 654, loss 0.0696197, acc 0.99\n",
      "2018-04-13T00:25:46.234008: step 655, loss 0.0877398, acc 0.99\n",
      "2018-04-13T00:25:47.834523: step 656, loss 0.139155, acc 0.98\n",
      "2018-04-13T00:25:49.437720: step 657, loss 0.121514, acc 0.98\n",
      "2018-04-13T00:25:51.048963: step 658, loss 0.208354, acc 0.96\n",
      "2018-04-13T00:25:52.643708: step 659, loss 0.133832, acc 0.97\n",
      "2018-04-13T00:25:54.255671: step 660, loss 0.0837164, acc 0.99\n",
      "2018-04-13T00:25:55.868539: step 661, loss 0.167502, acc 0.95\n",
      "2018-04-13T00:25:57.471985: step 662, loss 0.0869308, acc 0.97\n",
      "2018-04-13T00:25:59.079536: step 663, loss 0.104729, acc 0.96\n",
      "2018-04-13T00:26:00.686718: step 664, loss 0.105869, acc 0.98\n",
      "2018-04-13T00:26:02.302521: step 665, loss 0.126456, acc 0.97\n",
      "2018-04-13T00:26:03.911463: step 666, loss 0.161593, acc 0.95\n",
      "2018-04-13T00:26:05.517363: step 667, loss 0.0573668, acc 1\n",
      "2018-04-13T00:26:07.130200: step 668, loss 0.0783495, acc 0.99\n",
      "2018-04-13T00:26:08.740534: step 669, loss 0.114357, acc 0.97\n",
      "2018-04-13T00:26:10.354058: step 670, loss 0.142366, acc 0.96\n",
      "2018-04-13T00:26:11.962895: step 671, loss 0.0873539, acc 1\n",
      "2018-04-13T00:26:13.571097: step 672, loss 0.0832452, acc 0.98\n",
      "2018-04-13T00:26:15.177842: step 673, loss 0.163208, acc 0.95\n",
      "2018-04-13T00:26:16.784958: step 674, loss 0.134618, acc 0.97\n",
      "2018-04-13T00:26:18.395214: step 675, loss 0.135812, acc 0.97\n",
      "2018-04-13T00:26:20.012676: step 676, loss 0.189866, acc 0.96\n",
      "2018-04-13T00:26:21.622940: step 677, loss 0.114069, acc 0.97\n",
      "2018-04-13T00:26:23.235023: step 678, loss 0.0641886, acc 0.99\n",
      "2018-04-13T00:26:24.838192: step 679, loss 0.116086, acc 0.97\n",
      "2018-04-13T00:26:26.448590: step 680, loss 0.0681691, acc 0.99\n",
      "2018-04-13T00:26:28.054311: step 681, loss 0.0941565, acc 0.96\n",
      "2018-04-13T00:26:29.661009: step 682, loss 0.224696, acc 0.95\n",
      "2018-04-13T00:26:31.275786: step 683, loss 0.0829616, acc 0.98\n",
      "2018-04-13T00:26:32.889340: step 684, loss 0.111865, acc 0.98\n",
      "2018-04-13T00:26:34.500973: step 685, loss 0.193945, acc 0.97\n",
      "2018-04-13T00:26:36.117058: step 686, loss 0.114512, acc 0.98\n",
      "2018-04-13T00:26:37.717104: step 687, loss 0.0890291, acc 0.99\n",
      "2018-04-13T00:26:39.329082: step 688, loss 0.132228, acc 0.99\n",
      "2018-04-13T00:26:40.944477: step 689, loss 0.107556, acc 0.97\n",
      "2018-04-13T00:26:42.564562: step 690, loss 0.143818, acc 0.97\n",
      "2018-04-13T00:26:44.181376: step 691, loss 0.0883148, acc 0.98\n",
      "2018-04-13T00:26:45.798787: step 692, loss 0.0611598, acc 0.99\n",
      "2018-04-13T00:26:47.408022: step 693, loss 0.193284, acc 0.95\n",
      "2018-04-13T00:26:49.025505: step 694, loss 0.113843, acc 0.98\n",
      "2018-04-13T00:26:50.633999: step 695, loss 0.216339, acc 0.94\n",
      "2018-04-13T00:26:52.248420: step 696, loss 0.0879978, acc 0.98\n",
      "2018-04-13T00:26:53.855006: step 697, loss 0.0645464, acc 1\n",
      "2018-04-13T00:26:55.463115: step 698, loss 0.0764891, acc 0.99\n",
      "2018-04-13T00:26:57.069079: step 699, loss 0.125028, acc 0.98\n",
      "2018-04-13T00:26:58.671832: step 700, loss 0.095159, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:27:04.095816: step 700, loss 0.0858436, acc 0.987654\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-700\n",
      "\n",
      "2018-04-13T00:27:05.873428: step 701, loss 0.0975924, acc 0.98\n",
      "2018-04-13T00:27:07.493303: step 702, loss 0.0968242, acc 0.99\n",
      "2018-04-13T00:27:09.106920: step 703, loss 0.121371, acc 0.99\n",
      "2018-04-13T00:27:10.717407: step 704, loss 0.0764543, acc 0.99\n",
      "2018-04-13T00:27:12.325813: step 705, loss 0.0630527, acc 0.99\n",
      "2018-04-13T00:27:13.938289: step 706, loss 0.0810097, acc 0.99\n",
      "2018-04-13T00:27:15.547216: step 707, loss 0.0506967, acc 1\n",
      "2018-04-13T00:27:17.165686: step 708, loss 0.156428, acc 0.97\n",
      "2018-04-13T00:27:18.778642: step 709, loss 0.0726849, acc 0.99\n",
      "2018-04-13T00:27:20.393965: step 710, loss 0.0782901, acc 0.99\n",
      "2018-04-13T00:27:22.008580: step 711, loss 0.0741286, acc 0.98\n",
      "2018-04-13T00:27:23.612184: step 712, loss 0.110928, acc 0.98\n",
      "2018-04-13T00:27:25.231284: step 713, loss 0.10768, acc 0.97\n",
      "2018-04-13T00:27:26.844514: step 714, loss 0.154176, acc 0.96\n",
      "2018-04-13T00:27:28.455758: step 715, loss 0.147995, acc 0.97\n",
      "2018-04-13T00:27:30.061525: step 716, loss 0.146179, acc 0.98\n",
      "2018-04-13T00:27:31.674474: step 717, loss 0.0826281, acc 0.99\n",
      "2018-04-13T00:27:33.284380: step 718, loss 0.109863, acc 0.99\n",
      "2018-04-13T00:27:34.895430: step 719, loss 0.0759021, acc 0.99\n",
      "2018-04-13T00:27:36.499011: step 720, loss 0.0571651, acc 1\n",
      "2018-04-13T00:27:38.102344: step 721, loss 0.101675, acc 0.97\n",
      "2018-04-13T00:27:39.709044: step 722, loss 0.0645452, acc 0.99\n",
      "2018-04-13T00:27:41.320506: step 723, loss 0.0949178, acc 0.97\n",
      "2018-04-13T00:27:42.928105: step 724, loss 0.0560778, acc 0.99\n",
      "2018-04-13T00:27:44.533385: step 725, loss 0.0677332, acc 0.99\n",
      "2018-04-13T00:27:46.147511: step 726, loss 0.119873, acc 0.97\n",
      "2018-04-13T00:27:47.753003: step 727, loss 0.0541924, acc 1\n",
      "2018-04-13T00:27:49.363230: step 728, loss 0.147223, acc 0.97\n",
      "2018-04-13T00:27:50.966640: step 729, loss 0.0726864, acc 0.99\n",
      "2018-04-13T00:27:52.583541: step 730, loss 0.109963, acc 0.98\n",
      "2018-04-13T00:27:54.202342: step 731, loss 0.113197, acc 0.98\n",
      "2018-04-13T00:27:55.815001: step 732, loss 0.0780139, acc 0.98\n",
      "2018-04-13T00:27:57.431091: step 733, loss 0.110524, acc 0.98\n",
      "2018-04-13T00:27:59.046378: step 734, loss 0.0563996, acc 1\n",
      "2018-04-13T00:28:00.660956: step 735, loss 0.0738247, acc 1\n",
      "2018-04-13T00:28:02.281697: step 736, loss 0.0985518, acc 0.98\n",
      "2018-04-13T00:28:03.890688: step 737, loss 0.121052, acc 0.95\n",
      "2018-04-13T00:28:05.503766: step 738, loss 0.108076, acc 0.99\n",
      "2018-04-13T00:28:07.113053: step 739, loss 0.164274, acc 0.97\n",
      "2018-04-13T00:28:08.720916: step 740, loss 0.104258, acc 0.99\n",
      "2018-04-13T00:28:10.324057: step 741, loss 0.0682257, acc 0.99\n",
      "2018-04-13T00:28:11.927401: step 742, loss 0.303744, acc 0.94\n",
      "2018-04-13T00:28:13.540318: step 743, loss 0.100927, acc 0.98\n",
      "2018-04-13T00:28:15.150164: step 744, loss 0.0841253, acc 0.99\n",
      "2018-04-13T00:28:16.756656: step 745, loss 0.168114, acc 0.97\n",
      "2018-04-13T00:28:18.361653: step 746, loss 0.117961, acc 0.98\n",
      "2018-04-13T00:28:19.968441: step 747, loss 0.0590227, acc 1\n",
      "2018-04-13T00:28:21.578122: step 748, loss 0.0681859, acc 1\n",
      "2018-04-13T00:28:23.196761: step 749, loss 0.0976114, acc 0.99\n",
      "2018-04-13T00:28:24.801811: step 750, loss 0.0925432, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:28:30.218691: step 750, loss 0.0785226, acc 0.989418\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-750\n",
      "\n",
      "2018-04-13T00:28:31.994161: step 751, loss 0.124794, acc 0.98\n",
      "2018-04-13T00:28:33.602711: step 752, loss 0.113435, acc 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:28:35.214767: step 753, loss 0.086914, acc 0.98\n",
      "2018-04-13T00:28:36.819337: step 754, loss 0.144911, acc 0.97\n",
      "2018-04-13T00:28:38.427739: step 755, loss 0.126201, acc 0.97\n",
      "2018-04-13T00:28:40.039488: step 756, loss 0.0799959, acc 0.99\n",
      "2018-04-13T00:28:41.649911: step 757, loss 0.087874, acc 0.98\n",
      "2018-04-13T00:28:43.259362: step 758, loss 0.0599924, acc 1\n",
      "2018-04-13T00:28:44.870182: step 759, loss 0.0590788, acc 0.99\n",
      "2018-04-13T00:28:46.479255: step 760, loss 0.0548369, acc 0.99\n",
      "2018-04-13T00:28:48.090464: step 761, loss 0.0452974, acc 1\n",
      "2018-04-13T00:28:49.701401: step 762, loss 0.082007, acc 0.98\n",
      "2018-04-13T00:28:51.314075: step 763, loss 0.0990955, acc 0.98\n",
      "2018-04-13T00:28:52.933541: step 764, loss 0.063454, acc 0.99\n",
      "2018-04-13T00:28:54.540868: step 765, loss 0.0844904, acc 0.98\n",
      "2018-04-13T00:28:56.155728: step 766, loss 0.11125, acc 0.97\n",
      "2018-04-13T00:28:57.769016: step 767, loss 0.0718185, acc 0.99\n",
      "2018-04-13T00:28:59.373947: step 768, loss 0.133373, acc 0.96\n",
      "2018-04-13T00:29:00.983189: step 769, loss 0.0717175, acc 0.99\n",
      "2018-04-13T00:29:02.592757: step 770, loss 0.0611819, acc 1\n",
      "2018-04-13T00:29:04.201415: step 771, loss 0.0442004, acc 1\n",
      "2018-04-13T00:29:05.812239: step 772, loss 0.187152, acc 0.96\n",
      "2018-04-13T00:29:07.420964: step 773, loss 0.0857236, acc 0.97\n",
      "2018-04-13T00:29:09.021983: step 774, loss 0.050801, acc 1\n",
      "2018-04-13T00:29:10.631759: step 775, loss 0.0517133, acc 1\n",
      "2018-04-13T00:29:12.239333: step 776, loss 0.0996267, acc 0.99\n",
      "2018-04-13T00:29:13.846647: step 777, loss 0.0513173, acc 1\n",
      "2018-04-13T00:29:15.455990: step 778, loss 0.116409, acc 0.98\n",
      "2018-04-13T00:29:17.068126: step 779, loss 0.0980072, acc 0.99\n",
      "2018-04-13T00:29:18.673993: step 780, loss 0.0860773, acc 0.97\n",
      "2018-04-13T00:29:20.282541: step 781, loss 0.105171, acc 0.98\n",
      "2018-04-13T00:29:21.887832: step 782, loss 0.0628237, acc 0.99\n",
      "2018-04-13T00:29:23.497420: step 783, loss 0.117226, acc 0.96\n",
      "2018-04-13T00:29:25.106041: step 784, loss 0.0479727, acc 1\n",
      "2018-04-13T00:29:26.719509: step 785, loss 0.0520987, acc 0.99\n",
      "2018-04-13T00:29:28.336034: step 786, loss 0.068652, acc 0.99\n",
      "2018-04-13T00:29:29.948982: step 787, loss 0.0503088, acc 1\n",
      "2018-04-13T00:29:31.555853: step 788, loss 0.0423908, acc 1\n",
      "2018-04-13T00:29:33.168278: step 789, loss 0.110255, acc 0.98\n",
      "2018-04-13T00:29:34.775197: step 790, loss 0.0538584, acc 1\n",
      "2018-04-13T00:29:36.393374: step 791, loss 0.143187, acc 0.98\n",
      "2018-04-13T00:29:38.005180: step 792, loss 0.0561231, acc 1\n",
      "2018-04-13T00:29:39.618954: step 793, loss 0.137466, acc 0.96\n",
      "2018-04-13T00:29:41.230562: step 794, loss 0.115634, acc 0.97\n",
      "2018-04-13T00:29:42.847796: step 795, loss 0.0516044, acc 1\n",
      "2018-04-13T00:29:44.457912: step 796, loss 0.094743, acc 0.98\n",
      "2018-04-13T00:29:46.065300: step 797, loss 0.0835953, acc 0.98\n",
      "2018-04-13T00:29:47.676787: step 798, loss 0.139492, acc 0.97\n",
      "2018-04-13T00:29:49.284046: step 799, loss 0.147991, acc 0.96\n",
      "2018-04-13T00:29:50.892414: step 800, loss 0.0681238, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:29:56.311495: step 800, loss 0.0658354, acc 0.989418\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-800\n",
      "\n",
      "2018-04-13T00:29:58.095480: step 801, loss 0.0461244, acc 1\n",
      "2018-04-13T00:29:59.709161: step 802, loss 0.0538613, acc 0.99\n",
      "2018-04-13T00:30:01.313281: step 803, loss 0.0479323, acc 1\n",
      "2018-04-13T00:30:02.921313: step 804, loss 0.0660403, acc 0.99\n",
      "2018-04-13T00:30:04.524115: step 805, loss 0.10105, acc 0.98\n",
      "2018-04-13T00:30:06.133941: step 806, loss 0.0396724, acc 1\n",
      "2018-04-13T00:30:07.742293: step 807, loss 0.137224, acc 0.98\n",
      "2018-04-13T00:30:09.352504: step 808, loss 0.124721, acc 0.98\n",
      "2018-04-13T00:30:10.961802: step 809, loss 0.103303, acc 0.99\n",
      "2018-04-13T00:30:12.570599: step 810, loss 0.0724629, acc 0.99\n",
      "2018-04-13T00:30:14.180280: step 811, loss 0.0853003, acc 0.99\n",
      "2018-04-13T00:30:15.796329: step 812, loss 0.0791059, acc 0.98\n",
      "2018-04-13T00:30:17.402697: step 813, loss 0.0428959, acc 1\n",
      "2018-04-13T00:30:19.024991: step 814, loss 0.128745, acc 0.97\n",
      "2018-04-13T00:30:20.637513: step 815, loss 0.0966684, acc 0.99\n",
      "2018-04-13T00:30:22.247661: step 816, loss 0.0737844, acc 0.98\n",
      "2018-04-13T00:30:23.864276: step 817, loss 0.0771996, acc 0.98\n",
      "2018-04-13T00:30:25.471785: step 818, loss 0.082233, acc 0.97\n",
      "2018-04-13T00:30:27.087859: step 819, loss 0.0915425, acc 0.97\n",
      "2018-04-13T00:30:28.703295: step 820, loss 0.150675, acc 0.96\n",
      "2018-04-13T00:30:30.312747: step 821, loss 0.0584608, acc 0.99\n",
      "2018-04-13T00:30:31.922821: step 822, loss 0.0562438, acc 0.99\n",
      "2018-04-13T00:30:33.531110: step 823, loss 0.087926, acc 0.99\n",
      "2018-04-13T00:30:35.143791: step 824, loss 0.0350716, acc 1\n",
      "2018-04-13T00:30:36.752432: step 825, loss 0.0655266, acc 0.99\n",
      "2018-04-13T00:30:38.357398: step 826, loss 0.0523173, acc 0.99\n",
      "2018-04-13T00:30:39.974391: step 827, loss 0.0893683, acc 0.97\n",
      "2018-04-13T00:30:41.588116: step 828, loss 0.0536481, acc 0.99\n",
      "2018-04-13T00:30:43.205246: step 829, loss 0.108959, acc 0.98\n",
      "2018-04-13T00:30:44.819239: step 830, loss 0.0552664, acc 0.99\n",
      "2018-04-13T00:30:46.438117: step 831, loss 0.10008, acc 0.98\n",
      "2018-04-13T00:30:48.052854: step 832, loss 0.0982106, acc 0.98\n",
      "2018-04-13T00:30:49.676394: step 833, loss 0.0659579, acc 0.98\n",
      "2018-04-13T00:30:51.293572: step 834, loss 0.0367687, acc 1\n",
      "2018-04-13T00:30:52.908679: step 835, loss 0.050534, acc 1\n",
      "2018-04-13T00:30:54.525663: step 836, loss 0.113561, acc 0.96\n",
      "2018-04-13T00:30:56.130991: step 837, loss 0.0441799, acc 0.99\n",
      "2018-04-13T00:30:57.742989: step 838, loss 0.0715528, acc 0.99\n",
      "2018-04-13T00:30:59.356856: step 839, loss 0.125763, acc 0.97\n",
      "2018-04-13T00:31:00.977987: step 840, loss 0.0614402, acc 1\n",
      "2018-04-13T00:31:02.586103: step 841, loss 0.103655, acc 0.98\n",
      "2018-04-13T00:31:04.197131: step 842, loss 0.0432172, acc 1\n",
      "2018-04-13T00:31:05.809843: step 843, loss 0.0524773, acc 0.99\n",
      "2018-04-13T00:31:07.426630: step 844, loss 0.0494815, acc 1\n",
      "2018-04-13T00:31:09.034517: step 845, loss 0.122768, acc 0.97\n",
      "2018-04-13T00:31:10.643253: step 846, loss 0.14026, acc 0.96\n",
      "2018-04-13T00:31:12.247513: step 847, loss 0.154345, acc 0.97\n",
      "2018-04-13T00:31:13.862492: step 848, loss 0.0756035, acc 0.98\n",
      "2018-04-13T00:31:15.480598: step 849, loss 0.173195, acc 0.97\n",
      "2018-04-13T00:31:17.083373: step 850, loss 0.11272, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:31:22.547217: step 850, loss 0.0644991, acc 0.989418\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-850\n",
      "\n",
      "2018-04-13T00:31:24.326602: step 851, loss 0.119773, acc 0.98\n",
      "2018-04-13T00:31:25.940121: step 852, loss 0.134939, acc 0.98\n",
      "2018-04-13T00:31:27.555452: step 853, loss 0.0927081, acc 0.98\n",
      "2018-04-13T00:31:29.168554: step 854, loss 0.069723, acc 0.98\n",
      "2018-04-13T00:31:30.782987: step 855, loss 0.0646618, acc 1\n",
      "2018-04-13T00:31:32.390263: step 856, loss 0.0760887, acc 0.98\n",
      "2018-04-13T00:31:33.999358: step 857, loss 0.0656835, acc 0.98\n",
      "2018-04-13T00:31:35.609584: step 858, loss 0.112498, acc 0.98\n",
      "2018-04-13T00:31:37.221847: step 859, loss 0.0566062, acc 0.99\n",
      "2018-04-13T00:31:38.832787: step 860, loss 0.0687316, acc 0.98\n",
      "2018-04-13T00:31:40.438686: step 861, loss 0.104146, acc 0.98\n",
      "2018-04-13T00:31:42.055083: step 862, loss 0.0664697, acc 0.98\n",
      "2018-04-13T00:31:43.670315: step 863, loss 0.101406, acc 0.99\n",
      "2018-04-13T00:31:45.280967: step 864, loss 0.151502, acc 0.97\n",
      "2018-04-13T00:31:46.892538: step 865, loss 0.0742579, acc 0.99\n",
      "2018-04-13T00:31:48.497052: step 866, loss 0.0722445, acc 0.99\n",
      "2018-04-13T00:31:50.103905: step 867, loss 0.0477619, acc 0.99\n",
      "2018-04-13T00:31:51.706237: step 868, loss 0.0439191, acc 1\n",
      "2018-04-13T00:31:53.313467: step 869, loss 0.112501, acc 0.98\n",
      "2018-04-13T00:31:54.920767: step 870, loss 0.101043, acc 0.98\n",
      "2018-04-13T00:31:56.531347: step 871, loss 0.0754909, acc 0.98\n",
      "2018-04-13T00:31:58.139260: step 872, loss 0.0967699, acc 0.98\n",
      "2018-04-13T00:31:59.740336: step 873, loss 0.0765588, acc 0.98\n",
      "2018-04-13T00:32:01.351946: step 874, loss 0.0482479, acc 1\n",
      "2018-04-13T00:32:02.956350: step 875, loss 0.0547631, acc 0.99\n",
      "2018-04-13T00:32:04.566949: step 876, loss 0.114081, acc 0.98\n",
      "2018-04-13T00:32:06.171210: step 877, loss 0.0522187, acc 0.99\n",
      "2018-04-13T00:32:07.781505: step 878, loss 0.040468, acc 1\n",
      "2018-04-13T00:32:09.386433: step 879, loss 0.0818122, acc 0.99\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:32:10.993279: step 880, loss 0.067679, acc 0.97\n",
      "2018-04-13T00:32:12.603885: step 881, loss 0.0603829, acc 0.98\n",
      "2018-04-13T00:32:14.211776: step 882, loss 0.0510811, acc 0.99\n",
      "2018-04-13T00:32:15.827943: step 883, loss 0.136312, acc 0.97\n",
      "2018-04-13T00:32:17.434979: step 884, loss 0.21286, acc 0.97\n",
      "2018-04-13T00:32:19.047409: step 885, loss 0.0864216, acc 0.97\n",
      "2018-04-13T00:32:20.656776: step 886, loss 0.0432801, acc 1\n",
      "2018-04-13T00:32:22.269921: step 887, loss 0.0718389, acc 0.97\n",
      "2018-04-13T00:32:23.887660: step 888, loss 0.146995, acc 0.96\n",
      "2018-04-13T00:32:25.501959: step 889, loss 0.123689, acc 0.97\n",
      "2018-04-13T00:32:27.107552: step 890, loss 0.0929575, acc 0.99\n",
      "2018-04-13T00:32:28.719715: step 891, loss 0.0387415, acc 1\n",
      "2018-04-13T00:32:30.325484: step 892, loss 0.0485311, acc 0.99\n",
      "2018-04-13T00:32:31.937688: step 893, loss 0.109648, acc 0.97\n",
      "2018-04-13T00:32:33.551392: step 894, loss 0.0793303, acc 0.99\n",
      "2018-04-13T00:32:35.152772: step 895, loss 0.0796448, acc 0.97\n",
      "2018-04-13T00:32:36.764766: step 896, loss 0.0698999, acc 0.99\n",
      "2018-04-13T00:32:38.369081: step 897, loss 0.0641948, acc 1\n",
      "2018-04-13T00:32:39.970679: step 898, loss 0.12482, acc 0.98\n",
      "2018-04-13T00:32:41.578843: step 899, loss 0.0572373, acc 1\n",
      "2018-04-13T00:32:43.191396: step 900, loss 0.0528516, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:32:48.589261: step 900, loss 0.0618907, acc 0.991182\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-900\n",
      "\n",
      "2018-04-13T00:32:50.362714: step 901, loss 0.0514893, acc 0.99\n",
      "2018-04-13T00:32:51.962983: step 902, loss 0.080472, acc 0.98\n",
      "2018-04-13T00:32:53.573604: step 903, loss 0.075873, acc 0.98\n",
      "2018-04-13T00:32:55.181376: step 904, loss 0.0670875, acc 0.99\n",
      "2018-04-13T00:32:56.792636: step 905, loss 0.0531354, acc 0.99\n",
      "2018-04-13T00:32:58.405827: step 906, loss 0.0705511, acc 0.99\n",
      "2018-04-13T00:33:00.015860: step 907, loss 0.0505572, acc 0.99\n",
      "2018-04-13T00:33:01.626446: step 908, loss 0.0523806, acc 0.99\n",
      "2018-04-13T00:33:03.241778: step 909, loss 0.0349221, acc 1\n",
      "2018-04-13T00:33:04.847849: step 910, loss 0.113203, acc 0.97\n",
      "2018-04-13T00:33:06.455365: step 911, loss 0.210894, acc 0.96\n",
      "2018-04-13T00:33:08.067404: step 912, loss 0.0343183, acc 1\n",
      "2018-04-13T00:33:09.672547: step 913, loss 0.0522263, acc 0.99\n",
      "2018-04-13T00:33:11.285016: step 914, loss 0.0957429, acc 0.99\n",
      "2018-04-13T00:33:12.891345: step 915, loss 0.0928131, acc 0.99\n",
      "2018-04-13T00:33:14.500049: step 916, loss 0.0885921, acc 0.98\n",
      "2018-04-13T00:33:16.109403: step 917, loss 0.0346702, acc 1\n",
      "2018-04-13T00:33:17.719232: step 918, loss 0.0614517, acc 0.99\n",
      "2018-04-13T00:33:19.330175: step 919, loss 0.0307525, acc 1\n",
      "2018-04-13T00:33:20.937932: step 920, loss 0.0702065, acc 0.98\n",
      "2018-04-13T00:33:22.546034: step 921, loss 0.165879, acc 0.96\n",
      "2018-04-13T00:33:24.152026: step 922, loss 0.0629977, acc 0.99\n",
      "2018-04-13T00:33:25.758490: step 923, loss 0.170008, acc 0.97\n",
      "2018-04-13T00:33:27.369507: step 924, loss 0.0502957, acc 0.99\n",
      "2018-04-13T00:33:28.978530: step 925, loss 0.0882628, acc 0.98\n",
      "2018-04-13T00:33:30.590130: step 926, loss 0.0545612, acc 1\n",
      "2018-04-13T00:33:32.205285: step 927, loss 0.0853926, acc 0.99\n",
      "2018-04-13T00:33:33.815770: step 928, loss 0.0692532, acc 0.98\n",
      "2018-04-13T00:33:35.423937: step 929, loss 0.0670706, acc 0.99\n",
      "2018-04-13T00:33:37.029157: step 930, loss 0.10933, acc 0.97\n",
      "2018-04-13T00:33:38.640765: step 931, loss 0.0763157, acc 0.99\n",
      "2018-04-13T00:33:40.250679: step 932, loss 0.170022, acc 0.97\n",
      "2018-04-13T00:33:41.866118: step 933, loss 0.123285, acc 0.98\n",
      "2018-04-13T00:33:43.480329: step 934, loss 0.062894, acc 0.98\n",
      "2018-04-13T00:33:45.086389: step 935, loss 0.102664, acc 0.98\n",
      "2018-04-13T00:33:46.696303: step 936, loss 0.0441626, acc 0.99\n",
      "2018-04-13T00:33:48.307259: step 937, loss 0.0536075, acc 0.99\n",
      "2018-04-13T00:33:49.917862: step 938, loss 0.0516112, acc 0.99\n",
      "2018-04-13T00:33:51.534050: step 939, loss 0.0453325, acc 0.99\n",
      "2018-04-13T00:33:53.141434: step 940, loss 0.0501067, acc 1\n",
      "2018-04-13T00:33:54.743890: step 941, loss 0.142404, acc 0.98\n",
      "2018-04-13T00:33:56.352747: step 942, loss 0.0769054, acc 0.97\n",
      "2018-04-13T00:33:57.966579: step 943, loss 0.0775758, acc 0.98\n",
      "2018-04-13T00:33:59.573195: step 944, loss 0.150055, acc 0.95\n",
      "2018-04-13T00:34:01.180126: step 945, loss 0.0665267, acc 0.98\n",
      "2018-04-13T00:34:02.787387: step 946, loss 0.12386, acc 0.97\n",
      "2018-04-13T00:34:04.398264: step 947, loss 0.171956, acc 0.98\n",
      "2018-04-13T00:34:06.002673: step 948, loss 0.0534063, acc 0.99\n",
      "2018-04-13T00:34:07.614715: step 949, loss 0.0956302, acc 0.98\n",
      "2018-04-13T00:34:09.224190: step 950, loss 0.0563499, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:34:14.657349: step 950, loss 0.0552387, acc 0.989418\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-950\n",
      "\n",
      "2018-04-13T00:34:16.431041: step 951, loss 0.0909966, acc 0.99\n",
      "2018-04-13T00:34:18.041583: step 952, loss 0.10249, acc 0.98\n",
      "2018-04-13T00:34:19.642225: step 953, loss 0.0381506, acc 1\n",
      "2018-04-13T00:34:21.248421: step 954, loss 0.0457519, acc 0.99\n",
      "2018-04-13T00:34:22.856633: step 955, loss 0.0446869, acc 1\n",
      "2018-04-13T00:34:24.466024: step 956, loss 0.0650514, acc 0.99\n",
      "2018-04-13T00:34:26.070945: step 957, loss 0.0453292, acc 1\n",
      "2018-04-13T00:34:27.676410: step 958, loss 0.0777399, acc 0.98\n",
      "2018-04-13T00:34:29.276770: step 959, loss 0.145761, acc 0.97\n",
      "2018-04-13T00:34:30.887958: step 960, loss 0.070616, acc 0.99\n",
      "2018-04-13T00:34:32.495997: step 961, loss 0.117191, acc 0.98\n",
      "2018-04-13T00:34:34.109676: step 962, loss 0.0794556, acc 0.98\n",
      "2018-04-13T00:34:35.717138: step 963, loss 0.05982, acc 0.99\n",
      "2018-04-13T00:34:37.329268: step 964, loss 0.190854, acc 0.96\n",
      "2018-04-13T00:34:38.945412: step 965, loss 0.0722176, acc 0.99\n",
      "2018-04-13T00:34:40.560622: step 966, loss 0.0623349, acc 1\n",
      "2018-04-13T00:34:42.174937: step 967, loss 0.0681607, acc 0.99\n",
      "2018-04-13T00:34:43.792101: step 968, loss 0.0514051, acc 1\n",
      "2018-04-13T00:34:45.406498: step 969, loss 0.0970361, acc 0.97\n",
      "2018-04-13T00:34:47.016766: step 970, loss 0.0433002, acc 1\n",
      "2018-04-13T00:34:48.626534: step 971, loss 0.126376, acc 0.98\n",
      "2018-04-13T00:34:50.236021: step 972, loss 0.0463703, acc 1\n",
      "2018-04-13T00:34:51.842359: step 973, loss 0.134381, acc 0.97\n",
      "2018-04-13T00:34:53.450776: step 974, loss 0.149833, acc 0.93\n",
      "2018-04-13T00:34:55.058596: step 975, loss 0.0485385, acc 0.99\n",
      "2018-04-13T00:34:56.661977: step 976, loss 0.114362, acc 0.97\n",
      "2018-04-13T00:34:58.270276: step 977, loss 0.0989657, acc 0.98\n",
      "2018-04-13T00:34:59.878437: step 978, loss 0.0719467, acc 0.98\n",
      "2018-04-13T00:35:01.485234: step 979, loss 0.0952977, acc 0.97\n",
      "2018-04-13T00:35:03.093099: step 980, loss 0.117454, acc 0.98\n",
      "2018-04-13T00:35:04.696577: step 981, loss 0.062245, acc 0.99\n",
      "2018-04-13T00:35:06.301444: step 982, loss 0.0954893, acc 0.99\n",
      "2018-04-13T00:35:07.908281: step 983, loss 0.0697469, acc 0.98\n",
      "2018-04-13T00:35:09.507702: step 984, loss 0.0906787, acc 0.98\n",
      "2018-04-13T00:35:11.116179: step 985, loss 0.0476906, acc 1\n",
      "2018-04-13T00:35:12.716948: step 986, loss 0.0660769, acc 0.99\n",
      "2018-04-13T00:35:14.323866: step 987, loss 0.118114, acc 0.98\n",
      "2018-04-13T00:35:15.929502: step 988, loss 0.0413601, acc 1\n",
      "2018-04-13T00:35:17.541418: step 989, loss 0.0537377, acc 1\n",
      "2018-04-13T00:35:19.154045: step 990, loss 0.0955622, acc 0.99\n",
      "2018-04-13T00:35:20.761873: step 991, loss 0.0654068, acc 0.99\n",
      "2018-04-13T00:35:22.367470: step 992, loss 0.0733416, acc 0.99\n",
      "2018-04-13T00:35:23.978754: step 993, loss 0.14723, acc 0.98\n",
      "2018-04-13T00:35:25.581151: step 994, loss 0.13887, acc 0.97\n",
      "2018-04-13T00:35:27.190951: step 995, loss 0.0489957, acc 0.99\n",
      "2018-04-13T00:35:28.802119: step 996, loss 0.048984, acc 0.99\n",
      "2018-04-13T00:35:30.412764: step 997, loss 0.0685133, acc 0.99\n",
      "2018-04-13T00:35:32.022410: step 998, loss 0.0992587, acc 0.99\n",
      "2018-04-13T00:35:33.627004: step 999, loss 0.0446323, acc 1\n",
      "2018-04-13T00:35:35.232023: step 1000, loss 0.0400464, acc 1\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:35:40.689545: step 1000, loss 0.0611314, acc 0.992945\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-1000\n",
      "\n",
      "2018-04-13T00:35:42.468521: step 1001, loss 0.0848294, acc 0.98\n",
      "2018-04-13T00:35:44.079604: step 1002, loss 0.0454014, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:35:45.689537: step 1003, loss 0.0580828, acc 0.99\n",
      "2018-04-13T00:35:47.302094: step 1004, loss 0.0503707, acc 0.99\n",
      "2018-04-13T00:35:48.903786: step 1005, loss 0.121831, acc 0.97\n",
      "2018-04-13T00:35:50.515405: step 1006, loss 0.0437205, acc 1\n",
      "2018-04-13T00:35:52.123909: step 1007, loss 0.0550719, acc 0.99\n",
      "2018-04-13T00:35:53.741296: step 1008, loss 0.0850687, acc 0.99\n",
      "2018-04-13T00:35:55.355682: step 1009, loss 0.098847, acc 0.98\n",
      "2018-04-13T00:35:56.962359: step 1010, loss 0.0644178, acc 0.98\n",
      "2018-04-13T00:35:58.564542: step 1011, loss 0.0557212, acc 0.99\n",
      "2018-04-13T00:36:00.171330: step 1012, loss 0.0971138, acc 0.98\n",
      "2018-04-13T00:36:01.779857: step 1013, loss 0.0621171, acc 0.99\n",
      "2018-04-13T00:36:03.395736: step 1014, loss 0.0462939, acc 1\n",
      "2018-04-13T00:36:05.006780: step 1015, loss 0.0736223, acc 0.98\n",
      "2018-04-13T00:36:06.616934: step 1016, loss 0.148317, acc 0.96\n",
      "2018-04-13T00:36:08.223182: step 1017, loss 0.0667969, acc 0.99\n",
      "2018-04-13T00:36:09.838026: step 1018, loss 0.0389519, acc 0.99\n",
      "2018-04-13T00:36:11.450120: step 1019, loss 0.0725107, acc 0.98\n",
      "2018-04-13T00:36:13.060918: step 1020, loss 0.0414627, acc 0.99\n",
      "2018-04-13T00:36:14.668205: step 1021, loss 0.0528129, acc 0.99\n",
      "2018-04-13T00:36:16.283013: step 1022, loss 0.0907051, acc 0.96\n",
      "2018-04-13T00:36:17.889485: step 1023, loss 0.0366595, acc 1\n",
      "2018-04-13T00:36:19.504653: step 1024, loss 0.0616388, acc 0.98\n",
      "2018-04-13T00:36:21.119100: step 1025, loss 0.105604, acc 0.98\n",
      "2018-04-13T00:36:22.733199: step 1026, loss 0.0386293, acc 0.99\n",
      "2018-04-13T00:36:24.354237: step 1027, loss 0.0414852, acc 0.99\n",
      "2018-04-13T00:36:25.966356: step 1028, loss 0.0550198, acc 0.98\n",
      "2018-04-13T00:36:27.572451: step 1029, loss 0.0702204, acc 0.99\n",
      "2018-04-13T00:36:29.180264: step 1030, loss 0.0709019, acc 0.98\n",
      "2018-04-13T00:36:30.787495: step 1031, loss 0.112377, acc 0.98\n",
      "2018-04-13T00:36:32.401581: step 1032, loss 0.0525729, acc 0.99\n",
      "2018-04-13T00:36:34.012275: step 1033, loss 0.123436, acc 0.98\n",
      "2018-04-13T00:36:35.621513: step 1034, loss 0.0588453, acc 0.99\n",
      "2018-04-13T00:36:37.230348: step 1035, loss 0.0473966, acc 0.99\n",
      "2018-04-13T00:36:38.833751: step 1036, loss 0.140846, acc 0.97\n",
      "2018-04-13T00:36:40.445437: step 1037, loss 0.170168, acc 0.97\n",
      "2018-04-13T00:36:42.061520: step 1038, loss 0.0868028, acc 0.97\n",
      "2018-04-13T00:36:43.667177: step 1039, loss 0.108254, acc 0.97\n",
      "2018-04-13T00:36:45.276863: step 1040, loss 0.0613478, acc 0.99\n",
      "2018-04-13T00:36:46.881323: step 1041, loss 0.0632042, acc 0.99\n",
      "2018-04-13T00:36:48.491909: step 1042, loss 0.1077, acc 0.97\n",
      "2018-04-13T00:36:50.104057: step 1043, loss 0.0378768, acc 1\n",
      "2018-04-13T00:36:51.713973: step 1044, loss 0.0748851, acc 0.99\n",
      "2018-04-13T00:36:53.321559: step 1045, loss 0.0837148, acc 0.98\n",
      "2018-04-13T00:36:54.927247: step 1046, loss 0.104233, acc 0.98\n",
      "2018-04-13T00:36:56.539031: step 1047, loss 0.0425917, acc 1\n",
      "2018-04-13T00:36:58.148671: step 1048, loss 0.0387762, acc 1\n",
      "2018-04-13T00:36:59.754122: step 1049, loss 0.10177, acc 0.97\n",
      "2018-04-13T00:37:01.360513: step 1050, loss 0.112443, acc 0.98\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:37:06.809231: step 1050, loss 0.0595771, acc 0.991182\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-1050\n",
      "\n",
      "2018-04-13T00:37:08.588195: step 1051, loss 0.0359713, acc 1\n",
      "2018-04-13T00:37:10.199003: step 1052, loss 0.0415004, acc 1\n",
      "2018-04-13T00:37:11.805551: step 1053, loss 0.0757143, acc 0.99\n",
      "2018-04-13T00:37:13.415890: step 1054, loss 0.0915035, acc 0.96\n",
      "2018-04-13T00:37:15.029024: step 1055, loss 0.0644611, acc 0.99\n",
      "2018-04-13T00:37:16.650778: step 1056, loss 0.0368818, acc 1\n",
      "2018-04-13T00:37:18.256743: step 1057, loss 0.0349077, acc 1\n",
      "2018-04-13T00:37:19.863988: step 1058, loss 0.059538, acc 0.97\n",
      "2018-04-13T00:37:21.479058: step 1059, loss 0.188059, acc 0.96\n",
      "2018-04-13T00:37:23.088858: step 1060, loss 0.0504649, acc 0.98\n",
      "2018-04-13T00:37:24.701656: step 1061, loss 0.0685327, acc 0.99\n",
      "2018-04-13T00:37:26.315080: step 1062, loss 0.093991, acc 0.98\n",
      "2018-04-13T00:37:27.932449: step 1063, loss 0.0697628, acc 0.98\n",
      "2018-04-13T00:37:29.545550: step 1064, loss 0.0357647, acc 1\n",
      "2018-04-13T00:37:31.158628: step 1065, loss 0.0583247, acc 0.99\n",
      "2018-04-13T00:37:32.776361: step 1066, loss 0.0998037, acc 0.98\n",
      "2018-04-13T00:37:34.392985: step 1067, loss 0.0372982, acc 1\n",
      "2018-04-13T00:37:36.002103: step 1068, loss 0.0513092, acc 1\n",
      "2018-04-13T00:37:37.620099: step 1069, loss 0.0369335, acc 1\n",
      "2018-04-13T00:37:39.233321: step 1070, loss 0.0317865, acc 1\n",
      "2018-04-13T00:37:40.840526: step 1071, loss 0.123851, acc 0.97\n",
      "2018-04-13T00:37:42.451280: step 1072, loss 0.0292248, acc 1\n",
      "2018-04-13T00:37:44.056346: step 1073, loss 0.0985714, acc 0.98\n",
      "2018-04-13T00:37:45.667303: step 1074, loss 0.0644757, acc 0.99\n",
      "2018-04-13T00:37:47.283122: step 1075, loss 0.0807043, acc 0.99\n",
      "2018-04-13T00:37:48.890348: step 1076, loss 0.0328908, acc 1\n",
      "2018-04-13T00:37:50.500737: step 1077, loss 0.0560372, acc 0.98\n",
      "2018-04-13T00:37:52.114644: step 1078, loss 0.0275981, acc 1\n",
      "2018-04-13T00:37:53.726423: step 1079, loss 0.0292604, acc 1\n",
      "2018-04-13T00:37:55.337795: step 1080, loss 0.0664829, acc 0.98\n",
      "2018-04-13T00:37:56.937633: step 1081, loss 0.0723399, acc 0.98\n",
      "2018-04-13T00:37:58.558686: step 1082, loss 0.059367, acc 0.99\n",
      "2018-04-13T00:38:00.166633: step 1083, loss 0.0294097, acc 1\n",
      "2018-04-13T00:38:01.780727: step 1084, loss 0.036234, acc 1\n",
      "2018-04-13T00:38:03.394880: step 1085, loss 0.0473833, acc 0.99\n",
      "2018-04-13T00:38:05.010098: step 1086, loss 0.0366332, acc 1\n",
      "2018-04-13T00:38:06.625016: step 1087, loss 0.043463, acc 1\n",
      "2018-04-13T00:38:08.247668: step 1088, loss 0.107217, acc 0.98\n",
      "2018-04-13T00:38:09.866948: step 1089, loss 0.120657, acc 0.98\n",
      "2018-04-13T00:38:11.479555: step 1090, loss 0.0786593, acc 0.98\n",
      "2018-04-13T00:38:13.094339: step 1091, loss 0.122428, acc 0.96\n",
      "2018-04-13T00:38:14.708138: step 1092, loss 0.0268078, acc 1\n",
      "2018-04-13T00:38:16.320235: step 1093, loss 0.0783386, acc 0.97\n",
      "2018-04-13T00:38:17.932010: step 1094, loss 0.0517986, acc 0.98\n",
      "2018-04-13T00:38:19.543961: step 1095, loss 0.154388, acc 0.97\n",
      "2018-04-13T00:38:21.156061: step 1096, loss 0.0964882, acc 0.98\n",
      "2018-04-13T00:38:22.760619: step 1097, loss 0.0910894, acc 0.98\n",
      "2018-04-13T00:38:24.379582: step 1098, loss 0.105571, acc 0.97\n",
      "2018-04-13T00:38:25.998704: step 1099, loss 0.10651, acc 0.98\n",
      "2018-04-13T00:38:27.617654: step 1100, loss 0.0607531, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-13T00:38:33.029415: step 1100, loss 0.0718529, acc 0.994709\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523578017/checkpoints/model-1100\n",
      "\n",
      "2018-04-13T00:38:34.798752: step 1101, loss 0.0510774, acc 1\n",
      "2018-04-13T00:38:36.405591: step 1102, loss 0.0563023, acc 1\n",
      "2018-04-13T00:38:38.014627: step 1103, loss 0.0539659, acc 1\n",
      "2018-04-13T00:38:39.629139: step 1104, loss 0.0403608, acc 1\n",
      "2018-04-13T00:38:41.248520: step 1105, loss 0.0383101, acc 1\n",
      "2018-04-13T00:38:42.859269: step 1106, loss 0.0634209, acc 0.99\n",
      "2018-04-13T00:38:44.465698: step 1107, loss 0.0665562, acc 0.99\n",
      "2018-04-13T00:38:46.075424: step 1108, loss 0.0804399, acc 0.97\n",
      "2018-04-13T00:38:47.685130: step 1109, loss 0.0320672, acc 1\n",
      "2018-04-13T00:38:49.293462: step 1110, loss 0.0556241, acc 0.98\n",
      "2018-04-13T00:38:50.898575: step 1111, loss 0.050552, acc 0.99\n",
      "2018-04-13T00:38:52.498182: step 1112, loss 0.102888, acc 0.98\n",
      "2018-04-13T00:38:54.110375: step 1113, loss 0.0591079, acc 0.98\n",
      "2018-04-13T00:38:55.722002: step 1114, loss 0.0346257, acc 1\n",
      "2018-04-13T00:38:57.329856: step 1115, loss 0.0334295, acc 1\n",
      "2018-04-13T00:38:58.935450: step 1116, loss 0.0550535, acc 0.98\n",
      "2018-04-13T00:39:00.547980: step 1117, loss 0.0505859, acc 0.99\n",
      "2018-04-13T00:39:02.161238: step 1118, loss 0.131609, acc 0.98\n",
      "2018-04-13T00:39:03.769017: step 1119, loss 0.091786, acc 0.98\n",
      "2018-04-13T00:39:05.372957: step 1120, loss 0.0704065, acc 0.99\n",
      "2018-04-13T00:39:06.978835: step 1121, loss 0.100254, acc 0.99\n",
      "2018-04-13T00:39:08.587664: step 1122, loss 0.0350954, acc 1\n",
      "2018-04-13T00:39:10.203919: step 1123, loss 0.105077, acc 0.98\n",
      "2018-04-13T00:39:11.820721: step 1124, loss 0.101721, acc 0.99\n",
      "2018-04-13T00:39:13.427681: step 1125, loss 0.120754, acc 0.98\n",
      "2018-04-13T00:39:15.035120: step 1126, loss 0.105799, acc 0.98\n",
      "2018-04-13T00:39:16.652485: step 1127, loss 0.0320223, acc 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-13T00:39:17.501863: step 1128, loss 0.0362549, acc 1\n",
      "\n",
      "Test set accuracy:\n",
      "2018-04-13T00:39:24.730829: step 1128, loss 2.6692, acc 0.135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epoch = 1\n",
    "num_checkpoints = 5\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=feature_size, num_classes=3, vocab_size=V,\n",
    "        embedding_size=100, filter_heights=[5,5,5], filter_widths=[5,5,5],num_filters=3,channels_in=[1,10,12]\n",
    "              ,channels_out=[10,12,24]\n",
    "              ,init_scale=0.08, l2_reg_lambda=0.1)\n",
    "\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            \n",
    "        from sklearn.model_selection import train_test_split \n",
    "\n",
    "        \n",
    "        U_train,U_dev,y_train, y_dev = train_test_split(U_sent, pred_label_U[['label_0','label_1','label_2']], \n",
    "                                                        test_size=0.005, random_state=42)\n",
    "        \n",
    "        for j in range(num_epoch):\n",
    "        \n",
    "            t0 = time.time()\n",
    "            total_batches = 0\n",
    "            total_examples = 0\n",
    "        \n",
    "            for (bx, by) in utils.multi_batch_generator(batch_size, U_train, y_train):\n",
    "                train_step(bx,by)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 50 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(U_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % 50 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        print(\"\\nTest set accuracy:\")\n",
    "        dev_step(X_sent_test[0:600], Y_label.iloc[0:600][['label_0','label_1','label_2']], writer=dev_summary_writer)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
