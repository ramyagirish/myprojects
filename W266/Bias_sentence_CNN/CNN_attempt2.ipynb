{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.\"))\n",
    "\n",
    "# Helper libraries\n",
    "from common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sentences = pd.read_csv(\"bias_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    113299\n",
       "label       113299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_annotator(sent, list1, list2):\n",
    "    ind1 = ind2 = len(sent)\n",
    "    if any(word in sent for word in list1):\n",
    "        ind1 = max([sent.lower().find(word) for word in list1])\n",
    "    elif any(word in sent for word in list2):\n",
    "        ind2 = max([sent.lower().find(word) for word in list2])\n",
    "    else:\n",
    "        ind1 = ind2 = len(sent)\n",
    "        \n",
    "    if ind1 < ind2:\n",
    "        return 'liberal'\n",
    "    elif ind2 < ind1:\n",
    "        return 'conservative'\n",
    "    else: \n",
    "        return 'neutral'       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_list = ['abortion',\n",
    " 'congress',\n",
    " 'rights',\n",
    " 'anti',\n",
    " 'law',\n",
    " 'court',\n",
    " 'party',\n",
    " 'government',\n",
    " 'women',\n",
    " 'federal',\n",
    " 'caucuses',\n",
    " 'issue',\n",
    " 'national',\n",
    " 'right',\n",
    " 'people',\n",
    " 'support',\n",
    " 'million',\n",
    " 'health',\n",
    " 'public',\n",
    " 'city',\n",
    " 'make',\n",
    " '000',\n",
    " 'caucus',\n",
    " 'political',\n",
    " 'percent',\n",
    " 'supreme',\n",
    " 'like',\n",
    " 'groups',\n",
    " 'time',\n",
    " 'money',\n",
    " 'legislature',\n",
    " 'say',\n",
    " 'decision',\n",
    " 'civil',\n",
    " 'american',\n",
    " 'life',\n",
    " 'mr',\n",
    " 'bush']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['house',\n",
    " 'committee',\n",
    " 'senator',\n",
    " 'leader',\n",
    " 'democrat',\n",
    " 'approved',\n",
    " 'majority',\n",
    " 'chairman',\n",
    " 'republicans',\n",
    " 'democratic',\n",
    " 'democrats',\n",
    " 'week',\n",
    " 'budget',\n",
    " 'measure',\n",
    " 'assembly',\n",
    " 'legislation',\n",
    " 'members',\n",
    " 'nomination',\n",
    " 'united',\n",
    " 'expected',\n",
    " 'floor',\n",
    " 'white',\n",
    " 'campaign',\n",
    " 'hearings',\n",
    " 'judiciary',\n",
    " 'plan',\n",
    " 'voted',\n",
    " 'leaders',\n",
    " 'finance',\n",
    " 'judge',\n",
    " 'banking',\n",
    " 'reagan',\n",
    " 'governor',\n",
    " 'conference',\n",
    " 'intelligence',\n",
    " 'dole',\n",
    " 'senate',\n",
    " 'republican',\n",
    " 'today',\n",
    " 'president',\n",
    " 'new',\n",
    " 'year',\n",
    " 'passed',\n",
    " 'states',\n",
    " 'administration',\n",
    " 'tax',\n",
    " 'billion',\n",
    " 'years']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database = pd.read_csv(\"ibcData.csv\",sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['liberal', 'conservative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels_ibc = []\n",
    " \n",
    "for i in range(ibc_database.count()[0]):\n",
    "    weak_labels_ibc.append(weak_annotator(ibc_database.iloc[i][0],lib_list,con_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels_bias = []\n",
    "\n",
    "for i in range(bias_sentences.sentence.count()):\n",
    "    weak_labels_bias.append(weak_annotator(bias_sentences.sentence[i],lib_list,con_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bias_sentences = pd.DataFrame(data = nyt_reddit,columns = ['sentence'])\n",
    "bias_sentences['label'] = weak_labels_bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In the last 365 days, we've had problems: crit...</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>As expected, the end of the quarter and the ye...</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This traditional ''window-dressing'' helped sh...</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Last year I was spot-on about the collapse of ...</td>\n",
       "      <td>conservative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THE average consumer's telephone bill has ris...</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence         label\n",
       "0  In the last 365 days, we've had problems: crit...       liberal\n",
       "1  As expected, the end of the quarter and the ye...  conservative\n",
       "2  This traditional ''window-dressing'' helped sh...       liberal\n",
       "3  Last year I was spot-on about the collapse of ...  conservative\n",
       "4   THE average consumer's telephone bill has ris...       liberal"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2847"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('liberal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1016"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "463"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('conservative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database['weak_label'] = weak_labels_ibc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>weak_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forcing middle-class workers to bear a greater...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Because it would not be worthwhile to bring a ...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indeed , Lind argues that high profits and hig...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In fairness , it should be noted that he devot...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psychological tactics are social control techn...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1 weak_label\n",
       "0  Forcing middle-class workers to bear a greater...  liberal    liberal\n",
       "1  Because it would not be worthwhile to bring a ...  liberal    neutral\n",
       "2  Indeed , Lind argues that high profits and hig...  liberal    neutral\n",
       "3  In fairness , it should be noted that he devot...  liberal    liberal\n",
       "4  Psychological tactics are social control techn...  liberal    neutral"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize IBC database\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "X = ibc_database[0]\n",
    "U = bias_sentences['sentence']\n",
    "\n",
    "X_tokens = []\n",
    "U_tokens = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X_tokens.append(tokenizer.tokenize(X[i]))\n",
    "    \n",
    "for i in range(len(U)):\n",
    "    U_tokens.append(tokenizer.tokenize(U[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalize IBC database\n",
    "\n",
    "X_tokens_canon = []\n",
    "U_tokens_canon = []\n",
    "\n",
    "for i in range(len(X_tokens)):\n",
    "    X_tokens_canon.append(utils.canonicalize_words(X_tokens[i]))\n",
    "    \n",
    "for i in range(len(U_tokens)):\n",
    "    U_tokens_canon.append(utils.canonicalize_words(U_tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of lists consisting of canonicalized sentences\n",
    "\n",
    "X_final = list(itertools.chain.from_iterable(X_tokens_canon))\n",
    "U_final = list(itertools.chain.from_iterable(U_tokens_canon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15721\n",
      "54687\n"
     ]
    }
   ],
   "source": [
    "# number of unique tokens in database\n",
    "print(len(set(X_final)))\n",
    "print(len(set(U_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigrams, could be used for creating distribution plots\n",
    "\n",
    "unigram_counts_X = collections.Counter()\n",
    "unigram_counts_U = collections.Counter()\n",
    "\n",
    "for word in X_final:  \n",
    "    unigram_counts_X[word] += 1\n",
    "    \n",
    "for word in U_final:  \n",
    "    unigram_counts_U[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_X = unigram_counts_X.keys()\n",
    "\n",
    "counts_X = []\n",
    "for w in words_X:\n",
    "    counts_X.append(unigram_counts_X[w])\n",
    "    \n",
    "words_U = unigram_counts_U.keys()\n",
    "\n",
    "counts_U = []\n",
    "for w in words_U:\n",
    "    counts_U.append(unigram_counts_U[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the words according to their counts\n",
    "\n",
    "unigram_list_X = [x for _,x in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "unigram_count_X = [y for y,_ in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "\n",
    "unigram_list_U = [x for _,x in sorted(zip(counts_U,words_U),  reverse=True)]\n",
    "unigram_count_U = [y for y,_ in sorted(zip(counts_U,words_U),  reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"950f71b0-9b1f-4eab-8634-85eb006675f9\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[toinsert.length - 1]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '950f71b0-9b1f-4eab-8634-85eb006675f9' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '950f71b0-9b1f-4eab-8634-85eb006675f9' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.15.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.15.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.15.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"950f71b0-9b1f-4eab-8634-85eb006675f9\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of unigrams\n",
    "\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_X, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"69d58031-ac86-4376-997a-a78c6e41cc32\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"53803dbe-b402-4aa7-8561-590279c7622b\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"01ad0bd5-c14e-486e-9478-16f46f9b32c9\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"top\"],\"data\":{\"top\":[9943,9672,5550,5241,5165,4325,3054,2841,2762,1811,1593,1249,1106,1071,1046,1014,942,881,832,827,793,744,741,722,712,680,658,640,602,564,526,522,521,516,511,489,485,484,479,475,469,456,440,440,432,420,414,406,394,384,374,371,350,345,335,329,323,318,317,313,296,295,292,289,288,287,285,284,282,280,277,261,259,257,256,251,249,244,241,240,236,235,230,226,225,223,222,217,213,213,210,207,205,203,203,203,202,200,197,195],\"x\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]},\"selected\":null,\"selection_policy\":null},\"id\":\"7f92ce1a-8691-4d69-891e-f456bc9eb3c2\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"12dc492d-a6a3-4b33-bf67-2819fa57e703\",\"type\":\"PanTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"45f618e4-68dc-4add-a4a4-fa3cadf1f187\",\"type\":\"VBar\"},{\"attributes\":{\"formatter\":{\"id\":\"c0d74494-d16f-4f36-bcf0-3ddbbe07d9ed\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"8b38e057-d96c-459e-be37-38015d9a5034\",\"type\":\"CategoricalTicker\"}},\"id\":\"55cf929f-2131-4c3a-972b-972381236510\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"30a2b55e-c123-46d2-972f-b05bdaf6660d\",\"type\":\"LinearScale\"},{\"attributes\":{\"below\":[{\"id\":\"55cf929f-2131-4c3a-972b-972381236510\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"e2a71942-3cae-4a82-a68c-0d32d875f8f7\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"55cf929f-2131-4c3a-972b-972381236510\",\"type\":\"CategoricalAxis\"},{\"id\":\"eeb59b59-54f3-4e85-a5cb-441cb1987c8c\",\"type\":\"Grid\"},{\"id\":\"e2a71942-3cae-4a82-a68c-0d32d875f8f7\",\"type\":\"LinearAxis\"},{\"id\":\"5cd037dd-f483-4b65-865a-53e077be2056\",\"type\":\"Grid\"},{\"id\":\"9f7b833e-aa70-4b3b-80a1-13fb1224e923\",\"type\":\"BoxAnnotation\"},{\"id\":\"245d6ae0-a08a-4a0a-a70c-a1ef3798b003\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"b0a5bfbf-c634-4080-a33b-13423e70b4fa\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"77cc2342-f742-4ee0-9afc-d9d9305fbbdf\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"738b7b50-cbdc-4ea2-a3c5-860cd0752d67\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"cc07e86d-e117-4f2e-9f3a-60e9fcb13cd4\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"e845594a-83b7-456e-b334-a05697c70b5b\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"30a2b55e-c123-46d2-972f-b05bdaf6660d\",\"type\":\"LinearScale\"}},\"id\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"01ad0bd5-c14e-486e-9478-16f46f9b32c9\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bbb5cea4-874f-4037-8d31-2fe768fddc15\",\"type\":\"BasicTicker\"}},\"id\":\"e2a71942-3cae-4a82-a68c-0d32d875f8f7\",\"type\":\"LinearAxis\"},{\"attributes\":{},\"id\":\"2f062354-e37e-481c-bbc2-d0f2d0b60bdd\",\"type\":\"HelpTool\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"bbb5cea4-874f-4037-8d31-2fe768fddc15\",\"type\":\"BasicTicker\"}},\"id\":\"5cd037dd-f483-4b65-865a-53e077be2056\",\"type\":\"Grid\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"12dc492d-a6a3-4b33-bf67-2819fa57e703\",\"type\":\"PanTool\"},{\"id\":\"7e5c97c5-ad69-4eb8-9894-570cd5a1ae9f\",\"type\":\"WheelZoomTool\"},{\"id\":\"dd105e3b-0cbc-4a73-b237-92ac7d8ba89b\",\"type\":\"BoxZoomTool\"},{\"id\":\"29c92eb8-8583-40eb-aec5-fc7c0b8599a2\",\"type\":\"SaveTool\"},{\"id\":\"a856ac19-cad0-45b3-a594-04f06b210715\",\"type\":\"ResetTool\"},{\"id\":\"2f062354-e37e-481c-bbc2-d0f2d0b60bdd\",\"type\":\"HelpTool\"},{\"id\":\"b992a2a7-5c8d-44d2-b3a6-3ef1f5dbd0f8\",\"type\":\"HoverTool\"}]},\"id\":\"77cc2342-f742-4ee0-9afc-d9d9305fbbdf\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"bbb5cea4-874f-4037-8d31-2fe768fddc15\",\"type\":\"BasicTicker\"},{\"attributes\":{\"data_source\":{\"id\":\"7f92ce1a-8691-4d69-891e-f456bc9eb3c2\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"45f618e4-68dc-4add-a4a4-fa3cadf1f187\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"79021ca0-b0cc-422e-b236-4699c8759011\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"255468b6-6279-4cdc-902a-df292522c4f6\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"efa0c526-b5f1-4b72-978a-7fc469b5e5dc\",\"type\":\"CDSView\"}},\"id\":\"245d6ae0-a08a-4a0a-a70c-a1ef3798b003\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"8b38e057-d96c-459e-be37-38015d9a5034\",\"type\":\"CategoricalTicker\"}},\"id\":\"eeb59b59-54f3-4e85-a5cb-441cb1987c8c\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"end\":11931.6,\"start\":0},\"id\":\"e845594a-83b7-456e-b334-a05697c70b5b\",\"type\":\"DataRange1d\"},{\"attributes\":{\"source\":{\"id\":\"7f92ce1a-8691-4d69-891e-f456bc9eb3c2\",\"type\":\"ColumnDataSource\"}},\"id\":\"efa0c526-b5f1-4b72-978a-7fc469b5e5dc\",\"type\":\"CDSView\"},{\"attributes\":{},\"id\":\"c0d74494-d16f-4f36-bcf0-3ddbbe07d9ed\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{},\"id\":\"29c92eb8-8583-40eb-aec5-fc7c0b8599a2\",\"type\":\"SaveTool\"},{\"attributes\":{\"callback\":null,\"factors\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]},\"id\":\"738b7b50-cbdc-4ea2-a3c5-860cd0752d67\",\"type\":\"FactorRange\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"9f7b833e-aa70-4b3b-80a1-13fb1224e923\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"79021ca0-b0cc-422e-b236-4699c8759011\",\"type\":\"VBar\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"245d6ae0-a08a-4a0a-a70c-a1ef3798b003\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"b992a2a7-5c8d-44d2-b3a6-3ef1f5dbd0f8\",\"type\":\"HoverTool\"},{\"attributes\":{\"overlay\":{\"id\":\"9f7b833e-aa70-4b3b-80a1-13fb1224e923\",\"type\":\"BoxAnnotation\"}},\"id\":\"dd105e3b-0cbc-4a73-b237-92ac7d8ba89b\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"a856ac19-cad0-45b3-a594-04f06b210715\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"7e5c97c5-ad69-4eb8-9894-570cd5a1ae9f\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"255468b6-6279-4cdc-902a-df292522c4f6\",\"type\":\"VBar\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"b0a5bfbf-c634-4080-a33b-13423e70b4fa\",\"type\":\"Title\"},{\"attributes\":{},\"id\":\"8b38e057-d96c-459e-be37-38015d9a5034\",\"type\":\"CategoricalTicker\"},{\"attributes\":{},\"id\":\"cc07e86d-e117-4f2e-9f3a-60e9fcb13cd4\",\"type\":\"CategoricalScale\"}],\"root_ids\":[\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.15\"}};\n",
       "  var render_items = [{\"docid\":\"53803dbe-b402-4aa7-8561-590279c7622b\",\"elementid\":\"69d58031-ac86-4376-997a-a78c6e41cc32\",\"modelid\":\"99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "99f1b75e-1e4f-4677-b8c4-c2b8fad1ec88"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_X[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_X[:nplot], width=0.4, top=unigram_count_X[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_X)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_U, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"89740129-c4e6-4fe0-86f2-9dd1718a37f6\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"b303459e-5446-488e-aad9-756bab8c91df\":{\"roots\":{\"references\":[{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"37ddde2b-9fd6-4811-a62f-43820dd38788\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"20af1368-87e9-4f75-b52e-82ae735bb187\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"e3a96452-819e-4b43-94ed-4aa1360ba166\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"6a715996-8bea-4319-bd1a-6417fa36990a\",\"type\":\"VBar\"},{\"attributes\":{\"source\":{\"id\":\"e5f4333c-e87e-42dc-80dc-377367ae9ba8\",\"type\":\"ColumnDataSource\"}},\"id\":\"70bfc3eb-cbd8-450d-b6a5-554dee9257da\",\"type\":\"CDSView\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"3adaf2a8-a8c2-4caf-a499-760169d18d92\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"580a63d7-8a2d-416e-8d75-41c44034fc92\",\"type\":\"Title\"},{\"attributes\":{\"data_source\":{\"id\":\"e5f4333c-e87e-42dc-80dc-377367ae9ba8\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"6a715996-8bea-4319-bd1a-6417fa36990a\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"7f5ee648-cba0-42be-8255-96b02ff5efd8\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"85a55ce9-6d69-4175-80ae-70852918f5d1\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"70bfc3eb-cbd8-450d-b6a5-554dee9257da\",\"type\":\"CDSView\"}},\"id\":\"37ddde2b-9fd6-4811-a62f-43820dd38788\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"a318ae01-3e40-4990-a787-84a2f1f1d12b\",\"type\":\"LinearScale\"},{\"attributes\":{\"below\":[{\"id\":\"5ab66b97-3d83-47c3-9d3e-75b2f5a934e1\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"d424b26b-24ea-4358-bc9f-dd1f6661efa7\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"5ab66b97-3d83-47c3-9d3e-75b2f5a934e1\",\"type\":\"CategoricalAxis\"},{\"id\":\"289ff66a-032d-4b18-ba0c-32c30f12d44e\",\"type\":\"Grid\"},{\"id\":\"d424b26b-24ea-4358-bc9f-dd1f6661efa7\",\"type\":\"LinearAxis\"},{\"id\":\"a4ae2a45-0a72-4093-a2c4-99c36228be15\",\"type\":\"Grid\"},{\"id\":\"3adaf2a8-a8c2-4caf-a499-760169d18d92\",\"type\":\"BoxAnnotation\"},{\"id\":\"37ddde2b-9fd6-4811-a62f-43820dd38788\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"580a63d7-8a2d-416e-8d75-41c44034fc92\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"0679bed5-7181-48c6-ba2d-0765450f9a9c\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"5aead797-a549-4a79-8845-cab9b7a6dfac\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"d0566817-ba58-48ad-8e1c-47b5740e73d2\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"674c4029-d43a-4044-8f13-2e8eb3bb21a2\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"a318ae01-3e40-4990-a787-84a2f1f1d12b\",\"type\":\"LinearScale\"}},\"id\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{},\"id\":\"b4307aba-c9e2-4365-b04b-1ced1f700eb4\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"3bb4d4b0-3295-44f8-b435-4468701c7033\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"30e9b549-3ef4-42be-83e2-53ca32cb9a95\",\"type\":\"PanTool\"},{\"id\":\"46254436-15aa-412e-b379-ec767e7f9666\",\"type\":\"WheelZoomTool\"},{\"id\":\"fab5dfc7-5a5e-48e5-8779-ad913a71b8d4\",\"type\":\"BoxZoomTool\"},{\"id\":\"b80c1ff8-a43d-442b-ae1a-aca81daf0a4a\",\"type\":\"SaveTool\"},{\"id\":\"b4307aba-c9e2-4365-b04b-1ced1f700eb4\",\"type\":\"ResetTool\"},{\"id\":\"adbef8f3-fa15-464b-81d3-d02aa0def43e\",\"type\":\"HelpTool\"},{\"id\":\"20af1368-87e9-4f75-b52e-82ae735bb187\",\"type\":\"HoverTool\"}]},\"id\":\"0679bed5-7181-48c6-ba2d-0765450f9a9c\",\"type\":\"Toolbar\"},{\"attributes\":{\"callback\":null,\"end\":315195.6,\"start\":0},\"id\":\"674c4029-d43a-4044-8f13-2e8eb3bb21a2\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"9272f12c-ff40-4ffb-bed4-1611240ec9d2\",\"type\":\"BasicTicker\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9272f12c-ff40-4ffb-bed4-1611240ec9d2\",\"type\":\"BasicTicker\"}},\"id\":\"a4ae2a45-0a72-4093-a2c4-99c36228be15\",\"type\":\"Grid\"},{\"attributes\":{},\"id\":\"d0566817-ba58-48ad-8e1c-47b5740e73d2\",\"type\":\"CategoricalScale\"},{\"attributes\":{},\"id\":\"46254436-15aa-412e-b379-ec767e7f9666\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"e3a96452-819e-4b43-94ed-4aa1360ba166\",\"type\":\"CategoricalTicker\"}},\"id\":\"289ff66a-032d-4b18-ba0c-32c30f12d44e\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"factors\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]},\"id\":\"5aead797-a549-4a79-8845-cab9b7a6dfac\",\"type\":\"FactorRange\"},{\"attributes\":{},\"id\":\"b80c1ff8-a43d-442b-ae1a-aca81daf0a4a\",\"type\":\"SaveTool\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"85a55ce9-6d69-4175-80ae-70852918f5d1\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"30e9b549-3ef4-42be-83e2-53ca32cb9a95\",\"type\":\"PanTool\"},{\"attributes\":{\"formatter\":{\"id\":\"3bb4d4b0-3295-44f8-b435-4468701c7033\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"e3a96452-819e-4b43-94ed-4aa1360ba166\",\"type\":\"CategoricalTicker\"}},\"id\":\"5ab66b97-3d83-47c3-9d3e-75b2f5a934e1\",\"type\":\"CategoricalAxis\"},{\"attributes\":{},\"id\":\"2f710e99-710a-4a67-8d81-812b06a0135c\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"2f710e99-710a-4a67-8d81-812b06a0135c\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"9272f12c-ff40-4ffb-bed4-1611240ec9d2\",\"type\":\"BasicTicker\"}},\"id\":\"d424b26b-24ea-4358-bc9f-dd1f6661efa7\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"x\",\"top\"],\"data\":{\"top\":[262663,175107,112277,101315,97575,86820,70380,68361,59032,46670,44546,38023,34077,29305,26325,23835,21807,20233,19947,19424,18851,17921,17222,17030,16010,15420,15128,14976,14823,14735,14392,13661,13363,13160,12929,12329,12124,11678,11527,11527,11479,10724,9889,9341,8894,8630,8517,8503,8382,8310,8236,7688,7671,7642,7580,7428,7302,6925,6912,6435,6331,6101,6051,6030,5871,5867,5847,5705,5677,5584,5583,5326,5303,5277,5077,4877,4833,4785,4757,4679,4650,4629,4591,4584,4577,4567,4464,4424,4346,4296,4242,4172,4168,4088,4060,4025,3934,3927,3924,3862],\"x\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]},\"selected\":null,\"selection_policy\":null},\"id\":\"e5f4333c-e87e-42dc-80dc-377367ae9ba8\",\"type\":\"ColumnDataSource\"},{\"attributes\":{\"overlay\":{\"id\":\"3adaf2a8-a8c2-4caf-a499-760169d18d92\",\"type\":\"BoxAnnotation\"}},\"id\":\"fab5dfc7-5a5e-48e5-8779-ad913a71b8d4\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"adbef8f3-fa15-464b-81d3-d02aa0def43e\",\"type\":\"HelpTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"7f5ee648-cba0-42be-8255-96b02ff5efd8\",\"type\":\"VBar\"}],\"root_ids\":[\"4937591d-e8a4-48c7-b6df-4120519f8ff3\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.15\"}};\n",
       "  var render_items = [{\"docid\":\"b303459e-5446-488e-aad9-756bab8c91df\",\"elementid\":\"89740129-c4e6-4fe0-86f2-9dd1718a37f6\",\"modelid\":\"4937591d-e8a4-48c7-b6df-4120519f8ff3\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "4937591d-e8a4-48c7-b6df-4120519f8ff3"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_U[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_U[:nplot], width=0.4, top=unigram_count_U[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_U)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially we set vocabulary size equal to total number of unique tokens in the two dataset\n",
    "\n",
    "mergelist = set(unigram_list_X + unigram_list_U)\n",
    "\n",
    "V = len(mergelist)\n",
    "\n",
    "mergelist_XU = X_final + U_final\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(mergelist_XU, size=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum length of sentences\n",
    "lengths_X = map(len,X_tokens_canon)\n",
    "lengths_U = map(len,U_tokens_canon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_X = list(lengths_X)\n",
    "lengths_U = list(lengths_U)\n",
    "max_len1 = max(lengths_X)\n",
    "max_len2 = max(lengths_U)\n",
    "max_len = max(max_len1,max_len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = []\n",
    "U_ids = []\n",
    "for i in range(len(X_tokens_canon)):\n",
    "    X_ids.append(vocab.words_to_ids(X_tokens_canon[i]))\n",
    "    \n",
    "for i in range(len(U_tokens_canon)):\n",
    "    U_ids.append(vocab.words_to_ids(U_tokens_canon[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final padded sentences\n",
    "X_sent, ns = utils.pad_np_array(X_ids, max_len=max_len1, pad_id=0)\n",
    "U_sent, u_ns = utils.pad_np_array(U_ids, max_len=max_len2, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4326, 86)\n",
      "(113299, 493)\n"
     ]
    }
   ],
   "source": [
    "print(X_sent.shape)\n",
    "print(U_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ibc_database[1]\n",
    "Y_u = bias_sentences['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(Y)\n",
    "Y_u = list(Y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final labels\n",
    "Y_label = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] == 'liberal':\n",
    "        Y_label.append(0)\n",
    "    elif Y[i] == 'neutral':\n",
    "        Y_label.append(1)\n",
    "    else:\n",
    "        Y_label.append(2)\n",
    "\n",
    "Y_u_label = []\n",
    "\n",
    "for i in range(len(Y_u)):\n",
    "    if Y_u[i] == 'liberal':\n",
    "        Y_u_label.append(0)\n",
    "    elif Y_u[i] == 'neutral':\n",
    "        Y_u_label.append(1)\n",
    "    else:\n",
    "        Y_u_label.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_w = ibc_database['weak_label']\n",
    "\n",
    "Y_w =list(Y_w)\n",
    "Y_weak = []\n",
    "for i in range(len(Y)):\n",
    "    if Y_w[i] == 'liberal':\n",
    "        Y_weak.append(0)\n",
    "    elif Y_w[i] == 'neutral':\n",
    "        Y_weak.append(1)\n",
    "    else:\n",
    "        Y_weak.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = []\n",
    "for i in range(X_sent.shape[0]):\n",
    "    X_sent_w.append(np.append(X_sent[i], [Y_weak[i],ns[i]]))\n",
    "\n",
    "U_sent_w = []\n",
    "for i in range(U_sent.shape[0]):\n",
    "    U_sent_w.append(np.append(U_sent[i], [u_ns[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = np.reshape(X_sent_w, newshape= (X_sent.shape[0],X_sent.shape[1]+2))\n",
    "U_sent_w = np.reshape(U_sent_w, newshape= (U_sent.shape[0],U_sent.shape[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113299"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Y_u_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train,X_test,y_train, y_test = train_test_split(X_sent_w, Y_label, test_size=0.2, random_state=42)\n",
    "U_train,U_test,y_u_train, y_u_test = train_test_split(U_sent_w, Y_u_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_train = []\n",
    "ns_test = []\n",
    "\n",
    "ns_u_train = []\n",
    "ns_u_test = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    ns_train.append(X_train[i][-1])\n",
    "for i in range(X_test.shape[0]):\n",
    "    ns_test.append(X_test[i][-1])\n",
    "    \n",
    "for i in range(U_train.shape[0]):\n",
    "    ns_u_train.append(U_train[i][-1])\n",
    "for i in range(U_test.shape[0]):\n",
    "    ns_u_test.append(U_test[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n_train = np.delete(X_train,len(X_train[1])-1,axis=1)\n",
    "X_n_test  = np.delete(X_test,len(X_test[1])-1,axis=1)\n",
    "\n",
    "U_n_train = np.delete(U_train,len(U_train[1])-1,axis=1)\n",
    "U_n_test  = np.delete(U_test,len(U_test[1])-1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our model\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(name = 'W_embed', shape=[V,embed_dim],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_)\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        \n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation,    \n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                             , bias_initializer=tf.zeros_initializer(), name = 'Hidden_%d'%i)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.nn.dropout(h_, keep_prob=dropout_rate)\n",
    "            \n",
    "        #if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(inputs = h_, rate=dropout_rate, training = is_training)\n",
    "        \n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,is_training=None,\n",
    "                **unused_kw):\n",
    "    \n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    \n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001) \n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],dtype=tf.float32), -1)\n",
    "    xs_ =  mask_ * xs_\n",
    "    x_ = tf.reduce_sum(xs_,axis = 1)\n",
    "    h_ = fully_connected_layers(x_, hidden_dims, activation=tf.tanh, dropout_rate=dropout_rate,is_training=is_training)   \n",
    "        \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500 examples, moving-average loss 1.41\n",
      "1,000 examples, moving-average loss 1.43\n",
      "1,500 examples, moving-average loss 1.33\n",
      "2,000 examples, moving-average loss 1.38\n",
      "2,500 examples, moving-average loss 1.43\n",
      "3,000 examples, moving-average loss 1.40\n",
      "Completed no. 1 epoch in 0:00:05\n",
      "  500 examples, moving-average loss 1.03\n",
      "1,000 examples, moving-average loss 1.06\n",
      "1,500 examples, moving-average loss 1.02\n",
      "2,000 examples, moving-average loss 1.04\n",
      "2,500 examples, moving-average loss 1.22\n",
      "3,000 examples, moving-average loss 1.16\n",
      "Completed no. 2 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.79\n",
      "1,000 examples, moving-average loss 0.77\n",
      "1,500 examples, moving-average loss 0.75\n",
      "2,000 examples, moving-average loss 0.71\n",
      "2,500 examples, moving-average loss 0.89\n",
      "3,000 examples, moving-average loss 0.78\n",
      "Completed no. 3 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.59\n",
      "1,000 examples, moving-average loss 0.56\n",
      "1,500 examples, moving-average loss 0.53\n",
      "2,000 examples, moving-average loss 0.52\n",
      "2,500 examples, moving-average loss 0.62\n",
      "3,000 examples, moving-average loss 0.50\n",
      "Completed no. 4 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.47\n",
      "1,000 examples, moving-average loss 0.44\n",
      "1,500 examples, moving-average loss 0.41\n",
      "2,000 examples, moving-average loss 0.41\n",
      "2,500 examples, moving-average loss 0.48\n",
      "3,000 examples, moving-average loss 0.38\n",
      "Completed no. 5 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.41\n",
      "1,000 examples, moving-average loss 0.39\n",
      "1,500 examples, moving-average loss 0.35\n",
      "2,000 examples, moving-average loss 0.37\n",
      "2,500 examples, moving-average loss 0.43\n",
      "3,000 examples, moving-average loss 0.34\n",
      "Completed no. 6 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.38\n",
      "1,000 examples, moving-average loss 0.36\n",
      "1,500 examples, moving-average loss 0.33\n",
      "2,000 examples, moving-average loss 0.34\n",
      "2,500 examples, moving-average loss 0.41\n",
      "3,000 examples, moving-average loss 0.32\n",
      "Completed no. 7 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.37\n",
      "1,000 examples, moving-average loss 0.35\n",
      "1,500 examples, moving-average loss 0.32\n",
      "2,000 examples, moving-average loss 0.33\n",
      "2,500 examples, moving-average loss 0.39\n",
      "3,000 examples, moving-average loss 0.31\n",
      "Completed no. 8 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.36\n",
      "1,000 examples, moving-average loss 0.34\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.32\n",
      "2,500 examples, moving-average loss 0.38\n",
      "3,000 examples, moving-average loss 0.31\n",
      "Completed no. 9 epoch in 0:00:02\n",
      "  500 examples, moving-average loss 0.35\n",
      "1,000 examples, moving-average loss 0.33\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.31\n",
      "2,500 examples, moving-average loss 0.37\n",
      "3,000 examples, moving-average loss 0.30\n",
      "Completed no. 10 epoch in 0:00:02\n"
     ]
    }
   ],
   "source": [
    "# training the model (full supervision)\n",
    "\n",
    "batch_size = 5\n",
    "e = 175 # embedding dimension\n",
    "h = [32,64,128] # hidden dimensions\n",
    "l = 0.001 # learning rate\n",
    "num_epoch = 10 #no. of epochs\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=3,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "model_fn = classifier_model_fn\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_train.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "    ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "    \n",
    "    \n",
    "    y_conf = []\n",
    "     \n",
    "    for i in range(len(y_train)):\n",
    "        y_conf.append(2 - np.absolute(Y_weak[i]-y_train[i]))\n",
    "       \n",
    "           \n",
    "    \n",
    "    y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "    ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "    \n",
    "    for j in range(num_epoch):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_batches = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        for (bx, bns, by) in utils.multi_batch_generator(batch_size, X_n_train, ns_train, y_conf):\n",
    "            # feed NumPy arrays into the placeholder Tensors\n",
    "            feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "            batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "            # Compute some statistics\n",
    "            total_batches += 1\n",
    "            total_examples += len(bx)\n",
    "            total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "            # Compute moving average to smooth out noisy per-batch loss\n",
    "            loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "            if (total_batches % 100 == 0):\n",
    "                print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "        print(\"Completed no. {:d} epoch in {:s}\".format(j+1,utils.pretty_timedelta(since=t0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.device('/gpu:0'),tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_test.shape[1]])  \n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              \n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])             \n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                              params=model_params)\n",
    "    predict_ = estimator_spec.predictions\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    t = X_n_train.shape[0]\n",
    "    \n",
    "    true_conf_score = []\n",
    "    for i in range(X_n_test.shape[0]):\n",
    "        true_conf_score.append(2 - np.absolute(Y_weak[t+i]-y_test[i]))\n",
    "    \n",
    "    \n",
    "    feed_dict = {x_ph_: X_n_test, ns_ph_: ns_test, y_ph_: true_conf_score}\n",
    "    predictions = sess.run(predict_, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_conf_score.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "866"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_conf_score = predictions['max']\n",
    "len(true_conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07322557  0.03891807  0.88785636]  test_label: 2  weak_label: 1\n",
      "[ 0.07860599  0.04398205  0.87741202]  test_label: 0  weak_label: 0\n",
      "[ 0.07628779  0.04670405  0.87700808]  test_label: 2  weak_label: 0\n",
      "[ 0.07480229  0.04216186  0.8830359 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07665514  0.0467688   0.87657601]  test_label: 0  weak_label: 0\n",
      "[ 0.07649496  0.04859744  0.87490761]  test_label: 2  weak_label: 0\n",
      "[ 0.07275506  0.04085319  0.88639176]  test_label: 0  weak_label: 0\n",
      "[ 0.07269223  0.04755015  0.87975764]  test_label: 0  weak_label: 2\n",
      "[ 0.07792452  0.04726973  0.87480581]  test_label: 0  weak_label: 2\n",
      "[ 0.07670923  0.04410905  0.87918162]  test_label: 0  weak_label: 1\n",
      "[ 0.07708305  0.04954216  0.87337482]  test_label: 1  weak_label: 0\n",
      "[ 0.07896527  0.04551755  0.87551713]  test_label: 0  weak_label: 0\n",
      "[ 0.07713     0.04543407  0.87743598]  test_label: 0  weak_label: 0\n",
      "[ 0.07888339  0.04638249  0.8747341 ]  test_label: 2  weak_label: 2\n",
      "[ 0.08079293  0.04982736  0.8693797 ]  test_label: 2  weak_label: 2\n",
      "[ 0.07754856  0.04347486  0.87897658]  test_label: 2  weak_label: 0\n",
      "[ 0.07462167  0.04642986  0.87894851]  test_label: 2  weak_label: 2\n",
      "[ 0.0758348   0.04695376  0.87721145]  test_label: 0  weak_label: 1\n",
      "[ 0.07930692  0.0446022   0.87609088]  test_label: 2  weak_label: 0\n",
      "[ 0.07462727  0.04733131  0.87804145]  test_label: 1  weak_label: 1\n",
      "[ 0.07473008  0.04500047  0.88026947]  test_label: 2  weak_label: 1\n",
      "[ 0.07881313  0.05045254  0.87073439]  test_label: 2  weak_label: 0\n",
      "[ 0.0743029   0.04349292  0.88220417]  test_label: 2  weak_label: 1\n",
      "[ 0.07737819  0.05294356  0.86967826]  test_label: 2  weak_label: 1\n",
      "[ 0.07614098  0.05041239  0.87344664]  test_label: 1  weak_label: 1\n",
      "[ 0.08155427  0.04644178  0.87200391]  test_label: 1  weak_label: 0\n",
      "[ 0.07467917  0.04611132  0.87920946]  test_label: 2  weak_label: 0\n",
      "[ 0.07590029  0.04351751  0.88058227]  test_label: 1  weak_label: 1\n",
      "[ 0.0774182   0.04454394  0.87803787]  test_label: 0  weak_label: 1\n",
      "[ 0.07803138  0.04461149  0.87735713]  test_label: 0  weak_label: 0\n",
      "[ 0.07505452  0.04576517  0.87918025]  test_label: 2  weak_label: 0\n",
      "[ 0.0788374   0.04704669  0.87411588]  test_label: 2  weak_label: 0\n",
      "[ 0.07939803  0.04452537  0.87607664]  test_label: 1  weak_label: 1\n",
      "[ 0.07782346  0.04491477  0.87726182]  test_label: 2  weak_label: 1\n",
      "[ 0.07438204  0.04583221  0.87978578]  test_label: 2  weak_label: 1\n",
      "[ 0.07796676  0.04510672  0.87692654]  test_label: 2  weak_label: 0\n",
      "[ 0.0777078   0.04738374  0.87490845]  test_label: 2  weak_label: 0\n",
      "[ 0.07705076  0.04972094  0.87322825]  test_label: 2  weak_label: 0\n",
      "[ 0.07255666  0.0414102   0.88603312]  test_label: 2  weak_label: 0\n",
      "[ 0.07688677  0.05261965  0.87049359]  test_label: 0  weak_label: 0\n",
      "[ 0.07349212  0.04659039  0.8799175 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07491339  0.04347765  0.88160896]  test_label: 0  weak_label: 0\n",
      "[ 0.07579809  0.05082089  0.87338102]  test_label: 0  weak_label: 0\n",
      "[ 0.07822887  0.0449169   0.87685418]  test_label: 1  weak_label: 0\n",
      "[ 0.07356042  0.04062384  0.88581574]  test_label: 1  weak_label: 1\n",
      "[ 0.07652387  0.04499503  0.87848109]  test_label: 2  weak_label: 0\n",
      "[ 0.077461    0.04552691  0.87701207]  test_label: 2  weak_label: 1\n",
      "[ 0.07746662  0.05239559  0.87013781]  test_label: 2  weak_label: 0\n",
      "[ 0.07552367  0.04418406  0.8802923 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07571077  0.04526047  0.87902874]  test_label: 2  weak_label: 0\n",
      "[ 0.07440785  0.04617559  0.87941658]  test_label: 0  weak_label: 0\n",
      "[ 0.07856235  0.04852955  0.87290817]  test_label: 0  weak_label: 1\n",
      "[ 0.07653895  0.04585178  0.87760925]  test_label: 0  weak_label: 0\n",
      "[ 0.07205975  0.04428986  0.88365036]  test_label: 0  weak_label: 1\n",
      "[ 0.07773414  0.04775678  0.8745091 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07918223  0.04761298  0.87320483]  test_label: 0  weak_label: 0\n",
      "[ 0.07802135  0.04589328  0.8760854 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07610743  0.04677452  0.87711805]  test_label: 0  weak_label: 1\n",
      "[ 0.07548609  0.04742899  0.87708497]  test_label: 0  weak_label: 1\n",
      "[ 0.07263524  0.04107215  0.88629264]  test_label: 2  weak_label: 0\n",
      "[ 0.0808138   0.04931235  0.86987388]  test_label: 0  weak_label: 1\n",
      "[ 0.07658015  0.04557388  0.87784594]  test_label: 1  weak_label: 0\n",
      "[ 0.07400744  0.04312522  0.88286734]  test_label: 0  weak_label: 0\n",
      "[ 0.08122919  0.0443196   0.87445122]  test_label: 2  weak_label: 1\n",
      "[ 0.0788978   0.04405354  0.87704861]  test_label: 2  weak_label: 0\n",
      "[ 0.07243287  0.04513309  0.88243407]  test_label: 0  weak_label: 0\n",
      "[ 0.0766149   0.04645678  0.87692827]  test_label: 0  weak_label: 0\n",
      "[ 0.07798664  0.04726117  0.87475222]  test_label: 2  weak_label: 0\n",
      "[ 0.0751669   0.04293996  0.8818931 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07656054  0.04261057  0.88082886]  test_label: 0  weak_label: 0\n",
      "[ 0.07329306  0.0449718   0.88173515]  test_label: 0  weak_label: 0\n",
      "[ 0.07527278  0.04613008  0.87859714]  test_label: 2  weak_label: 0\n",
      "[ 0.07411086  0.04529323  0.88059592]  test_label: 2  weak_label: 0\n",
      "[ 0.07813261  0.04863753  0.8732298 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07435814  0.04333344  0.88230842]  test_label: 0  weak_label: 0\n",
      "[ 0.070364    0.04096292  0.88867307]  test_label: 0  weak_label: 0\n",
      "[ 0.07722291  0.04410686  0.87867028]  test_label: 2  weak_label: 0\n",
      "[ 0.07385454  0.05240624  0.87373924]  test_label: 0  weak_label: 0\n",
      "[ 0.07622346  0.04465627  0.87912029]  test_label: 0  weak_label: 0\n",
      "[ 0.08265678  0.04859795  0.86874527]  test_label: 2  weak_label: 1\n",
      "[ 0.07856398  0.04362889  0.8778072 ]  test_label: 0  weak_label: 1\n",
      "[ 0.08108509  0.04897532  0.86993957]  test_label: 0  weak_label: 0\n",
      "[ 0.08353126  0.04535664  0.87111205]  test_label: 2  weak_label: 1\n",
      "[ 0.08077876  0.04927096  0.86995029]  test_label: 0  weak_label: 0\n",
      "[ 0.07162835  0.04351681  0.88485485]  test_label: 0  weak_label: 0\n",
      "[ 0.07721645  0.04640402  0.87637955]  test_label: 0  weak_label: 0\n",
      "[ 0.07721464  0.04981454  0.87297082]  test_label: 2  weak_label: 0\n",
      "[ 0.07738369  0.04725773  0.87535852]  test_label: 2  weak_label: 2\n",
      "[ 0.07984524  0.04759297  0.87256175]  test_label: 1  weak_label: 0\n",
      "[ 0.07948937  0.04861539  0.87189525]  test_label: 2  weak_label: 0\n",
      "[ 0.07778086  0.0460926   0.87612659]  test_label: 2  weak_label: 1\n",
      "[ 0.07598961  0.04495727  0.87905312]  test_label: 0  weak_label: 2\n",
      "[ 0.07905927  0.04907453  0.87186617]  test_label: 0  weak_label: 1\n",
      "[ 0.07647483  0.04320735  0.88031781]  test_label: 0  weak_label: 2\n",
      "[ 0.0808503   0.04318628  0.87596345]  test_label: 0  weak_label: 0\n",
      "[ 0.07647718  0.04847682  0.87504607]  test_label: 1  weak_label: 0\n",
      "[ 0.07743788  0.04692842  0.87563372]  test_label: 2  weak_label: 0\n",
      "[ 0.07549273  0.04792557  0.87658167]  test_label: 2  weak_label: 0\n",
      "[ 0.07804604  0.04747327  0.87448066]  test_label: 0  weak_label: 0\n",
      "[ 0.07682134  0.04768265  0.87549603]  test_label: 2  weak_label: 1\n",
      "[ 0.07707561  0.04820186  0.87472248]  test_label: 2  weak_label: 0\n",
      "[ 0.07724828  0.04778111  0.87497056]  test_label: 0  weak_label: 0\n",
      "[ 0.07854737  0.04533545  0.87611717]  test_label: 0  weak_label: 1\n",
      "[ 0.077298    0.04763354  0.87506849]  test_label: 0  weak_label: 0\n",
      "[ 0.07774752  0.04819822  0.87405425]  test_label: 1  weak_label: 0\n",
      "[ 0.07667185  0.04394118  0.87938702]  test_label: 1  weak_label: 1\n",
      "[ 0.07711899  0.04967334  0.87320763]  test_label: 2  weak_label: 1\n",
      "[ 0.07595273  0.0424769   0.8815704 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07698009  0.04686101  0.87615889]  test_label: 2  weak_label: 0\n",
      "[ 0.07313842  0.04560376  0.88125777]  test_label: 1  weak_label: 2\n",
      "[ 0.07642294  0.04457445  0.87900257]  test_label: 0  weak_label: 0\n",
      "[ 0.07982565  0.04703092  0.87314337]  test_label: 0  weak_label: 0\n",
      "[ 0.07937861  0.05066228  0.86995912]  test_label: 2  weak_label: 2\n",
      "[ 0.07074046  0.04294536  0.88631415]  test_label: 0  weak_label: 0\n",
      "[ 0.07930154  0.04693313  0.87376535]  test_label: 2  weak_label: 0\n",
      "[ 0.07740033  0.04645376  0.8761459 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07672201  0.04813529  0.87514275]  test_label: 0  weak_label: 0\n",
      "[ 0.07068877  0.04582272  0.88348854]  test_label: 0  weak_label: 0\n",
      "[ 0.0767709   0.04794592  0.87528318]  test_label: 1  weak_label: 0\n",
      "[ 0.08079502  0.0514112   0.86779374]  test_label: 0  weak_label: 0\n",
      "[ 0.07640031  0.04601274  0.87758696]  test_label: 0  weak_label: 2\n",
      "[ 0.07813978  0.05138903  0.87047118]  test_label: 0  weak_label: 0\n",
      "[ 0.07738917  0.04762351  0.8749873 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07515571  0.04150967  0.88333458]  test_label: 0  weak_label: 1\n",
      "[ 0.07602773  0.04517139  0.87880087]  test_label: 0  weak_label: 0\n",
      "[ 0.07655134  0.04614268  0.87730592]  test_label: 0  weak_label: 1\n",
      "[ 0.07933135  0.04733605  0.87333262]  test_label: 2  weak_label: 0\n",
      "[ 0.0743413   0.04530509  0.88035363]  test_label: 2  weak_label: 0\n",
      "[ 0.07815769  0.04781057  0.87403172]  test_label: 2  weak_label: 1\n",
      "[ 0.07493842  0.04216694  0.88289464]  test_label: 0  weak_label: 1\n",
      "[ 0.07647455  0.0466209   0.87690449]  test_label: 2  weak_label: 2\n",
      "[ 0.07360066  0.04319848  0.88320082]  test_label: 2  weak_label: 0\n",
      "[ 0.07336349  0.04270554  0.88393104]  test_label: 0  weak_label: 1\n",
      "[ 0.07774442  0.0467504   0.87550515]  test_label: 0  weak_label: 0\n",
      "[ 0.07712303  0.04806241  0.87481457]  test_label: 0  weak_label: 0\n",
      "[ 0.07807539  0.04683857  0.87508601]  test_label: 0  weak_label: 0\n",
      "[ 0.07455896  0.0458665   0.87957454]  test_label: 2  weak_label: 0\n",
      "[ 0.07884896  0.04851109  0.87263989]  test_label: 0  weak_label: 0\n",
      "[ 0.07371315  0.04399878  0.8822881 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07711473  0.04683331  0.8760519 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07900576  0.04202076  0.87897348]  test_label: 2  weak_label: 2\n",
      "[ 0.07560866  0.04615539  0.878236  ]  test_label: 0  weak_label: 0\n",
      "[ 0.07373244  0.04304827  0.88321924]  test_label: 1  weak_label: 0\n",
      "[ 0.07697765  0.0474242   0.87559819]  test_label: 2  weak_label: 0\n",
      "[ 0.07735022  0.04612507  0.87652469]  test_label: 1  weak_label: 0\n",
      "[ 0.0771488   0.04627366  0.8765775 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07651406  0.04812215  0.87536383]  test_label: 2  weak_label: 1\n",
      "[ 0.07672222  0.04734882  0.875929  ]  test_label: 2  weak_label: 0\n",
      "[ 0.07382669  0.0446721   0.8815012 ]  test_label: 2  weak_label: 1\n",
      "[ 0.0780165   0.04835761  0.87362587]  test_label: 2  weak_label: 0\n",
      "[ 0.07164466  0.04236874  0.88598663]  test_label: 2  weak_label: 0\n",
      "[ 0.07778732  0.05005442  0.87215823]  test_label: 0  weak_label: 0\n",
      "[ 0.07374793  0.04595036  0.88030171]  test_label: 2  weak_label: 0\n",
      "[ 0.07520479  0.04566396  0.87913126]  test_label: 2  weak_label: 0\n",
      "[ 0.07580552  0.04572979  0.8784647 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07377651  0.04463506  0.8815884 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07486273  0.04282008  0.88231725]  test_label: 0  weak_label: 0\n",
      "[ 0.07622186  0.04835612  0.875422  ]  test_label: 1  weak_label: 0\n",
      "[ 0.07382321  0.04634331  0.87983352]  test_label: 0  weak_label: 0\n",
      "[ 0.07602997  0.04765695  0.87631309]  test_label: 2  weak_label: 0\n",
      "[ 0.0758155   0.04184875  0.88233578]  test_label: 1  weak_label: 0\n",
      "[ 0.07657374  0.05190301  0.8715232 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07760489  0.04631869  0.87607646]  test_label: 2  weak_label: 0\n",
      "[ 0.07941898  0.04845308  0.87212795]  test_label: 0  weak_label: 0\n",
      "[ 0.07832962  0.04556636  0.876104  ]  test_label: 0  weak_label: 0\n",
      "[ 0.0793974   0.04850541  0.87209719]  test_label: 1  weak_label: 1\n",
      "[ 0.07613884  0.04932136  0.87453973]  test_label: 0  weak_label: 1\n",
      "[ 0.07823706  0.04620201  0.875561  ]  test_label: 2  weak_label: 0\n",
      "[ 0.07415774  0.04654672  0.87929559]  test_label: 1  weak_label: 1\n",
      "[ 0.07428981  0.04046649  0.88524365]  test_label: 0  weak_label: 2\n",
      "[ 0.0760079   0.04923197  0.87476009]  test_label: 2  weak_label: 1\n",
      "[ 0.07768062  0.04680257  0.87551677]  test_label: 0  weak_label: 2\n",
      "[ 0.07631683  0.04602258  0.87766057]  test_label: 2  weak_label: 0\n",
      "[ 0.07219604  0.04037958  0.88742441]  test_label: 2  weak_label: 1\n",
      "[ 0.07735766  0.04840233  0.87423998]  test_label: 2  weak_label: 1\n",
      "[ 0.07117215  0.04699477  0.88183308]  test_label: 2  weak_label: 0\n",
      "[ 0.07453322  0.0426923   0.88277453]  test_label: 0  weak_label: 1\n",
      "[ 0.07565098  0.04671988  0.8776291 ]  test_label: 1  weak_label: 1\n",
      "[ 0.0754464   0.04469245  0.87986112]  test_label: 2  weak_label: 0\n",
      "[ 0.07829014  0.04800842  0.87370139]  test_label: 2  weak_label: 0\n",
      "[ 0.07651884  0.04551573  0.87796539]  test_label: 0  weak_label: 0\n",
      "[ 0.07970519  0.04914508  0.87114978]  test_label: 2  weak_label: 0\n",
      "[ 0.07455061  0.04572269  0.87972665]  test_label: 2  weak_label: 0\n",
      "[ 0.07695283  0.04452594  0.8785212 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07484321  0.03974818  0.88540858]  test_label: 2  weak_label: 0\n",
      "[ 0.07774384  0.05027059  0.87198561]  test_label: 2  weak_label: 2\n",
      "[ 0.07665957  0.04992466  0.87341577]  test_label: 2  weak_label: 2\n",
      "[ 0.07568803  0.04451258  0.87979937]  test_label: 0  weak_label: 0\n",
      "[ 0.0809871   0.04735766  0.87165529]  test_label: 1  weak_label: 0\n",
      "[ 0.07797922  0.04791974  0.87410104]  test_label: 2  weak_label: 0\n",
      "[ 0.07314645  0.04071118  0.88614231]  test_label: 0  weak_label: 0\n",
      "[ 0.07629231  0.05004384  0.8736639 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07531895  0.04882164  0.87585938]  test_label: 2  weak_label: 0\n",
      "[ 0.07393159  0.05005523  0.87601322]  test_label: 2  weak_label: 1\n",
      "[ 0.0749977   0.04889135  0.87611091]  test_label: 0  weak_label: 0\n",
      "[ 0.07109505  0.04836684  0.88053805]  test_label: 2  weak_label: 0\n",
      "[ 0.07747705  0.04450368  0.87801921]  test_label: 0  weak_label: 2\n",
      "[ 0.07827931  0.04702974  0.87469095]  test_label: 0  weak_label: 1\n",
      "[ 0.08071175  0.04510737  0.87418085]  test_label: 2  weak_label: 0\n",
      "[ 0.08088557  0.04843291  0.87068152]  test_label: 2  weak_label: 0\n",
      "[ 0.07380392  0.04502368  0.88117242]  test_label: 1  weak_label: 0\n",
      "[ 0.07937522  0.04744502  0.87317973]  test_label: 0  weak_label: 1\n",
      "[ 0.07390823  0.0472873   0.8788045 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07583103  0.04861901  0.87554991]  test_label: 0  weak_label: 2\n",
      "[ 0.07591168  0.04719629  0.87689203]  test_label: 0  weak_label: 2\n",
      "[ 0.07903351  0.05077654  0.87018991]  test_label: 2  weak_label: 1\n",
      "[ 0.07841529  0.04496894  0.87661576]  test_label: 2  weak_label: 2\n",
      "[ 0.07628096  0.05089286  0.87282622]  test_label: 2  weak_label: 0\n",
      "[ 0.07749648  0.04647227  0.87603122]  test_label: 0  weak_label: 2\n",
      "[ 0.08194689  0.04657676  0.87147635]  test_label: 0  weak_label: 0\n",
      "[ 0.07903155  0.04419373  0.87677473]  test_label: 2  weak_label: 0\n",
      "[ 0.07950101  0.05223763  0.86826134]  test_label: 2  weak_label: 0\n",
      "[ 0.07829155  0.04378579  0.87792265]  test_label: 2  weak_label: 0\n",
      "[ 0.07720842  0.04666768  0.87612391]  test_label: 0  weak_label: 0\n",
      "[ 0.07582165  0.04173487  0.88244343]  test_label: 0  weak_label: 0\n",
      "[ 0.07818741  0.04620175  0.87561077]  test_label: 2  weak_label: 2\n",
      "[ 0.07964674  0.04423242  0.87612081]  test_label: 0  weak_label: 0\n",
      "[ 0.07378258  0.04274681  0.88347059]  test_label: 2  weak_label: 1\n",
      "[ 0.07504518  0.0440315   0.88092333]  test_label: 0  weak_label: 1\n",
      "[ 0.07550619  0.04298246  0.88151133]  test_label: 2  weak_label: 0\n",
      "[ 0.07551794  0.0451742   0.87930781]  test_label: 0  weak_label: 0\n",
      "[ 0.07934313  0.04896905  0.87168789]  test_label: 1  weak_label: 0\n",
      "[ 0.07292753  0.04043755  0.88663489]  test_label: 0  weak_label: 0\n",
      "[ 0.07501902  0.04743136  0.87754959]  test_label: 1  weak_label: 2\n",
      "[ 0.07691362  0.04784105  0.87524527]  test_label: 2  weak_label: 0\n",
      "[ 0.07987446  0.04632986  0.87379569]  test_label: 0  weak_label: 0\n",
      "[ 0.07696675  0.04763352  0.87539971]  test_label: 1  weak_label: 0\n",
      "[ 0.07724683  0.04560498  0.87714821]  test_label: 0  weak_label: 2\n",
      "[ 0.07701533  0.05197744  0.87100726]  test_label: 0  weak_label: 0\n",
      "[ 0.07484479  0.04828469  0.87687051]  test_label: 0  weak_label: 1\n",
      "[ 0.07152572  0.04362424  0.88484997]  test_label: 0  weak_label: 1\n",
      "[ 0.07826917  0.04359867  0.8781321 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07802321  0.04604446  0.87593234]  test_label: 1  weak_label: 0\n",
      "[ 0.08403198  0.05076566  0.86520243]  test_label: 2  weak_label: 0\n",
      "[ 0.07617199  0.04976835  0.87405974]  test_label: 0  weak_label: 1\n",
      "[ 0.07685726  0.04436103  0.87878174]  test_label: 0  weak_label: 2\n",
      "[ 0.07588091  0.0460359   0.87808317]  test_label: 0  weak_label: 0\n",
      "[ 0.077209    0.04501639  0.87777454]  test_label: 1  weak_label: 1\n",
      "[ 0.07285051  0.04427358  0.88287586]  test_label: 0  weak_label: 0\n",
      "[ 0.07812966  0.04265571  0.87921464]  test_label: 0  weak_label: 2\n",
      "[ 0.07678945  0.04600126  0.87720931]  test_label: 0  weak_label: 0\n",
      "[ 0.07936649  0.04788237  0.87275112]  test_label: 0  weak_label: 0\n",
      "[ 0.07294258  0.04623397  0.88082349]  test_label: 0  weak_label: 0\n",
      "[ 0.07738511  0.0433071   0.87930775]  test_label: 0  weak_label: 1\n",
      "[ 0.07555544  0.0452293   0.87921518]  test_label: 0  weak_label: 0\n",
      "[ 0.07429583  0.04849681  0.8772074 ]  test_label: 2  weak_label: 2\n",
      "[ 0.0754471   0.04508357  0.87946934]  test_label: 0  weak_label: 1\n",
      "[ 0.07638361  0.04653617  0.8770802 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07895578  0.04508242  0.87596184]  test_label: 2  weak_label: 1\n",
      "[ 0.07797643  0.04969104  0.87233257]  test_label: 0  weak_label: 0\n",
      "[ 0.07742646  0.04383215  0.87874144]  test_label: 0  weak_label: 0\n",
      "[ 0.07574917  0.04459684  0.87965399]  test_label: 0  weak_label: 0\n",
      "[ 0.07488298  0.04408839  0.88102859]  test_label: 2  weak_label: 0\n",
      "[ 0.07470421  0.04512972  0.88016611]  test_label: 0  weak_label: 0\n",
      "[ 0.07720669  0.04692805  0.87586522]  test_label: 0  weak_label: 0\n",
      "[ 0.07805369  0.04844553  0.87350076]  test_label: 1  weak_label: 0\n",
      "[ 0.07934076  0.04666485  0.87399441]  test_label: 1  weak_label: 0\n",
      "[ 0.07312323  0.04659089  0.88028592]  test_label: 0  weak_label: 2\n",
      "[ 0.08129825  0.04667669  0.87202507]  test_label: 2  weak_label: 0\n",
      "[ 0.07594886  0.04738742  0.87666368]  test_label: 0  weak_label: 1\n",
      "[ 0.07832886  0.04491342  0.8767578 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07665917  0.04769436  0.87564647]  test_label: 2  weak_label: 0\n",
      "[ 0.07393249  0.04077125  0.88529629]  test_label: 2  weak_label: 0\n",
      "[ 0.07256342  0.04122159  0.88621497]  test_label: 2  weak_label: 1\n",
      "[ 0.07543679  0.04900385  0.87555933]  test_label: 0  weak_label: 1\n",
      "[ 0.07210778  0.04403835  0.88385391]  test_label: 0  weak_label: 1\n",
      "[ 0.07520739  0.04371151  0.88108116]  test_label: 1  weak_label: 1\n",
      "[ 0.07510566  0.04432495  0.8805694 ]  test_label: 0  weak_label: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07389481  0.04384984  0.88225532]  test_label: 0  weak_label: 2\n",
      "[ 0.07844873  0.04633005  0.87522125]  test_label: 0  weak_label: 0\n",
      "[ 0.08005022  0.04443716  0.87551266]  test_label: 2  weak_label: 2\n",
      "[ 0.07763632  0.04711861  0.87524509]  test_label: 2  weak_label: 0\n",
      "[ 0.08000875  0.04721426  0.87277699]  test_label: 0  weak_label: 0\n",
      "[ 0.08033216  0.04886681  0.87080109]  test_label: 0  weak_label: 0\n",
      "[ 0.07923782  0.04746956  0.87329262]  test_label: 2  weak_label: 1\n",
      "[ 0.07665247  0.04337799  0.87996954]  test_label: 2  weak_label: 1\n",
      "[ 0.07532654  0.04494201  0.87973148]  test_label: 0  weak_label: 2\n",
      "[ 0.07312514  0.04436945  0.88250548]  test_label: 0  weak_label: 1\n",
      "[ 0.07737187  0.04375817  0.87886995]  test_label: 1  weak_label: 0\n",
      "[ 0.07863861  0.05245231  0.86890906]  test_label: 2  weak_label: 0\n",
      "[ 0.07284531  0.04069449  0.88646019]  test_label: 1  weak_label: 1\n",
      "[ 0.07571222  0.0425666   0.8817212 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07756718  0.04525257  0.87718022]  test_label: 1  weak_label: 0\n",
      "[ 0.07695348  0.04860429  0.87444222]  test_label: 1  weak_label: 1\n",
      "[ 0.07524155  0.0416669   0.88309151]  test_label: 2  weak_label: 1\n",
      "[ 0.07476743  0.04584633  0.87938625]  test_label: 1  weak_label: 0\n",
      "[ 0.07248259  0.04672921  0.88078815]  test_label: 0  weak_label: 0\n",
      "[ 0.07568365  0.05009813  0.87421823]  test_label: 2  weak_label: 1\n",
      "[ 0.07808755  0.04734183  0.87457055]  test_label: 0  weak_label: 1\n",
      "[ 0.07390238  0.04395493  0.88214266]  test_label: 0  weak_label: 1\n",
      "[ 0.07538121  0.04692725  0.87769157]  test_label: 1  weak_label: 1\n",
      "[ 0.08152027  0.04755842  0.87092131]  test_label: 2  weak_label: 1\n",
      "[ 0.07602278  0.0464014   0.87757587]  test_label: 0  weak_label: 1\n",
      "[ 0.07440522  0.04706379  0.87853098]  test_label: 0  weak_label: 1\n",
      "[ 0.07410517  0.04396628  0.88192856]  test_label: 0  weak_label: 1\n",
      "[ 0.07752755  0.04553434  0.87693816]  test_label: 2  weak_label: 1\n",
      "[ 0.07505273  0.04446752  0.88047975]  test_label: 0  weak_label: 1\n",
      "[ 0.07516357  0.04538433  0.87945211]  test_label: 2  weak_label: 1\n",
      "[ 0.07702268  0.04543953  0.87753773]  test_label: 2  weak_label: 2\n",
      "[ 0.07436463  0.04767501  0.87796044]  test_label: 2  weak_label: 0\n",
      "[ 0.07815926  0.04471392  0.87712687]  test_label: 0  weak_label: 1\n",
      "[ 0.07864758  0.0474393   0.87391311]  test_label: 0  weak_label: 0\n",
      "[ 0.07460321  0.0466848   0.878712  ]  test_label: 0  weak_label: 1\n",
      "[ 0.0724403   0.04203633  0.88552338]  test_label: 2  weak_label: 1\n",
      "[ 0.08137057  0.04630556  0.87232381]  test_label: 0  weak_label: 0\n",
      "[ 0.07676353  0.04429489  0.8789416 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07636409  0.04647194  0.87716389]  test_label: 0  weak_label: 0\n",
      "[ 0.07369197  0.0440958   0.88221216]  test_label: 0  weak_label: 0\n",
      "[ 0.08436403  0.04904383  0.86659211]  test_label: 2  weak_label: 0\n",
      "[ 0.07512284  0.04592492  0.87895226]  test_label: 0  weak_label: 1\n",
      "[ 0.07731705  0.04814674  0.87453622]  test_label: 2  weak_label: 0\n",
      "[ 0.07258838  0.04224476  0.88516688]  test_label: 1  weak_label: 0\n",
      "[ 0.07552304  0.04762331  0.87685364]  test_label: 0  weak_label: 1\n",
      "[ 0.0747123   0.04369617  0.88159156]  test_label: 1  weak_label: 0\n",
      "[ 0.07448298  0.04667775  0.87883931]  test_label: 0  weak_label: 0\n",
      "[ 0.07433072  0.04990843  0.87576079]  test_label: 0  weak_label: 0\n",
      "[ 0.07595544  0.0461037   0.87794083]  test_label: 2  weak_label: 0\n",
      "[ 0.0804352   0.05162494  0.86793989]  test_label: 2  weak_label: 1\n",
      "[ 0.06978617  0.04536072  0.88485318]  test_label: 1  weak_label: 0\n",
      "[ 0.07691034  0.04568657  0.87740308]  test_label: 2  weak_label: 2\n",
      "[ 0.0794092   0.04980092  0.87078989]  test_label: 0  weak_label: 0\n",
      "[ 0.07346654  0.04545114  0.88108224]  test_label: 2  weak_label: 0\n",
      "[ 0.08115721  0.04680612  0.87203664]  test_label: 0  weak_label: 0\n",
      "[ 0.07361361  0.04295328  0.8834331 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07484166  0.04487352  0.88028479]  test_label: 0  weak_label: 0\n",
      "[ 0.07586008  0.04650062  0.87763929]  test_label: 2  weak_label: 0\n",
      "[ 0.07400365  0.04503164  0.88096476]  test_label: 0  weak_label: 0\n",
      "[ 0.07711396  0.0483433   0.87454277]  test_label: 2  weak_label: 0\n",
      "[ 0.07430539  0.04799826  0.87769634]  test_label: 0  weak_label: 2\n",
      "[ 0.074466    0.04465282  0.88088113]  test_label: 0  weak_label: 2\n",
      "[ 0.07616642  0.05105851  0.87277508]  test_label: 2  weak_label: 1\n",
      "[ 0.07946967  0.05159536  0.86893499]  test_label: 1  weak_label: 0\n",
      "[ 0.07782953  0.04786261  0.87430787]  test_label: 0  weak_label: 0\n",
      "[ 0.07531046  0.04566345  0.87902611]  test_label: 1  weak_label: 2\n",
      "[ 0.07842586  0.04261323  0.87896091]  test_label: 1  weak_label: 2\n",
      "[ 0.07911821  0.04529342  0.87558842]  test_label: 2  weak_label: 1\n",
      "[ 0.07687181  0.04903138  0.87409675]  test_label: 2  weak_label: 1\n",
      "[ 0.0761302   0.04268525  0.88118452]  test_label: 0  weak_label: 0\n",
      "[ 0.08020949  0.04587784  0.87391263]  test_label: 0  weak_label: 0\n",
      "[ 0.07307713  0.03882582  0.88809711]  test_label: 0  weak_label: 0\n",
      "[ 0.07705513  0.05437385  0.86857104]  test_label: 0  weak_label: 0\n",
      "[ 0.07959653  0.04614439  0.874259  ]  test_label: 0  weak_label: 0\n",
      "[ 0.07627787  0.04479189  0.87893027]  test_label: 1  weak_label: 1\n",
      "[ 0.0742338   0.04501614  0.88075006]  test_label: 0  weak_label: 0\n",
      "[ 0.07583176  0.05029302  0.87387526]  test_label: 0  weak_label: 0\n",
      "[ 0.08256206  0.04837515  0.86906284]  test_label: 2  weak_label: 1\n",
      "[ 0.0776401   0.04688302  0.8754769 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07588051  0.044186    0.87993348]  test_label: 2  weak_label: 2\n",
      "[ 0.07390083  0.04159653  0.88450265]  test_label: 0  weak_label: 2\n",
      "[ 0.07567474  0.04431857  0.88000667]  test_label: 1  weak_label: 0\n",
      "[ 0.07711551  0.05005005  0.8728345 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07501993  0.04411343  0.88086659]  test_label: 1  weak_label: 0\n",
      "[ 0.07803647  0.04805372  0.87390983]  test_label: 0  weak_label: 0\n",
      "[ 0.08345245  0.04923508  0.86731249]  test_label: 2  weak_label: 0\n",
      "[ 0.07998233  0.04750884  0.87250882]  test_label: 0  weak_label: 1\n",
      "[ 0.07112718  0.04636897  0.88250387]  test_label: 2  weak_label: 1\n",
      "[ 0.07505516  0.04487059  0.88007426]  test_label: 1  weak_label: 1\n",
      "[ 0.07766731  0.04709648  0.87523621]  test_label: 2  weak_label: 0\n",
      "[ 0.07871684  0.0449967   0.87628645]  test_label: 2  weak_label: 2\n",
      "[ 0.07586008  0.04650062  0.87763929]  test_label: 0  weak_label: 0\n",
      "[ 0.07828181  0.0471569   0.87456125]  test_label: 0  weak_label: 1\n",
      "[ 0.07749221  0.04824645  0.87426132]  test_label: 0  weak_label: 0\n",
      "[ 0.07199386  0.04490926  0.88309687]  test_label: 2  weak_label: 1\n",
      "[ 0.07641833  0.04710041  0.87648129]  test_label: 1  weak_label: 0\n",
      "[ 0.07748462  0.05293071  0.86958468]  test_label: 1  weak_label: 1\n",
      "[ 0.0751838   0.05098506  0.87383115]  test_label: 0  weak_label: 1\n",
      "[ 0.07352312  0.04802136  0.87845552]  test_label: 2  weak_label: 1\n",
      "[ 0.07607058  0.04401334  0.87991607]  test_label: 2  weak_label: 1\n",
      "[ 0.07550086  0.04385156  0.88064754]  test_label: 2  weak_label: 0\n",
      "[ 0.07982569  0.04666768  0.87350672]  test_label: 0  weak_label: 2\n",
      "[ 0.0779162   0.04421972  0.87786412]  test_label: 1  weak_label: 1\n",
      "[ 0.0774728   0.04262827  0.87989891]  test_label: 2  weak_label: 1\n",
      "[ 0.07497431  0.04379006  0.88123566]  test_label: 1  weak_label: 1\n",
      "[ 0.07597265  0.04413974  0.87988758]  test_label: 0  weak_label: 0\n",
      "[ 0.07359713  0.04641257  0.87999034]  test_label: 1  weak_label: 0\n",
      "[ 0.07285817  0.04555896  0.88158286]  test_label: 2  weak_label: 1\n",
      "[ 0.07604345  0.0430606   0.88089591]  test_label: 1  weak_label: 1\n",
      "[ 0.07523137  0.04570614  0.87906253]  test_label: 2  weak_label: 1\n",
      "[ 0.07325643  0.04327823  0.88346535]  test_label: 2  weak_label: 1\n",
      "[ 0.07548568  0.04292751  0.88158679]  test_label: 0  weak_label: 1\n",
      "[ 0.07330322  0.04586332  0.88083351]  test_label: 1  weak_label: 0\n",
      "[ 0.078877    0.04774778  0.87337518]  test_label: 0  weak_label: 0\n",
      "[ 0.08318509  0.05064804  0.86616689]  test_label: 1  weak_label: 0\n",
      "[ 0.0774557   0.05118962  0.87135464]  test_label: 2  weak_label: 1\n",
      "[ 0.07235424  0.04709423  0.88055158]  test_label: 2  weak_label: 0\n",
      "[ 0.07644797  0.04351111  0.88004088]  test_label: 2  weak_label: 0\n",
      "[ 0.07648133  0.04488286  0.87863576]  test_label: 0  weak_label: 0\n",
      "[ 0.07501788  0.043981    0.88100111]  test_label: 0  weak_label: 1\n",
      "[ 0.07788722  0.04320731  0.87890548]  test_label: 0  weak_label: 1\n",
      "[ 0.08081068  0.0501339   0.86905545]  test_label: 0  weak_label: 1\n",
      "[ 0.07619297  0.0483897   0.87541735]  test_label: 2  weak_label: 0\n",
      "[ 0.07749593  0.04647444  0.87602961]  test_label: 0  weak_label: 0\n",
      "[ 0.07687223  0.04553465  0.8775931 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07703923  0.05014626  0.87281448]  test_label: 0  weak_label: 0\n",
      "[ 0.07950227  0.04645669  0.87404102]  test_label: 0  weak_label: 0\n",
      "[ 0.0764669   0.04345471  0.88007838]  test_label: 0  weak_label: 1\n",
      "[ 0.07568038  0.04593302  0.87838662]  test_label: 0  weak_label: 0\n",
      "[ 0.07540645  0.04616123  0.87843233]  test_label: 2  weak_label: 0\n",
      "[ 0.07690854  0.05147556  0.87161595]  test_label: 0  weak_label: 0\n",
      "[ 0.07594334  0.0526155   0.87144119]  test_label: 0  weak_label: 0\n",
      "[ 0.07560088  0.04574416  0.8786549 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07493759  0.04660815  0.87845433]  test_label: 1  weak_label: 1\n",
      "[ 0.07900589  0.04769391  0.87330019]  test_label: 0  weak_label: 2\n",
      "[ 0.07495578  0.04197449  0.88306975]  test_label: 1  weak_label: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07669669  0.04692104  0.87638223]  test_label: 2  weak_label: 0\n",
      "[ 0.07815537  0.05087761  0.87096697]  test_label: 0  weak_label: 1\n",
      "[ 0.07682319  0.04480662  0.87837023]  test_label: 2  weak_label: 1\n",
      "[ 0.07634269  0.0447303   0.87892705]  test_label: 0  weak_label: 2\n",
      "[ 0.0778381   0.04818557  0.87397629]  test_label: 2  weak_label: 1\n",
      "[ 0.07608122  0.04611478  0.87780398]  test_label: 2  weak_label: 0\n",
      "[ 0.07718474  0.05229888  0.87051642]  test_label: 0  weak_label: 1\n",
      "[ 0.07321989  0.05050107  0.87627906]  test_label: 2  weak_label: 0\n",
      "[ 0.07773992  0.04859049  0.87366962]  test_label: 2  weak_label: 1\n",
      "[ 0.07490274  0.0471005   0.8779968 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07589426  0.04836215  0.87574363]  test_label: 0  weak_label: 0\n",
      "[ 0.07901502  0.04669275  0.87429219]  test_label: 1  weak_label: 2\n",
      "[ 0.07132446  0.04332893  0.88534665]  test_label: 0  weak_label: 0\n",
      "[ 0.07907503  0.0502668   0.87065822]  test_label: 0  weak_label: 1\n",
      "[ 0.07902095  0.0480226   0.87295645]  test_label: 0  weak_label: 0\n",
      "[ 0.08020715  0.04639871  0.87339413]  test_label: 0  weak_label: 0\n",
      "[ 0.07553177  0.04754806  0.87692016]  test_label: 1  weak_label: 1\n",
      "[ 0.07860785  0.04919407  0.87219805]  test_label: 1  weak_label: 1\n",
      "[ 0.07505452  0.04576517  0.87918025]  test_label: 2  weak_label: 2\n",
      "[ 0.07456112  0.04254546  0.88289344]  test_label: 0  weak_label: 0\n",
      "[ 0.0763551   0.04827022  0.87537467]  test_label: 1  weak_label: 1\n",
      "[ 0.07818547  0.0439746   0.87783998]  test_label: 1  weak_label: 1\n",
      "[ 0.07573497  0.04549189  0.87877309]  test_label: 2  weak_label: 0\n",
      "[ 0.07762127  0.05010056  0.87227821]  test_label: 2  weak_label: 1\n",
      "[ 0.08086234  0.04571296  0.87342465]  test_label: 0  weak_label: 1\n",
      "[ 0.07726774  0.04631566  0.87641668]  test_label: 2  weak_label: 0\n",
      "[ 0.08030093  0.04575505  0.87394404]  test_label: 2  weak_label: 0\n",
      "[ 0.07579526  0.04334245  0.88086224]  test_label: 0  weak_label: 1\n",
      "[ 0.07617077  0.0493139   0.8745153 ]  test_label: 0  weak_label: 0\n",
      "[ 0.0769182   0.0434586   0.87962323]  test_label: 0  weak_label: 1\n",
      "[ 0.07774899  0.04519886  0.87705213]  test_label: 1  weak_label: 0\n",
      "[ 0.07545692  0.04581135  0.87873179]  test_label: 1  weak_label: 1\n",
      "[ 0.07563459  0.04541466  0.87895077]  test_label: 0  weak_label: 2\n",
      "[ 0.07982691  0.04633141  0.87384164]  test_label: 0  weak_label: 1\n",
      "[ 0.07962342  0.04402928  0.87634724]  test_label: 0  weak_label: 1\n",
      "[ 0.07724332  0.0458265   0.87693018]  test_label: 0  weak_label: 0\n",
      "[ 0.07398371  0.04442784  0.8815884 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07947722  0.05061127  0.86991149]  test_label: 2  weak_label: 1\n",
      "[ 0.07725096  0.04386141  0.87888765]  test_label: 0  weak_label: 1\n",
      "[ 0.07980006  0.04832647  0.87187344]  test_label: 0  weak_label: 0\n",
      "[ 0.07808068  0.04900306  0.87291622]  test_label: 0  weak_label: 0\n",
      "[ 0.07686243  0.04894925  0.8741883 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07911825  0.05129008  0.86959165]  test_label: 2  weak_label: 1\n",
      "[ 0.0752644   0.04404557  0.88069004]  test_label: 0  weak_label: 0\n",
      "[ 0.07495748  0.04236927  0.88267326]  test_label: 1  weak_label: 0\n",
      "[ 0.07559311  0.04543522  0.8789717 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07091995  0.0464241   0.88265598]  test_label: 2  weak_label: 0\n",
      "[ 0.07707947  0.04691196  0.87600857]  test_label: 2  weak_label: 0\n",
      "[ 0.07534538  0.04484316  0.87981147]  test_label: 1  weak_label: 1\n",
      "[ 0.07896629  0.04549564  0.87553805]  test_label: 2  weak_label: 2\n",
      "[ 0.07716939  0.0475471   0.87528354]  test_label: 1  weak_label: 0\n",
      "[ 0.07910155  0.05106449  0.86983395]  test_label: 0  weak_label: 0\n",
      "[ 0.07503946  0.04665924  0.87830126]  test_label: 0  weak_label: 0\n",
      "[ 0.07636286  0.04767693  0.87596017]  test_label: 0  weak_label: 0\n",
      "[ 0.07829601  0.04246086  0.87924314]  test_label: 0  weak_label: 0\n",
      "[ 0.07616869  0.046652    0.87717932]  test_label: 0  weak_label: 0\n",
      "[ 0.07796267  0.0478161   0.87422121]  test_label: 0  weak_label: 0\n",
      "[ 0.07500325  0.04052185  0.88447493]  test_label: 2  weak_label: 0\n",
      "[ 0.07285443  0.0459227   0.88122284]  test_label: 1  weak_label: 2\n",
      "[ 0.07639462  0.04934046  0.87426496]  test_label: 2  weak_label: 1\n",
      "[ 0.07950785  0.04723812  0.873254  ]  test_label: 0  weak_label: 1\n",
      "[ 0.07571099  0.0460301   0.87825888]  test_label: 2  weak_label: 0\n",
      "[ 0.07564388  0.04766043  0.87669563]  test_label: 0  weak_label: 1\n",
      "[ 0.07742479  0.0521953   0.87037987]  test_label: 1  weak_label: 1\n",
      "[ 0.07733592  0.04934631  0.87331772]  test_label: 0  weak_label: 0\n",
      "[ 0.08003967  0.04793797  0.87202239]  test_label: 2  weak_label: 2\n",
      "[ 0.07869492  0.04798692  0.8733182 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07844215  0.04899361  0.87256432]  test_label: 1  weak_label: 1\n",
      "[ 0.07785034  0.04200102  0.88014865]  test_label: 2  weak_label: 0\n",
      "[ 0.07656645  0.04533355  0.87810004]  test_label: 0  weak_label: 1\n",
      "[ 0.07566286  0.0509878   0.87334937]  test_label: 0  weak_label: 2\n",
      "[ 0.07393105  0.04885289  0.87721604]  test_label: 0  weak_label: 1\n",
      "[ 0.07779698  0.04651212  0.87569088]  test_label: 0  weak_label: 0\n",
      "[ 0.07866819  0.04701597  0.87431586]  test_label: 2  weak_label: 0\n",
      "[ 0.0778955   0.04460004  0.87750441]  test_label: 0  weak_label: 1\n",
      "[ 0.07453629  0.04751054  0.87795317]  test_label: 0  weak_label: 0\n",
      "[ 0.07589416  0.04541896  0.87868685]  test_label: 0  weak_label: 0\n",
      "[ 0.07842167  0.04400321  0.8775751 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07713459  0.05195459  0.87091082]  test_label: 0  weak_label: 0\n",
      "[ 0.07847476  0.05318018  0.86834508]  test_label: 1  weak_label: 2\n",
      "[ 0.07405505  0.04855877  0.87738621]  test_label: 2  weak_label: 0\n",
      "[ 0.07808625  0.04702465  0.87488908]  test_label: 1  weak_label: 1\n",
      "[ 0.07959901  0.0510727   0.86932832]  test_label: 0  weak_label: 1\n",
      "[ 0.07952934  0.04743998  0.87303072]  test_label: 2  weak_label: 0\n",
      "[ 0.07737676  0.04841751  0.87420571]  test_label: 2  weak_label: 0\n",
      "[ 0.07631041  0.04600662  0.87768298]  test_label: 0  weak_label: 0\n",
      "[ 0.07700075  0.04589535  0.87710392]  test_label: 2  weak_label: 2\n",
      "[ 0.07590865  0.04537076  0.87872064]  test_label: 1  weak_label: 0\n",
      "[ 0.07514339  0.04493544  0.8799212 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07416467  0.04176787  0.88406748]  test_label: 2  weak_label: 2\n",
      "[ 0.07510179  0.04564837  0.87924987]  test_label: 0  weak_label: 1\n",
      "[ 0.07524645  0.04702351  0.87773001]  test_label: 0  weak_label: 1\n",
      "[ 0.07488099  0.04612363  0.87899536]  test_label: 2  weak_label: 0\n",
      "[ 0.07752223  0.04661885  0.8758589 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07702404  0.04454283  0.87843317]  test_label: 2  weak_label: 1\n",
      "[ 0.07507122  0.04818828  0.87674046]  test_label: 0  weak_label: 1\n",
      "[ 0.07514551  0.04096289  0.88389152]  test_label: 0  weak_label: 0\n",
      "[ 0.07982986  0.04791365  0.87225652]  test_label: 0  weak_label: 1\n",
      "[ 0.08010152  0.0430827   0.8768158 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07720234  0.04584533  0.87695235]  test_label: 0  weak_label: 0\n",
      "[ 0.07495912  0.04548103  0.87955981]  test_label: 2  weak_label: 1\n",
      "[ 0.07938515  0.04284981  0.877765  ]  test_label: 0  weak_label: 0\n",
      "[ 0.07447063  0.04651833  0.87901103]  test_label: 0  weak_label: 1\n",
      "[ 0.0757871   0.044671    0.87954187]  test_label: 0  weak_label: 0\n",
      "[ 0.07728665  0.04411188  0.87860155]  test_label: 2  weak_label: 0\n",
      "[ 0.08105372  0.04254863  0.87639761]  test_label: 2  weak_label: 0\n",
      "[ 0.0870433   0.05013037  0.86282635]  test_label: 2  weak_label: 0\n",
      "[ 0.08243017  0.04566247  0.87190741]  test_label: 1  weak_label: 0\n",
      "[ 0.07439246  0.04670252  0.87890506]  test_label: 1  weak_label: 1\n",
      "[ 0.07296375  0.04746781  0.87956846]  test_label: 1  weak_label: 0\n",
      "[ 0.07336032  0.0446968   0.88194293]  test_label: 1  weak_label: 1\n",
      "[ 0.07451514  0.04523482  0.88024998]  test_label: 2  weak_label: 0\n",
      "[ 0.08013602  0.0454707   0.87439328]  test_label: 0  weak_label: 0\n",
      "[ 0.07683424  0.0489681   0.8741976 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07744366  0.05005765  0.87249869]  test_label: 0  weak_label: 0\n",
      "[ 0.0742765   0.04597512  0.8797484 ]  test_label: 0  weak_label: 1\n",
      "[ 0.0753233   0.04536769  0.879309  ]  test_label: 1  weak_label: 1\n",
      "[ 0.07881622  0.04596728  0.87521648]  test_label: 2  weak_label: 0\n",
      "[ 0.07788983  0.04575127  0.87635887]  test_label: 0  weak_label: 0\n",
      "[ 0.07671259  0.05115901  0.87212843]  test_label: 1  weak_label: 1\n",
      "[ 0.07800813  0.0486373   0.87335455]  test_label: 2  weak_label: 0\n",
      "[ 0.0797811   0.04599703  0.87422186]  test_label: 0  weak_label: 2\n",
      "[ 0.0749275   0.04501503  0.88005745]  test_label: 1  weak_label: 1\n",
      "[ 0.07988835  0.04795307  0.87215859]  test_label: 2  weak_label: 0\n",
      "[ 0.07103905  0.04370787  0.88525301]  test_label: 0  weak_label: 1\n",
      "[ 0.07817024  0.04548125  0.87634856]  test_label: 2  weak_label: 0\n",
      "[ 0.07328179  0.04437956  0.88233864]  test_label: 2  weak_label: 1\n",
      "[ 0.07565703  0.05051191  0.87383109]  test_label: 0  weak_label: 2\n",
      "[ 0.0765638   0.04910395  0.87433231]  test_label: 1  weak_label: 1\n",
      "[ 0.07608905  0.04991002  0.87400091]  test_label: 0  weak_label: 1\n",
      "[ 0.07472863  0.04200898  0.8832624 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07609287  0.04394111  0.87996602]  test_label: 2  weak_label: 1\n",
      "[ 0.07819306  0.05230967  0.8694973 ]  test_label: 0  weak_label: 1\n",
      "[ 0.0796013   0.04778351  0.87261522]  test_label: 0  weak_label: 2\n",
      "[ 0.07635672  0.05123581  0.8724075 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07542973  0.04435305  0.88021719]  test_label: 1  weak_label: 1\n",
      "[ 0.07438148  0.04546046  0.88015807]  test_label: 0  weak_label: 0\n",
      "[ 0.07874243  0.05400194  0.86725563]  test_label: 2  weak_label: 0\n",
      "[ 0.07860177  0.04982101  0.8715772 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07426437  0.04722951  0.87850612]  test_label: 0  weak_label: 0\n",
      "[ 0.07478376  0.04734631  0.8778699 ]  test_label: 1  weak_label: 1\n",
      "[ 0.07547061  0.04736264  0.87716675]  test_label: 2  weak_label: 0\n",
      "[ 0.07478997  0.04398135  0.88122869]  test_label: 0  weak_label: 1\n",
      "[ 0.07439157  0.04661123  0.87899721]  test_label: 0  weak_label: 0\n",
      "[ 0.07755239  0.04794771  0.87449992]  test_label: 2  weak_label: 0\n",
      "[ 0.0779672   0.04580965  0.87622315]  test_label: 0  weak_label: 1\n",
      "[ 0.07978445  0.04669261  0.87352288]  test_label: 0  weak_label: 0\n",
      "[ 0.07634773  0.04703426  0.87661797]  test_label: 0  weak_label: 1\n",
      "[ 0.07856397  0.04768563  0.87375033]  test_label: 2  weak_label: 0\n",
      "[ 0.07497273  0.04880555  0.87622178]  test_label: 1  weak_label: 0\n",
      "[ 0.07818113  0.04530205  0.87651682]  test_label: 2  weak_label: 0\n",
      "[ 0.07632092  0.04872565  0.87495339]  test_label: 2  weak_label: 0\n",
      "[ 0.07816695  0.04822574  0.87360728]  test_label: 2  weak_label: 1\n",
      "[ 0.07545653  0.04724356  0.87729985]  test_label: 0  weak_label: 0\n",
      "[ 0.07574263  0.04723101  0.87702638]  test_label: 2  weak_label: 1\n",
      "[ 0.07567749  0.04277924  0.88154328]  test_label: 1  weak_label: 0\n",
      "[ 0.07059959  0.04190823  0.88749218]  test_label: 1  weak_label: 0\n",
      "[ 0.07976156  0.04663355  0.87360495]  test_label: 0  weak_label: 0\n",
      "[ 0.07439817  0.04743875  0.8781631 ]  test_label: 1  weak_label: 2\n",
      "[ 0.07098942  0.04729674  0.88171387]  test_label: 2  weak_label: 0\n",
      "[ 0.07810469  0.04975571  0.87213963]  test_label: 2  weak_label: 0\n",
      "[ 0.07434455  0.04585829  0.87979716]  test_label: 2  weak_label: 0\n",
      "[ 0.0809596   0.04897667  0.87006372]  test_label: 2  weak_label: 1\n",
      "[ 0.07442908  0.04699381  0.87857717]  test_label: 1  weak_label: 2\n",
      "[ 0.07617466  0.04743664  0.87638861]  test_label: 1  weak_label: 0\n",
      "[ 0.07508902  0.04544371  0.87946719]  test_label: 0  weak_label: 0\n",
      "[ 0.07752047  0.04730103  0.87517852]  test_label: 2  weak_label: 0\n",
      "[ 0.07722836  0.04951743  0.87325418]  test_label: 0  weak_label: 1\n",
      "[ 0.08080342  0.04417668  0.87501997]  test_label: 0  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07710686  0.0466591   0.87623399]  test_label: 0  weak_label: 1\n",
      "[ 0.08089908  0.04917818  0.86992276]  test_label: 2  weak_label: 1\n",
      "[ 0.0791147   0.04636082  0.87452441]  test_label: 0  weak_label: 2\n",
      "[ 0.07971541  0.04585975  0.87442487]  test_label: 0  weak_label: 0\n",
      "[ 0.07412148  0.04320603  0.88267249]  test_label: 2  weak_label: 0\n",
      "[ 0.07702614  0.04719362  0.87578028]  test_label: 0  weak_label: 0\n",
      "[ 0.07913478  0.04810462  0.87276059]  test_label: 2  weak_label: 0\n",
      "[ 0.07382566  0.04688043  0.87929392]  test_label: 0  weak_label: 0\n",
      "[ 0.07904511  0.04609101  0.87486392]  test_label: 2  weak_label: 2\n",
      "[ 0.07859848  0.04807043  0.87333107]  test_label: 2  weak_label: 0\n",
      "[ 0.07644848  0.04581723  0.87773424]  test_label: 0  weak_label: 0\n",
      "[ 0.07695933  0.04704826  0.87599236]  test_label: 2  weak_label: 0\n",
      "[ 0.0760684   0.04251555  0.88141608]  test_label: 2  weak_label: 0\n",
      "[ 0.07700743  0.04364786  0.8793447 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07254839  0.04454376  0.88290781]  test_label: 0  weak_label: 2\n",
      "[ 0.08191651  0.05067113  0.86741239]  test_label: 2  weak_label: 0\n",
      "[ 0.0770383   0.05268382  0.87027794]  test_label: 0  weak_label: 1\n",
      "[ 0.07824544  0.05028946  0.87146509]  test_label: 0  weak_label: 1\n",
      "[ 0.07991329  0.04879473  0.87129194]  test_label: 2  weak_label: 1\n",
      "[ 0.07567045  0.04692839  0.87740117]  test_label: 1  weak_label: 0\n",
      "[ 0.07359282  0.05040719  0.87599993]  test_label: 2  weak_label: 0\n",
      "[ 0.07604669  0.04462241  0.87933093]  test_label: 0  weak_label: 0\n",
      "[ 0.07774188  0.04521503  0.87704307]  test_label: 0  weak_label: 0\n",
      "[ 0.07708509  0.04676303  0.87615192]  test_label: 1  weak_label: 2\n",
      "[ 0.07669027  0.04625207  0.87705761]  test_label: 0  weak_label: 0\n",
      "[ 0.07554372  0.04429933  0.88015699]  test_label: 2  weak_label: 0\n",
      "[ 0.07644037  0.04333719  0.88022244]  test_label: 0  weak_label: 0\n",
      "[ 0.07507237  0.04297113  0.88195658]  test_label: 2  weak_label: 0\n",
      "[ 0.07861099  0.0464685   0.87492049]  test_label: 2  weak_label: 0\n",
      "[ 0.07185075  0.04642333  0.88172591]  test_label: 0  weak_label: 0\n",
      "[ 0.07587772  0.04590088  0.87822139]  test_label: 2  weak_label: 1\n",
      "[ 0.07880109  0.04792576  0.87327319]  test_label: 0  weak_label: 1\n",
      "[ 0.07958662  0.05259086  0.86782253]  test_label: 0  weak_label: 0\n",
      "[ 0.07708258  0.04604326  0.87687415]  test_label: 2  weak_label: 1\n",
      "[ 0.07609688  0.04788497  0.87601817]  test_label: 0  weak_label: 0\n",
      "[ 0.07491006  0.04762738  0.87746257]  test_label: 2  weak_label: 0\n",
      "[ 0.07694771  0.04495822  0.87809402]  test_label: 2  weak_label: 0\n",
      "[ 0.07719859  0.04792479  0.87487656]  test_label: 1  weak_label: 0\n",
      "[ 0.07502408  0.05210008  0.87287593]  test_label: 2  weak_label: 0\n",
      "[ 0.07581926  0.04563535  0.8785454 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07004703  0.04181913  0.88813382]  test_label: 2  weak_label: 0\n",
      "[ 0.07807076  0.04806314  0.87386614]  test_label: 0  weak_label: 0\n",
      "[ 0.07483353  0.04445828  0.88070816]  test_label: 2  weak_label: 1\n",
      "[ 0.07862931  0.04510669  0.87626398]  test_label: 2  weak_label: 0\n",
      "[ 0.08059996  0.04586065  0.87353933]  test_label: 1  weak_label: 1\n",
      "[ 0.07177247  0.04308797  0.88513964]  test_label: 0  weak_label: 0\n",
      "[ 0.07520834  0.04744559  0.87734604]  test_label: 0  weak_label: 0\n",
      "[ 0.07675438  0.04519808  0.87804753]  test_label: 0  weak_label: 0\n",
      "[ 0.07573083  0.04615523  0.87811399]  test_label: 2  weak_label: 0\n",
      "[ 0.07576851  0.04499705  0.87923443]  test_label: 0  weak_label: 0\n",
      "[ 0.07724015  0.04382465  0.87893522]  test_label: 0  weak_label: 1\n",
      "[ 0.07419065  0.04375355  0.88205576]  test_label: 0  weak_label: 2\n",
      "[ 0.07826427  0.04381705  0.87791872]  test_label: 2  weak_label: 1\n",
      "[ 0.07711148  0.04406693  0.87882161]  test_label: 0  weak_label: 1\n",
      "[ 0.07263863  0.04404727  0.88331413]  test_label: 0  weak_label: 0\n",
      "[ 0.07632679  0.04392471  0.87974852]  test_label: 0  weak_label: 1\n",
      "[ 0.08175988  0.04616633  0.87207383]  test_label: 0  weak_label: 0\n",
      "[ 0.07667854  0.04542099  0.87790048]  test_label: 0  weak_label: 2\n",
      "[ 0.07887649  0.04183364  0.87928987]  test_label: 0  weak_label: 1\n",
      "[ 0.07344036  0.04914884  0.87741083]  test_label: 2  weak_label: 0\n",
      "[ 0.07700939  0.04690797  0.87608266]  test_label: 0  weak_label: 0\n",
      "[ 0.0767884   0.04813366  0.87507802]  test_label: 0  weak_label: 1\n",
      "[ 0.08001965  0.04341721  0.87656319]  test_label: 2  weak_label: 1\n",
      "[ 0.07582927  0.04503904  0.87913173]  test_label: 1  weak_label: 0\n",
      "[ 0.07868073  0.04710494  0.87421429]  test_label: 0  weak_label: 2\n",
      "[ 0.07982185  0.04864594  0.8715322 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07770935  0.04975086  0.87253982]  test_label: 2  weak_label: 0\n",
      "[ 0.07733709  0.04503127  0.87763166]  test_label: 0  weak_label: 1\n",
      "[ 0.07578631  0.04592086  0.8782928 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07925959  0.04713329  0.8736071 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07613822  0.04460524  0.87925649]  test_label: 0  weak_label: 1\n",
      "[ 0.07530646  0.04295354  0.88173997]  test_label: 1  weak_label: 1\n",
      "[ 0.07721589  0.04903519  0.8737489 ]  test_label: 0  weak_label: 0\n",
      "[ 0.0739767   0.04632005  0.87970322]  test_label: 0  weak_label: 1\n",
      "[ 0.07094525  0.03914782  0.88990694]  test_label: 0  weak_label: 0\n",
      "[ 0.07112924  0.04101972  0.887851  ]  test_label: 0  weak_label: 0\n",
      "[ 0.07480007  0.04744304  0.87775695]  test_label: 2  weak_label: 0\n",
      "[ 0.07666315  0.04642107  0.87691581]  test_label: 2  weak_label: 0\n",
      "[ 0.07602278  0.0464014   0.87757587]  test_label: 0  weak_label: 2\n",
      "[ 0.07669681  0.04612618  0.87717706]  test_label: 1  weak_label: 1\n",
      "[ 0.07685518  0.04823151  0.87491333]  test_label: 0  weak_label: 1\n",
      "[ 0.07587294  0.04153213  0.88259494]  test_label: 2  weak_label: 0\n",
      "[ 0.0753054   0.04690357  0.87779099]  test_label: 0  weak_label: 2\n",
      "[ 0.07793453  0.04887548  0.87318999]  test_label: 1  weak_label: 0\n",
      "[ 0.07538272  0.04411035  0.88050693]  test_label: 1  weak_label: 1\n",
      "[ 0.07529815  0.04237818  0.88232374]  test_label: 2  weak_label: 0\n",
      "[ 0.07739835  0.04702066  0.87558103]  test_label: 2  weak_label: 1\n",
      "[ 0.08206303  0.05044515  0.86749184]  test_label: 2  weak_label: 0\n",
      "[ 0.07769289  0.04679793  0.8755092 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07883359  0.0477146   0.87345177]  test_label: 2  weak_label: 0\n",
      "[ 0.07615643  0.04674081  0.87710273]  test_label: 0  weak_label: 2\n",
      "[ 0.07382184  0.0434096   0.88276857]  test_label: 0  weak_label: 0\n",
      "[ 0.07632942  0.04552001  0.87815058]  test_label: 2  weak_label: 1\n",
      "[ 0.0780516   0.04818064  0.87376785]  test_label: 0  weak_label: 0\n",
      "[ 0.07431768  0.04096712  0.88471526]  test_label: 2  weak_label: 1\n",
      "[ 0.07811186  0.04956949  0.87231869]  test_label: 0  weak_label: 0\n",
      "[ 0.07769087  0.04391286  0.87839627]  test_label: 0  weak_label: 1\n",
      "[ 0.0776591   0.04361486  0.87872607]  test_label: 2  weak_label: 1\n",
      "[ 0.0780475   0.04386069  0.87809181]  test_label: 0  weak_label: 0\n",
      "[ 0.0736003   0.04461986  0.88177985]  test_label: 0  weak_label: 2\n",
      "[ 0.07403151  0.04487285  0.88109559]  test_label: 0  weak_label: 2\n",
      "[ 0.07829013  0.04459888  0.87711102]  test_label: 0  weak_label: 0\n",
      "[ 0.07340459  0.04577049  0.88082486]  test_label: 0  weak_label: 1\n",
      "[ 0.07689557  0.04740572  0.87569875]  test_label: 0  weak_label: 0\n",
      "[ 0.07757393  0.04444722  0.87797886]  test_label: 0  weak_label: 1\n",
      "[ 0.07819197  0.04399734  0.87781066]  test_label: 0  weak_label: 1\n",
      "[ 0.07447939  0.04706088  0.87845975]  test_label: 2  weak_label: 1\n",
      "[ 0.07608303  0.04452895  0.87938803]  test_label: 2  weak_label: 0\n",
      "[ 0.07469171  0.04560123  0.8797071 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07685464  0.04332002  0.87982535]  test_label: 0  weak_label: 0\n",
      "[ 0.07007804  0.03999146  0.88993055]  test_label: 2  weak_label: 0\n",
      "[ 0.07718243  0.0471374   0.87568015]  test_label: 0  weak_label: 0\n",
      "[ 0.07529067  0.04604084  0.87866855]  test_label: 2  weak_label: 2\n",
      "[ 0.07921531  0.04664582  0.87413883]  test_label: 2  weak_label: 0\n",
      "[ 0.07849053  0.05057659  0.87093288]  test_label: 0  weak_label: 0\n",
      "[ 0.0744685   0.0452791   0.88025236]  test_label: 2  weak_label: 1\n",
      "[ 0.07664892  0.04500876  0.87834227]  test_label: 2  weak_label: 0\n",
      "[ 0.0790034   0.04823082  0.87276584]  test_label: 2  weak_label: 0\n",
      "[ 0.07845096  0.04389457  0.87765443]  test_label: 1  weak_label: 0\n",
      "[ 0.07836267  0.0439795   0.87765783]  test_label: 0  weak_label: 1\n",
      "[ 0.07853027  0.04862189  0.87284786]  test_label: 0  weak_label: 0\n",
      "[ 0.07632355  0.04785429  0.87582219]  test_label: 2  weak_label: 1\n",
      "[ 0.07594398  0.04737593  0.87668008]  test_label: 2  weak_label: 1\n",
      "[ 0.07931045  0.0454635   0.87522608]  test_label: 1  weak_label: 0\n",
      "[ 0.07448519  0.04157721  0.88393754]  test_label: 0  weak_label: 1\n",
      "[ 0.07622918  0.04738816  0.87638271]  test_label: 2  weak_label: 0\n",
      "[ 0.07398599  0.04471584  0.88129812]  test_label: 0  weak_label: 1\n",
      "[ 0.07871112  0.04696201  0.87432683]  test_label: 2  weak_label: 0\n",
      "[ 0.07481722  0.04879256  0.87639022]  test_label: 1  weak_label: 1\n",
      "[ 0.07412171  0.0442685   0.8816098 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07623837  0.04220953  0.8815521 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07815479  0.04870744  0.87313777]  test_label: 2  weak_label: 1\n",
      "[ 0.07346208  0.04733118  0.87920672]  test_label: 2  weak_label: 0\n",
      "[ 0.07929821  0.04811075  0.87259108]  test_label: 0  weak_label: 1\n",
      "[ 0.07757778  0.04608797  0.87633425]  test_label: 0  weak_label: 0\n",
      "[ 0.0761941   0.04633228  0.87747359]  test_label: 2  weak_label: 1\n",
      "[ 0.07294771  0.0431423   0.88391   ]  test_label: 0  weak_label: 0\n",
      "[ 0.07634475  0.04317303  0.88048226]  test_label: 0  weak_label: 0\n",
      "[ 0.07645531  0.04990312  0.87364161]  test_label: 1  weak_label: 2\n",
      "[ 0.08184434  0.04658373  0.87157196]  test_label: 2  weak_label: 2\n",
      "[ 0.07778958  0.04543546  0.87677503]  test_label: 0  weak_label: 2\n",
      "[ 0.07293209  0.04507957  0.88198835]  test_label: 2  weak_label: 1\n",
      "[ 0.0782555   0.04689801  0.87484646]  test_label: 0  weak_label: 0\n",
      "[ 0.07609677  0.04560582  0.87829739]  test_label: 0  weak_label: 0\n",
      "[ 0.07674799  0.04489638  0.87835562]  test_label: 1  weak_label: 0\n",
      "[ 0.07622817  0.04640391  0.87736797]  test_label: 0  weak_label: 1\n",
      "[ 0.07666042  0.04953546  0.87380403]  test_label: 0  weak_label: 0\n",
      "[ 0.07643812  0.04502461  0.8785373 ]  test_label: 0  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07458787  0.05035783  0.87505424]  test_label: 2  weak_label: 0\n",
      "[ 0.07757503  0.04788876  0.87453622]  test_label: 0  weak_label: 1\n",
      "[ 0.07562824  0.04926878  0.875103  ]  test_label: 1  weak_label: 0\n",
      "[ 0.07831099  0.0459839   0.87570512]  test_label: 2  weak_label: 1\n",
      "[ 0.07426885  0.04289051  0.88284063]  test_label: 0  weak_label: 1\n",
      "[ 0.07805273  0.04921089  0.87273639]  test_label: 2  weak_label: 2\n",
      "[ 0.07888001  0.04075365  0.88036638]  test_label: 2  weak_label: 0\n",
      "[ 0.07635033  0.04661885  0.87703085]  test_label: 0  weak_label: 0\n",
      "[ 0.08252972  0.04627773  0.87119251]  test_label: 0  weak_label: 0\n",
      "[ 0.07785185  0.04717712  0.87497103]  test_label: 1  weak_label: 1\n",
      "[ 0.07635922  0.04415928  0.87948149]  test_label: 2  weak_label: 1\n",
      "[ 0.07771444  0.04954935  0.87273622]  test_label: 1  weak_label: 1\n",
      "[ 0.07502719  0.04841599  0.87655675]  test_label: 0  weak_label: 0\n",
      "[ 0.08142447  0.04641297  0.87216258]  test_label: 0  weak_label: 0\n",
      "[ 0.07939268  0.04207359  0.87853372]  test_label: 2  weak_label: 0\n",
      "[ 0.07559962  0.04397481  0.88042563]  test_label: 2  weak_label: 0\n",
      "[ 0.07776734  0.04640083  0.87583184]  test_label: 1  weak_label: 1\n",
      "[ 0.07261863  0.04304771  0.88433367]  test_label: 0  weak_label: 0\n",
      "[ 0.07924176  0.04396223  0.87679601]  test_label: 0  weak_label: 2\n",
      "[ 0.07437934  0.04982895  0.87579173]  test_label: 1  weak_label: 0\n",
      "[ 0.07655909  0.04527688  0.87816399]  test_label: 0  weak_label: 0\n",
      "[ 0.08169231  0.04813787  0.87016988]  test_label: 1  weak_label: 0\n",
      "[ 0.07486735  0.0447959   0.88033682]  test_label: 0  weak_label: 1\n",
      "[ 0.07152475  0.04569859  0.88277662]  test_label: 0  weak_label: 0\n",
      "[ 0.076241    0.04501612  0.87874287]  test_label: 2  weak_label: 1\n",
      "[ 0.07713416  0.04733287  0.87553293]  test_label: 1  weak_label: 1\n",
      "[ 0.07225446  0.04622266  0.88152289]  test_label: 0  weak_label: 0\n",
      "[ 0.07701477  0.0458573   0.87712795]  test_label: 0  weak_label: 0\n",
      "[ 0.07924559  0.04327128  0.87748313]  test_label: 0  weak_label: 1\n",
      "[ 0.07668482  0.04837971  0.87493545]  test_label: 1  weak_label: 0\n",
      "[ 0.0766428   0.04589458  0.87746269]  test_label: 1  weak_label: 2\n",
      "[ 0.07953657  0.04808031  0.87238306]  test_label: 0  weak_label: 0\n",
      "[ 0.07590889  0.04570308  0.87838799]  test_label: 0  weak_label: 0\n",
      "[ 0.07640459  0.04611104  0.87748432]  test_label: 2  weak_label: 0\n",
      "[ 0.07636664  0.05060526  0.87302816]  test_label: 2  weak_label: 0\n",
      "[ 0.07561518  0.04429346  0.88009131]  test_label: 0  weak_label: 1\n",
      "[ 0.07619315  0.04864237  0.87516445]  test_label: 0  weak_label: 1\n",
      "[ 0.0761601   0.04916807  0.87467182]  test_label: 2  weak_label: 1\n",
      "[ 0.07627334  0.04247647  0.88125014]  test_label: 2  weak_label: 1\n",
      "[ 0.07544228  0.04551362  0.87904412]  test_label: 2  weak_label: 1\n",
      "[ 0.07665485  0.04673705  0.87660807]  test_label: 2  weak_label: 0\n",
      "[ 0.07630825  0.04730492  0.87638682]  test_label: 0  weak_label: 0\n",
      "[ 0.07605414  0.04758653  0.87635934]  test_label: 0  weak_label: 2\n",
      "[ 0.0751737   0.05351057  0.87131572]  test_label: 0  weak_label: 0\n",
      "[ 0.07417237  0.0448514   0.8809762 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07611845  0.04093593  0.88294554]  test_label: 0  weak_label: 1\n",
      "[ 0.07612715  0.04415774  0.87971514]  test_label: 0  weak_label: 2\n",
      "[ 0.07447487  0.04922762  0.87629747]  test_label: 2  weak_label: 0\n",
      "[ 0.07699554  0.04587887  0.87712556]  test_label: 2  weak_label: 1\n",
      "[ 0.07247756  0.0419991   0.88552338]  test_label: 0  weak_label: 0\n",
      "[ 0.07480789  0.04591376  0.8792783 ]  test_label: 0  weak_label: 2\n",
      "[ 0.07241431  0.04569008  0.8818956 ]  test_label: 2  weak_label: 0\n",
      "[ 0.07986047  0.04679691  0.87334263]  test_label: 0  weak_label: 0\n",
      "[ 0.0753589   0.04836475  0.87627631]  test_label: 2  weak_label: 1\n",
      "[ 0.07741221  0.04694628  0.87564152]  test_label: 0  weak_label: 1\n",
      "[ 0.07630131  0.04655473  0.87714392]  test_label: 0  weak_label: 1\n",
      "[ 0.07894194  0.04847485  0.87258315]  test_label: 2  weak_label: 0\n",
      "[ 0.07751803  0.04810394  0.87437803]  test_label: 1  weak_label: 0\n",
      "[ 0.07682157  0.04321047  0.87996799]  test_label: 2  weak_label: 1\n",
      "[ 0.07456026  0.04808291  0.87735683]  test_label: 0  weak_label: 0\n",
      "[ 0.07296067  0.04650534  0.88053399]  test_label: 2  weak_label: 2\n",
      "[ 0.07655469  0.04850664  0.87493873]  test_label: 0  weak_label: 0\n",
      "[ 0.08001033  0.04851142  0.8714782 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07703588  0.04791052  0.87505364]  test_label: 0  weak_label: 2\n",
      "[ 0.07758411  0.04936165  0.87305421]  test_label: 0  weak_label: 1\n",
      "[ 0.07492211  0.04390776  0.88117009]  test_label: 1  weak_label: 2\n",
      "[ 0.08198868  0.04235175  0.87565953]  test_label: 2  weak_label: 1\n",
      "[ 0.07405148  0.04951614  0.87643242]  test_label: 2  weak_label: 1\n",
      "[ 0.08191935  0.04785884  0.87022185]  test_label: 2  weak_label: 0\n",
      "[ 0.07573947  0.04492508  0.87933546]  test_label: 0  weak_label: 0\n",
      "[ 0.07685819  0.04834609  0.87479573]  test_label: 0  weak_label: 0\n",
      "[ 0.07330517  0.0435201   0.88317472]  test_label: 2  weak_label: 1\n",
      "[ 0.07914381  0.04347315  0.87738299]  test_label: 2  weak_label: 1\n",
      "[ 0.07626145  0.0472816   0.87645698]  test_label: 2  weak_label: 0\n",
      "[ 0.0763015   0.04364194  0.88005662]  test_label: 0  weak_label: 0\n",
      "[ 0.07736863  0.04381327  0.87881809]  test_label: 0  weak_label: 1\n",
      "[ 0.07196561  0.04640479  0.88162965]  test_label: 0  weak_label: 1\n",
      "[ 0.07710452  0.04525286  0.87764257]  test_label: 2  weak_label: 1\n",
      "[ 0.08003321  0.04871214  0.87125468]  test_label: 2  weak_label: 1\n",
      "[ 0.08168653  0.04419646  0.87411702]  test_label: 0  weak_label: 0\n",
      "[ 0.07226288  0.04561244  0.88212472]  test_label: 0  weak_label: 0\n",
      "[ 0.07464187  0.04209466  0.88326353]  test_label: 2  weak_label: 0\n",
      "[ 0.07589619  0.04787862  0.87622511]  test_label: 1  weak_label: 0\n",
      "[ 0.07379029  0.04583552  0.88037413]  test_label: 0  weak_label: 0\n",
      "[ 0.07704604  0.04872696  0.87422693]  test_label: 2  weak_label: 0\n",
      "[ 0.0764831   0.05129642  0.87222046]  test_label: 2  weak_label: 1\n",
      "[ 0.07674953  0.0465631   0.87668741]  test_label: 0  weak_label: 0\n",
      "[ 0.07688081  0.04521403  0.87790519]  test_label: 0  weak_label: 0\n",
      "[ 0.07603023  0.04986811  0.87410158]  test_label: 0  weak_label: 2\n",
      "[ 0.08042736  0.04853175  0.87104088]  test_label: 2  weak_label: 0\n",
      "[ 0.07545339  0.0438952   0.88065141]  test_label: 2  weak_label: 1\n",
      "[ 0.07727668  0.0511555   0.87156779]  test_label: 2  weak_label: 1\n",
      "[ 0.07415499  0.04909736  0.87674761]  test_label: 0  weak_label: 1\n",
      "[ 0.07439718  0.04348309  0.88211966]  test_label: 2  weak_label: 0\n",
      "[ 0.08064961  0.04810591  0.87124443]  test_label: 2  weak_label: 0\n",
      "[ 0.0764093   0.04442894  0.87916178]  test_label: 0  weak_label: 0\n",
      "[ 0.07557195  0.04272507  0.88170302]  test_label: 0  weak_label: 0\n",
      "[ 0.07704592  0.04507458  0.87787944]  test_label: 0  weak_label: 0\n",
      "[ 0.07590961  0.04491571  0.87917465]  test_label: 0  weak_label: 0\n",
      "[ 0.07665271  0.04521112  0.87813616]  test_label: 0  weak_label: 1\n",
      "[ 0.07940641  0.04948639  0.87110716]  test_label: 0  weak_label: 2\n",
      "[ 0.07979629  0.0479889   0.87221479]  test_label: 2  weak_label: 0\n",
      "[ 0.07921589  0.047113    0.87367111]  test_label: 1  weak_label: 0\n",
      "[ 0.0773802   0.04704522  0.87557459]  test_label: 0  weak_label: 0\n",
      "[ 0.07442668  0.03987398  0.88569933]  test_label: 2  weak_label: 0\n",
      "[ 0.07924171  0.04988169  0.87087655]  test_label: 2  weak_label: 1\n",
      "[ 0.07338969  0.04269553  0.88391483]  test_label: 0  weak_label: 0\n",
      "[ 0.08076923  0.04780892  0.87142181]  test_label: 1  weak_label: 0\n",
      "[ 0.07275361  0.04288396  0.8843624 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07624351  0.04540568  0.87835085]  test_label: 2  weak_label: 1\n",
      "[ 0.07595239  0.04640854  0.87763911]  test_label: 2  weak_label: 1\n",
      "[ 0.07724637  0.04968442  0.87306923]  test_label: 2  weak_label: 0\n",
      "[ 0.07209607  0.04666119  0.88124275]  test_label: 2  weak_label: 0\n",
      "[ 0.07677533  0.04286803  0.88035667]  test_label: 1  weak_label: 1\n",
      "[ 0.07834934  0.04573382  0.87591684]  test_label: 0  weak_label: 0\n",
      "[ 0.07874749  0.05102941  0.8702231 ]  test_label: 2  weak_label: 1\n",
      "[ 0.07571305  0.04219966  0.88208729]  test_label: 2  weak_label: 2\n",
      "[ 0.07670832  0.04710045  0.8761912 ]  test_label: 0  weak_label: 0\n",
      "[ 0.0760491   0.04601863  0.87793231]  test_label: 0  weak_label: 0\n",
      "[ 0.07936473  0.05004134  0.87059397]  test_label: 1  weak_label: 0\n",
      "[ 0.07998294  0.04895855  0.87105852]  test_label: 2  weak_label: 1\n",
      "[ 0.07538627  0.04450087  0.88011283]  test_label: 2  weak_label: 0\n",
      "[ 0.0719583   0.04681291  0.8812288 ]  test_label: 0  weak_label: 0\n",
      "[ 0.07796279  0.05132579  0.87071145]  test_label: 0  weak_label: 1\n",
      "[ 0.07419206  0.04696184  0.87884611]  test_label: 2  weak_label: 0\n",
      "[ 0.07700062  0.04968856  0.8733108 ]  test_label: 1  weak_label: 0\n",
      "[ 0.07435103  0.04692243  0.87872654]  test_label: 2  weak_label: 0\n",
      "[ 0.07665992  0.04479989  0.87854022]  test_label: 0  weak_label: 0\n",
      "[ 0.07481964  0.0454689   0.87971145]  test_label: 2  weak_label: 0\n",
      "[ 0.07638584  0.04387375  0.87974042]  test_label: 2  weak_label: 0\n",
      "[ 0.07186644  0.04569165  0.88244194]  test_label: 0  weak_label: 0\n",
      "[ 0.07578897  0.04708577  0.8771252 ]  test_label: 0  weak_label: 1\n",
      "[ 0.07425747  0.04572071  0.88002181]  test_label: 1  weak_label: 0\n",
      "[ 0.07706601  0.04700546  0.87592852]  test_label: 0  weak_label: 1\n",
      "[ 0.07471896  0.04618286  0.87909818]  test_label: 2  weak_label: 0\n",
      "[ 0.07003641  0.04484789  0.88511568]  test_label: 2  weak_label: 2\n",
      "[ 0.07599554  0.04392213  0.88008237]  test_label: 2  weak_label: 0\n",
      "[ 0.08038919  0.04709506  0.87251574]  test_label: 2  weak_label: 1\n",
      "[ 0.08019758  0.04570895  0.87409347]  test_label: 0  weak_label: 2\n",
      "[ 0.07771967  0.04492053  0.87735981]  test_label: 2  weak_label: 2\n",
      "[ 0.07112924  0.04101972  0.887851  ]  test_label: 0  weak_label: 0\n",
      "[ 0.07819415  0.04469824  0.87710762]  test_label: 0  weak_label: 0\n",
      "[ 0.07883096  0.04854573  0.87262326]  test_label: 2  weak_label: 0\n",
      "[ 0.08021992  0.0458075   0.87397254]  test_label: 0  weak_label: 1\n",
      "[ 0.07612134  0.04610261  0.87777603]  test_label: 1  weak_label: 2\n"
     ]
    }
   ],
   "source": [
    "conf_prob = predictions['proba']\n",
    "\n",
    "t = X_n_train.shape[0]\n",
    "\n",
    "for i in range(X_n_test.shape[0]):\n",
    "    print(conf_prob[i],\" test_label:\",y_test[i],\" weak_label:\",Y_weak[t+i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3625866050808314"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = [pred_conf_score == true_conf_score]\n",
    "sum(sum(num_correct))/len(pred_conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpoo8qf2hh\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_model_dir': '/tmp/tmpoo8qf2hh', '_save_summary_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=classifier_model_fn, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model (full supervision) 2nd method\n",
    "num_epoch = 50\n",
    "train_params = dict(batch_size=30, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_train, \"ns\": ns_train}, y=y_conf,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_test = np.reshape(ns_test,newshape=(len(ns_test),))\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_test, \"ns\": ns_test}, y=true_conf_score,\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2320\n",
      "INFO:tensorflow:Saving checkpoints for 2321 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.307958, step = 2321\n",
      "INFO:tensorflow:global_step/sec: 196.142\n",
      "INFO:tensorflow:loss = 0.307248, step = 2421 (0.511 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2436 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.290106.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:06:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2436\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:06:59\n",
      "INFO:tensorflow:Saving dict for global step 2436: accuracy = 0.364896, cross_entropy_loss = 2.60672, global_step = 2436, loss = 2.89108\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2436\n",
      "INFO:tensorflow:Saving checkpoints for 2437 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.305771, step = 2437\n",
      "INFO:tensorflow:global_step/sec: 189.21\n",
      "INFO:tensorflow:loss = 0.304984, step = 2537 (0.530 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2552 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.289475.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:01\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2552\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:01\n",
      "INFO:tensorflow:Saving dict for global step 2552: accuracy = 0.363741, cross_entropy_loss = 2.64708, global_step = 2552, loss = 2.93133\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2552\n",
      "INFO:tensorflow:Saving checkpoints for 2553 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.303891, step = 2553\n",
      "INFO:tensorflow:global_step/sec: 182.009\n",
      "INFO:tensorflow:loss = 0.303059, step = 2653 (0.551 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2668 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.288917.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:04\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2668\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:04\n",
      "INFO:tensorflow:Saving dict for global step 2668: accuracy = 0.366051, cross_entropy_loss = 2.68457, global_step = 2668, loss = 2.9687\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2668\n",
      "INFO:tensorflow:Saving checkpoints for 2669 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.302255, step = 2669\n",
      "INFO:tensorflow:global_step/sec: 186.852\n",
      "INFO:tensorflow:loss = 0.301398, step = 2769 (0.536 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2784 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.288416.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2784\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:06\n",
      "INFO:tensorflow:Saving dict for global step 2784: accuracy = 0.37067, cross_entropy_loss = 2.71943, global_step = 2784, loss = 3.00343\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2784\n",
      "INFO:tensorflow:Saving checkpoints for 2785 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.300814, step = 2785\n",
      "INFO:tensorflow:global_step/sec: 184.637\n",
      "INFO:tensorflow:loss = 0.299938, step = 2885 (0.543 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2900 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.287962.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2900\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:09\n",
      "INFO:tensorflow:Saving dict for global step 2900: accuracy = 0.374134, cross_entropy_loss = 2.75185, global_step = 2900, loss = 3.03572\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-2900\n",
      "INFO:tensorflow:Saving checkpoints for 2901 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.299526, step = 2901\n",
      "INFO:tensorflow:global_step/sec: 183.194\n",
      "INFO:tensorflow:loss = 0.298643, step = 3001 (0.547 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3016 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.287547.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3016\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:11\n",
      "INFO:tensorflow:Saving dict for global step 3016: accuracy = 0.374134, cross_entropy_loss = 2.7821, global_step = 3016, loss = 3.06582\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3016\n",
      "INFO:tensorflow:Saving checkpoints for 3017 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.298364, step = 3017\n",
      "INFO:tensorflow:global_step/sec: 193.763\n",
      "INFO:tensorflow:loss = 0.297485, step = 3117 (0.517 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3132 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.287165.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3132\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:13\n",
      "INFO:tensorflow:Saving dict for global step 3132: accuracy = 0.371824, cross_entropy_loss = 2.81041, global_step = 3132, loss = 3.094\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3132\n",
      "INFO:tensorflow:Saving checkpoints for 3133 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29731, step = 3133\n",
      "INFO:tensorflow:global_step/sec: 191.72\n",
      "INFO:tensorflow:loss = 0.296441, step = 3233 (0.522 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3248 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.286812.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3248\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:16\n",
      "INFO:tensorflow:Saving dict for global step 3248: accuracy = 0.369515, cross_entropy_loss = 2.83697, global_step = 3248, loss = 3.12041\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3248\n",
      "INFO:tensorflow:Saving checkpoints for 3249 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.296346, step = 3249\n",
      "INFO:tensorflow:global_step/sec: 191.413\n",
      "INFO:tensorflow:loss = 0.295492, step = 3349 (0.524 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3364 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28648.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:18\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3364\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:18\n",
      "INFO:tensorflow:Saving dict for global step 3364: accuracy = 0.369515, cross_entropy_loss = 2.86193, global_step = 3364, loss = 3.14523\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3364\n",
      "INFO:tensorflow:Saving checkpoints for 3365 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.295461, step = 3365\n",
      "INFO:tensorflow:global_step/sec: 193.927\n",
      "INFO:tensorflow:loss = 0.294628, step = 3465 (0.517 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3480 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.286167.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:20\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3480\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:21\n",
      "INFO:tensorflow:Saving dict for global step 3480: accuracy = 0.369515, cross_entropy_loss = 2.88545, global_step = 3480, loss = 3.1686\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3480\n",
      "INFO:tensorflow:Saving checkpoints for 3481 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.294645, step = 3481\n",
      "INFO:tensorflow:global_step/sec: 193.212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.293836, step = 3581 (0.519 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3596 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28587.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3596\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:23\n",
      "INFO:tensorflow:Saving dict for global step 3596: accuracy = 0.369515, cross_entropy_loss = 2.90766, global_step = 3596, loss = 3.19066\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3596\n",
      "INFO:tensorflow:Saving checkpoints for 3597 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.293889, step = 3597\n",
      "INFO:tensorflow:global_step/sec: 193.657\n",
      "INFO:tensorflow:loss = 0.293109, step = 3697 (0.517 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3712 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.285585.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:25\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3712\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:25\n",
      "INFO:tensorflow:Saving dict for global step 3712: accuracy = 0.37067, cross_entropy_loss = 2.92869, global_step = 3712, loss = 3.21154\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3712\n",
      "INFO:tensorflow:Saving checkpoints for 3713 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.293187, step = 3713\n",
      "INFO:tensorflow:global_step/sec: 190.915\n",
      "INFO:tensorflow:loss = 0.292436, step = 3813 (0.525 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3828 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.285312.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:27\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3828\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:27\n",
      "INFO:tensorflow:Saving dict for global step 3828: accuracy = 0.37067, cross_entropy_loss = 2.94864, global_step = 3828, loss = 3.23133\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3828\n",
      "INFO:tensorflow:Saving checkpoints for 3829 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.292532, step = 3829\n",
      "INFO:tensorflow:global_step/sec: 192.826\n",
      "INFO:tensorflow:loss = 0.291813, step = 3929 (0.520 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3944 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.285048.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3944\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:30\n",
      "INFO:tensorflow:Saving dict for global step 3944: accuracy = 0.371824, cross_entropy_loss = 2.96761, global_step = 3944, loss = 3.25015\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-3944\n",
      "INFO:tensorflow:Saving checkpoints for 3945 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.291919, step = 3945\n",
      "INFO:tensorflow:global_step/sec: 193.371\n",
      "INFO:tensorflow:loss = 0.291231, step = 4045 (0.518 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4060 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284793.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:32\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4060\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:32\n",
      "INFO:tensorflow:Saving dict for global step 4060: accuracy = 0.371824, cross_entropy_loss = 2.98568, global_step = 4060, loss = 3.26806\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4060\n",
      "INFO:tensorflow:Saving checkpoints for 4061 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.291344, step = 4061\n",
      "INFO:tensorflow:global_step/sec: 194.441\n",
      "INFO:tensorflow:loss = 0.290686, step = 4161 (0.515 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4176 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284544.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4176\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:35\n",
      "INFO:tensorflow:Saving dict for global step 4176: accuracy = 0.372979, cross_entropy_loss = 3.00294, global_step = 4176, loss = 3.28515\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4176\n",
      "INFO:tensorflow:Saving checkpoints for 4177 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.290802, step = 4177\n",
      "INFO:tensorflow:global_step/sec: 193.774\n",
      "INFO:tensorflow:loss = 0.290174, step = 4277 (0.517 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4292 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284302.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:37\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4292\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:37\n",
      "INFO:tensorflow:Saving dict for global step 4292: accuracy = 0.372979, cross_entropy_loss = 3.01943, global_step = 4292, loss = 3.30149\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4292\n",
      "INFO:tensorflow:Saving checkpoints for 4293 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29029, step = 4293\n",
      "INFO:tensorflow:global_step/sec: 184.991\n",
      "INFO:tensorflow:loss = 0.28969, step = 4393 (0.542 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4408 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284065.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4408\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:40\n",
      "INFO:tensorflow:Saving dict for global step 4408: accuracy = 0.371824, cross_entropy_loss = 3.03523, global_step = 4408, loss = 3.31713\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4408\n",
      "INFO:tensorflow:Saving checkpoints for 4409 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.289804, step = 4409\n",
      "INFO:tensorflow:global_step/sec: 181.747\n",
      "INFO:tensorflow:loss = 0.289232, step = 4509 (0.551 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4524 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283833.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:41\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4524\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:42\n",
      "INFO:tensorflow:Saving dict for global step 4524: accuracy = 0.372979, cross_entropy_loss = 3.05038, global_step = 4524, loss = 3.33212\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4524\n",
      "INFO:tensorflow:Saving checkpoints for 4525 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.289343, step = 4525\n",
      "INFO:tensorflow:global_step/sec: 187.246\n",
      "INFO:tensorflow:loss = 0.288796, step = 4625 (0.535 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4640 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283605.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:44\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4640\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:44\n",
      "INFO:tensorflow:Saving dict for global step 4640: accuracy = 0.371824, cross_entropy_loss = 3.06493, global_step = 4640, loss = 3.34651\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4640\n",
      "INFO:tensorflow:Saving checkpoints for 4641 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.288903, step = 4641\n",
      "INFO:tensorflow:global_step/sec: 186.514\n",
      "INFO:tensorflow:loss = 0.28838, step = 4741 (0.537 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4756 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283381.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:47\n",
      "INFO:tensorflow:Saving dict for global step 4756: accuracy = 0.371824, cross_entropy_loss = 3.07892, global_step = 4756, loss = 3.36033\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4756\n",
      "INFO:tensorflow:Saving checkpoints for 4757 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.288482, step = 4757\n",
      "INFO:tensorflow:global_step/sec: 181.515\n",
      "INFO:tensorflow:loss = 0.287981, step = 4857 (0.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4872 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283161.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:49\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4872\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:49\n",
      "INFO:tensorflow:Saving dict for global step 4872: accuracy = 0.371824, cross_entropy_loss = 3.09239, global_step = 4872, loss = 3.37364\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4872\n",
      "INFO:tensorflow:Saving checkpoints for 4873 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.288078, step = 4873\n",
      "INFO:tensorflow:global_step/sec: 176.454\n",
      "INFO:tensorflow:loss = 0.287599, step = 4973 (0.568 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4988 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282943.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4988\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:52\n",
      "INFO:tensorflow:Saving dict for global step 4988: accuracy = 0.371824, cross_entropy_loss = 3.10537, global_step = 4988, loss = 3.38645\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-4988\n",
      "INFO:tensorflow:Saving checkpoints for 4989 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.287691, step = 4989\n",
      "INFO:tensorflow:global_step/sec: 175.804\n",
      "INFO:tensorflow:loss = 0.287231, step = 5089 (0.570 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5104 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282728.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5104\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:54\n",
      "INFO:tensorflow:Saving dict for global step 5104: accuracy = 0.371824, cross_entropy_loss = 3.11789, global_step = 5104, loss = 3.39881\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5104\n",
      "INFO:tensorflow:Saving checkpoints for 5105 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.287317, step = 5105\n",
      "INFO:tensorflow:global_step/sec: 179.549\n",
      "INFO:tensorflow:loss = 0.286876, step = 5205 (0.559 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5220 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282516.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5220\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:57\n",
      "INFO:tensorflow:Saving dict for global step 5220: accuracy = 0.37067, cross_entropy_loss = 3.12998, global_step = 5220, loss = 3.41073\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5220\n",
      "INFO:tensorflow:Saving checkpoints for 5221 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.286957, step = 5221\n",
      "INFO:tensorflow:global_step/sec: 182.023\n",
      "INFO:tensorflow:loss = 0.286533, step = 5321 (0.551 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5336 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282306.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:07:58\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5336\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:07:59\n",
      "INFO:tensorflow:Saving dict for global step 5336: accuracy = 0.37067, cross_entropy_loss = 3.14166, global_step = 5336, loss = 3.42224\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5336\n",
      "INFO:tensorflow:Saving checkpoints for 5337 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.286609, step = 5337\n",
      "INFO:tensorflow:global_step/sec: 179.975\n",
      "INFO:tensorflow:loss = 0.286201, step = 5437 (0.557 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5452 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282099.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:01\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5452\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:02\n",
      "INFO:tensorflow:Saving dict for global step 5452: accuracy = 0.37067, cross_entropy_loss = 3.15295, global_step = 5452, loss = 3.43337\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5452\n",
      "INFO:tensorflow:Saving checkpoints for 5453 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.286271, step = 5453\n",
      "INFO:tensorflow:global_step/sec: 184.247\n",
      "INFO:tensorflow:loss = 0.285878, step = 5553 (0.544 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5568 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281893.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:03\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5568\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:04\n",
      "INFO:tensorflow:Saving dict for global step 5568: accuracy = 0.369515, cross_entropy_loss = 3.16389, global_step = 5568, loss = 3.44414\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5568\n",
      "INFO:tensorflow:Saving checkpoints for 5569 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285943, step = 5569\n",
      "INFO:tensorflow:global_step/sec: 175.213\n",
      "INFO:tensorflow:loss = 0.285564, step = 5669 (0.572 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5684 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281689.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5684\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:07\n",
      "INFO:tensorflow:Saving dict for global step 5684: accuracy = 0.369515, cross_entropy_loss = 3.17448, global_step = 5684, loss = 3.45456\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5684\n",
      "INFO:tensorflow:Saving checkpoints for 5685 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285625, step = 5685\n",
      "INFO:tensorflow:global_step/sec: 179.25\n",
      "INFO:tensorflow:loss = 0.285259, step = 5785 (0.559 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5800 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281487.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5800\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:09\n",
      "INFO:tensorflow:Saving dict for global step 5800: accuracy = 0.369515, cross_entropy_loss = 3.18474, global_step = 5800, loss = 3.46466\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5800\n",
      "INFO:tensorflow:Saving checkpoints for 5801 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285315, step = 5801\n",
      "INFO:tensorflow:global_step/sec: 178.446\n",
      "INFO:tensorflow:loss = 0.284961, step = 5901 (0.562 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5916 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281286.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5916\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:12\n",
      "INFO:tensorflow:Saving dict for global step 5916: accuracy = 0.36836, cross_entropy_loss = 3.19469, global_step = 5916, loss = 3.47444\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-5916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving checkpoints for 5917 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285012, step = 5917\n",
      "INFO:tensorflow:global_step/sec: 177.444\n",
      "INFO:tensorflow:loss = 0.28467, step = 6017 (0.565 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6032 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281087.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6032\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:14\n",
      "INFO:tensorflow:Saving dict for global step 6032: accuracy = 0.367206, cross_entropy_loss = 3.20435, global_step = 6032, loss = 3.48394\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6032\n",
      "INFO:tensorflow:Saving checkpoints for 6033 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.284717, step = 6033\n",
      "INFO:tensorflow:global_step/sec: 177.124\n",
      "INFO:tensorflow:loss = 0.284385, step = 6133 (0.566 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6148 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280889.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6148\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:17\n",
      "INFO:tensorflow:Saving dict for global step 6148: accuracy = 0.367206, cross_entropy_loss = 3.21373, global_step = 6148, loss = 3.49315\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6148\n",
      "INFO:tensorflow:Saving checkpoints for 6149 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.284428, step = 6149\n",
      "INFO:tensorflow:global_step/sec: 179.443\n",
      "INFO:tensorflow:loss = 0.284106, step = 6249 (0.559 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6264 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280692.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:19\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6264\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:19\n",
      "INFO:tensorflow:Saving dict for global step 6264: accuracy = 0.367206, cross_entropy_loss = 3.22285, global_step = 6264, loss = 3.50209\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6264\n",
      "INFO:tensorflow:Saving checkpoints for 6265 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.284145, step = 6265\n",
      "INFO:tensorflow:global_step/sec: 176.937\n",
      "INFO:tensorflow:loss = 0.283833, step = 6365 (0.567 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6380 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280497.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6380\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:22\n",
      "INFO:tensorflow:Saving dict for global step 6380: accuracy = 0.367206, cross_entropy_loss = 3.2317, global_step = 6380, loss = 3.51078\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6380\n",
      "INFO:tensorflow:Saving checkpoints for 6381 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285254, step = 6381\n",
      "INFO:tensorflow:global_step/sec: 181.342\n",
      "INFO:tensorflow:loss = 0.283536, step = 6481 (0.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6496 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280306.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6496\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:24\n",
      "INFO:tensorflow:Saving dict for global step 6496: accuracy = 0.367206, cross_entropy_loss = 3.2396, global_step = 6496, loss = 3.51851\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6496\n",
      "INFO:tensorflow:Saving checkpoints for 6497 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.283605, step = 6497\n",
      "INFO:tensorflow:global_step/sec: 184.749\n",
      "INFO:tensorflow:loss = 0.283303, step = 6597 (0.543 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6612 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280107.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:26\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6612\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:26\n",
      "INFO:tensorflow:Saving dict for global step 6612: accuracy = 0.369515, cross_entropy_loss = 3.24874, global_step = 6612, loss = 3.52748\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6612\n",
      "INFO:tensorflow:Saving checkpoints for 6613 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.283343, step = 6613\n",
      "INFO:tensorflow:global_step/sec: 176.867\n",
      "INFO:tensorflow:loss = 0.283045, step = 6713 (0.567 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6728 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.279915.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:28\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6728\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:29\n",
      "INFO:tensorflow:Saving dict for global step 6728: accuracy = 0.369515, cross_entropy_loss = 3.2569, global_step = 6728, loss = 3.53547\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6728\n",
      "INFO:tensorflow:Saving checkpoints for 6729 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.283079, step = 6729\n",
      "INFO:tensorflow:global_step/sec: 176.446\n",
      "INFO:tensorflow:loss = 0.28279, step = 6829 (0.568 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6844 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.279724.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6844\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:31\n",
      "INFO:tensorflow:Saving dict for global step 6844: accuracy = 0.369515, cross_entropy_loss = 3.26482, global_step = 6844, loss = 3.54323\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6844\n",
      "INFO:tensorflow:Saving checkpoints for 6845 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282819, step = 6845\n",
      "INFO:tensorflow:global_step/sec: 177.417\n",
      "INFO:tensorflow:loss = 0.282538, step = 6945 (0.565 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6960 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.279534.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6960\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:33\n",
      "INFO:tensorflow:Saving dict for global step 6960: accuracy = 0.369515, cross_entropy_loss = 3.27255, global_step = 6960, loss = 3.55079\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-6960\n",
      "INFO:tensorflow:Saving checkpoints for 6961 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282564, step = 6961\n",
      "INFO:tensorflow:global_step/sec: 174.454\n",
      "INFO:tensorflow:loss = 0.28229, step = 7061 (0.575 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7076 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.279345.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:35\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7076\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:36\n",
      "INFO:tensorflow:Saving dict for global step 7076: accuracy = 0.37067, cross_entropy_loss = 3.28009, global_step = 7076, loss = 3.55816\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7076\n",
      "INFO:tensorflow:Saving checkpoints for 7077 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282313, step = 7077\n",
      "INFO:tensorflow:global_step/sec: 183.757\n",
      "INFO:tensorflow:loss = 0.282046, step = 7177 (0.545 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7192 into /tmp/tmpoo8qf2hh/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.279156.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:38\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7192\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:39\n",
      "INFO:tensorflow:Saving dict for global step 7192: accuracy = 0.37067, cross_entropy_loss = 3.28744, global_step = 7192, loss = 3.56534\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7192\n",
      "INFO:tensorflow:Saving checkpoints for 7193 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282066, step = 7193\n",
      "INFO:tensorflow:global_step/sec: 171.482\n",
      "INFO:tensorflow:loss = 0.281805, step = 7293 (0.584 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7308 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278969.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:40\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7308\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:41\n",
      "INFO:tensorflow:Saving dict for global step 7308: accuracy = 0.37067, cross_entropy_loss = 3.29462, global_step = 7308, loss = 3.57235\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7308\n",
      "INFO:tensorflow:Saving checkpoints for 7309 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.281822, step = 7309\n",
      "INFO:tensorflow:global_step/sec: 181.26\n",
      "INFO:tensorflow:loss = 0.281567, step = 7409 (0.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7424 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278782.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:43\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7424\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:44\n",
      "INFO:tensorflow:Saving dict for global step 7424: accuracy = 0.37067, cross_entropy_loss = 3.30163, global_step = 7424, loss = 3.57919\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7424\n",
      "INFO:tensorflow:Saving checkpoints for 7425 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.332809, step = 7425\n",
      "INFO:tensorflow:global_step/sec: 182.915\n",
      "INFO:tensorflow:loss = 0.281309, step = 7525 (0.548 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7540 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278601.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7540\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:46\n",
      "INFO:tensorflow:Saving dict for global step 7540: accuracy = 0.37067, cross_entropy_loss = 3.30748, global_step = 7540, loss = 3.58488\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7540\n",
      "INFO:tensorflow:Saving checkpoints for 7541 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.281351, step = 7541\n",
      "INFO:tensorflow:global_step/sec: 180.308\n",
      "INFO:tensorflow:loss = 0.281101, step = 7641 (0.556 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7656 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278409.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:48\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7656\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:48\n",
      "INFO:tensorflow:Saving dict for global step 7656: accuracy = 0.37067, cross_entropy_loss = 3.31509, global_step = 7656, loss = 3.59232\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7656\n",
      "INFO:tensorflow:Saving checkpoints for 7657 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282134, step = 7657\n",
      "INFO:tensorflow:global_step/sec: 181.427\n",
      "INFO:tensorflow:loss = 0.280852, step = 7757 (0.553 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7772 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278228.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7772\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:51\n",
      "INFO:tensorflow:Saving dict for global step 7772: accuracy = 0.37067, cross_entropy_loss = 3.32099, global_step = 7772, loss = 3.59805\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7772\n",
      "INFO:tensorflow:Saving checkpoints for 7773 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.280891, step = 7773\n",
      "INFO:tensorflow:global_step/sec: 168.51\n",
      "INFO:tensorflow:loss = 0.280645, step = 7873 (0.595 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7888 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278039.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7888\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:53\n",
      "INFO:tensorflow:Saving dict for global step 7888: accuracy = 0.37067, cross_entropy_loss = 3.32807, global_step = 7888, loss = 3.60496\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-7888\n",
      "INFO:tensorflow:Saving checkpoints for 7889 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.280665, step = 7889\n",
      "INFO:tensorflow:global_step/sec: 180.178\n",
      "INFO:tensorflow:loss = 0.280421, step = 7989 (0.556 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8004 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.277855.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8004\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:55\n",
      "INFO:tensorflow:Saving dict for global step 8004: accuracy = 0.37067, cross_entropy_loss = 3.33433, global_step = 8004, loss = 3.61105\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8004\n",
      "INFO:tensorflow:Saving checkpoints for 8005 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.280435, step = 8005\n",
      "INFO:tensorflow:global_step/sec: 184.867\n",
      "INFO:tensorflow:loss = 0.280198, step = 8105 (0.542 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8120 into /tmp/tmpoo8qf2hh/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.277673.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:08:58\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8120\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:08:58\n",
      "INFO:tensorflow:Saving dict for global step 8120: accuracy = 0.369515, cross_entropy_loss = 3.3404, global_step = 8120, loss = 3.61695\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "        model.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:09:04\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8120\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:09:04\n",
      "INFO:tensorflow:Saving dict for global step 8120: accuracy = 0.369515, cross_entropy_loss = 3.3404, global_step = 8120, loss = 3.61695\n",
      "Accuracy on test set: 36.95%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.369515,\n",
       " 'cross_entropy_loss': 3.3404,\n",
       " 'global_step': 8120,\n",
       " 'loss': 3.6169541}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer_conf(h_, labels_, num_classes, conf):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        #c = tf.constant(conf,dtype = tf.float32)\n",
    "        loss_ = tf.reduce_mean(tf.cast(conf,dtype = tf.float32) * tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn_conf(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer_conf(h_, labels, params['num_classes'],conf=features['conf'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "        \n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_conf_score = []\n",
    "for i in range(U_n_train.shape[0]):\n",
    "    true_conf_score.append(2)\n",
    "    \n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train},\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_train = model.predict(input_fn=train_input_fn)  \n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test},\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_test = model.predict(input_fn=test_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8120\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpoo8qf2hh/model.ckpt-8120\n"
     ]
    }
   ],
   "source": [
    "predictions_train =list(predictions_train)\n",
    "predictions_test =list(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u_train = []\n",
    "\n",
    "for i in range(len(predictions_train)):\n",
    "    conf_score_u_train.append(predictions_train[i]['max'])\n",
    "    \n",
    "conf_score_u_test = []\n",
    "\n",
    "for i in range(len(predictions_test)):\n",
    "    conf_score_u_test.append(predictions_test[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpbzxc23gc\n",
      "INFO:tensorflow:Using config: {'_session_config': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_save_checkpoints_steps': None, '_model_dir': '/tmp/tmpbzxc23gc', '_save_summary_steps': 100, '_tf_random_seed': 1, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=3,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "\n",
    "\n",
    "model1 = tf.estimator.Estimator(model_fn=classifier_model_fn_conf, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # training the model (weak supervision)\n",
    "num_epochs = 200\n",
    "\n",
    "train_params = dict(batch_size=250, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "y_u_train = np.reshape(y_u_train,newshape=(len(y_u_train),))\n",
    "conf_score_u_train = np.reshape(conf_score_u_train,newshape=(U_n_train.shape[0],))\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train,\"conf\": conf_score_u_train}, y=y_u_train,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "y_u_test = np.reshape(y_u_test,newshape=(len(y_u_test),))\n",
    "conf_score_u_test = np.reshape(conf_score_u_test,newshape=(U_n_test.shape[0],))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test,\"conf\": conf_score_u_test}, y=y_u_test,\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.81522, step = 1\n",
      "INFO:tensorflow:global_step/sec: 12.0036\n",
      "INFO:tensorflow:loss = 1.19107, step = 101 (8.332 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.0935\n",
      "INFO:tensorflow:loss = 1.07852, step = 201 (8.269 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.0726\n",
      "INFO:tensorflow:loss = 1.0551, step = 301 (8.283 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 363 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.75269.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:10:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-363\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:10:53\n",
      "INFO:tensorflow:Saving dict for global step 363: accuracy = 0.838394, cross_entropy_loss = 0.603133, global_step = 363, loss = 0.894049\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-363\n",
      "INFO:tensorflow:Saving checkpoints for 364 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.890354, step = 364\n",
      "INFO:tensorflow:global_step/sec: 12.786\n",
      "INFO:tensorflow:loss = 0.826324, step = 464 (7.823 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.6448\n",
      "INFO:tensorflow:loss = 0.755526, step = 564 (7.908 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.4654\n",
      "INFO:tensorflow:loss = 0.808435, step = 664 (8.022 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 726 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.581293.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:11:23\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-726\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:11:26\n",
      "INFO:tensorflow:Saving dict for global step 726: accuracy = 0.894881, cross_entropy_loss = 0.427197, global_step = 726, loss = 0.721828\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-726\n",
      "INFO:tensorflow:Saving checkpoints for 727 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.689462, step = 727\n",
      "INFO:tensorflow:global_step/sec: 13.3284\n",
      "INFO:tensorflow:loss = 0.654815, step = 827 (7.504 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4027\n",
      "INFO:tensorflow:loss = 0.615562, step = 927 (7.461 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2381\n",
      "INFO:tensorflow:loss = 0.693943, step = 1027 (7.554 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1089 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.503884.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:11:55\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1089\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:11:58\n",
      "INFO:tensorflow:Saving dict for global step 1089: accuracy = 0.912092, cross_entropy_loss = 0.354083, global_step = 1089, loss = 0.651024\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1089\n",
      "INFO:tensorflow:Saving checkpoints for 1090 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.604376, step = 1090\n",
      "INFO:tensorflow:global_step/sec: 13.5148\n",
      "INFO:tensorflow:loss = 0.56726, step = 1190 (7.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4464\n",
      "INFO:tensorflow:loss = 0.544458, step = 1290 (7.437 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5003\n",
      "INFO:tensorflow:loss = 0.630738, step = 1390 (7.407 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1452 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.460261.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:12:26\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1452\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:12:29\n",
      "INFO:tensorflow:Saving dict for global step 1452: accuracy = 0.920079, cross_entropy_loss = 0.317678, global_step = 1452, loss = 0.615884\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1452\n",
      "INFO:tensorflow:Saving checkpoints for 1453 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.555839, step = 1453\n",
      "INFO:tensorflow:global_step/sec: 13.7222\n",
      "INFO:tensorflow:loss = 0.514302, step = 1553 (7.289 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7857\n",
      "INFO:tensorflow:loss = 0.502134, step = 1653 (7.254 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7664\n",
      "INFO:tensorflow:loss = 0.590792, step = 1753 (7.264 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1815 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.433277.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:12:57\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1815\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:13:00\n",
      "INFO:tensorflow:Saving dict for global step 1815: accuracy = 0.924228, cross_entropy_loss = 0.297053, global_step = 1815, loss = 0.595898\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-1815\n",
      "INFO:tensorflow:Saving checkpoints for 1816 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.520223, step = 1816\n",
      "INFO:tensorflow:global_step/sec: 13.8119\n",
      "INFO:tensorflow:loss = 0.479225, step = 1916 (7.241 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5745\n",
      "INFO:tensorflow:loss = 0.472953, step = 2016 (7.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7343\n",
      "INFO:tensorflow:loss = 0.564038, step = 2116 (7.281 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2178 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.414513.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:13:28\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2178\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:13:30\n",
      "INFO:tensorflow:Saving dict for global step 2178: accuracy = 0.927802, cross_entropy_loss = 0.284768, global_step = 2178, loss = 0.583874\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2178\n",
      "INFO:tensorflow:Saving checkpoints for 2179 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.491072, step = 2179\n",
      "INFO:tensorflow:global_step/sec: 13.6482\n",
      "INFO:tensorflow:loss = 0.454326, step = 2279 (7.328 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6241\n",
      "INFO:tensorflow:loss = 0.450526, step = 2379 (7.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6723\n",
      "INFO:tensorflow:loss = 0.543564, step = 2479 (7.314 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2541 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.400246.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:13:59\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2541\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:14:02\n",
      "INFO:tensorflow:Saving dict for global step 2541: accuracy = 0.929347, cross_entropy_loss = 0.277407, global_step = 2541, loss = 0.576539\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2541\n",
      "INFO:tensorflow:Saving checkpoints for 2542 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.466266, step = 2542\n",
      "INFO:tensorflow:global_step/sec: 13.6826\n",
      "INFO:tensorflow:loss = 0.435795, step = 2642 (7.310 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6559\n",
      "INFO:tensorflow:loss = 0.432008, step = 2742 (7.323 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7246\n",
      "INFO:tensorflow:loss = 0.526298, step = 2842 (7.286 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2904 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.38868.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:14:29\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2904\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:14:32\n",
      "INFO:tensorflow:Saving dict for global step 2904: accuracy = 0.930891, cross_entropy_loss = 0.273189, global_step = 2904, loss = 0.572197\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-2904\n",
      "INFO:tensorflow:Saving checkpoints for 2905 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.445038, step = 2905\n",
      "INFO:tensorflow:global_step/sec: 13.7296\n",
      "INFO:tensorflow:loss = 0.421157, step = 3005 (7.284 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.416034, step = 3105 (7.294 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7192\n",
      "INFO:tensorflow:loss = 0.510902, step = 3205 (7.289 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3267 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.378858.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:15:00\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3267\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:15:03\n",
      "INFO:tensorflow:Saving dict for global step 3267: accuracy = 0.931421, cross_entropy_loss = 0.271095, global_step = 3267, loss = 0.56988\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3267\n",
      "INFO:tensorflow:Saving checkpoints for 3268 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.427233, step = 3268\n",
      "INFO:tensorflow:global_step/sec: 13.8902\n",
      "INFO:tensorflow:loss = 0.409025, step = 3368 (7.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7541\n",
      "INFO:tensorflow:loss = 0.401843, step = 3468 (7.270 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7293\n",
      "INFO:tensorflow:loss = 0.496761, step = 3568 (7.284 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3630 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.370068.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:15:31\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3630\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:15:34\n",
      "INFO:tensorflow:Saving dict for global step 3630: accuracy = 0.931818, cross_entropy_loss = 0.270752, global_step = 3630, loss = 0.569249\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3630\n",
      "INFO:tensorflow:Saving checkpoints for 3631 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.41188, step = 3631\n",
      "INFO:tensorflow:global_step/sec: 13.8897\n",
      "INFO:tensorflow:loss = 0.398285, step = 3731 (7.201 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9189\n",
      "INFO:tensorflow:loss = 0.389001, step = 3831 (7.185 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0454\n",
      "INFO:tensorflow:loss = 0.483473, step = 3931 (7.120 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3993 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.362088.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:16:01\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3993\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:16:04\n",
      "INFO:tensorflow:Saving dict for global step 3993: accuracy = 0.932083, cross_entropy_loss = 0.271357, global_step = 3993, loss = 0.569525\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-3993\n",
      "INFO:tensorflow:Saving checkpoints for 3994 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.39827, step = 3994\n",
      "INFO:tensorflow:global_step/sec: 13.7501\n",
      "INFO:tensorflow:loss = 0.388803, step = 4094 (7.274 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8494\n",
      "INFO:tensorflow:loss = 0.377518, step = 4194 (7.220 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9559\n",
      "INFO:tensorflow:loss = 0.470927, step = 4294 (7.166 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4356 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.354862.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:16:32\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-4356\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:16:34\n",
      "INFO:tensorflow:Saving dict for global step 4356: accuracy = 0.932083, cross_entropy_loss = 0.27253, global_step = 4356, loss = 0.570343\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-4356\n",
      "INFO:tensorflow:Saving checkpoints for 4357 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.386663, step = 4357\n",
      "INFO:tensorflow:global_step/sec: 13.5203\n",
      "INFO:tensorflow:loss = 0.380496, step = 4457 (7.398 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7442\n",
      "INFO:tensorflow:loss = 0.367488, step = 4557 (7.276 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8191\n",
      "INFO:tensorflow:loss = 0.458666, step = 4657 (7.237 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4719 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.348078.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:17:02\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-4719\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:17:05\n",
      "INFO:tensorflow:Saving dict for global step 4719: accuracy = 0.931995, cross_entropy_loss = 0.274764, global_step = 4719, loss = 0.572202\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-4719\n",
      "INFO:tensorflow:Saving checkpoints for 4720 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.377416, step = 4720\n",
      "INFO:tensorflow:global_step/sec: 13.9039\n",
      "INFO:tensorflow:loss = 0.373186, step = 4820 (7.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9363\n",
      "INFO:tensorflow:loss = 0.358815, step = 4920 (7.175 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9056\n",
      "INFO:tensorflow:loss = 0.446751, step = 5020 (7.191 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5082 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.341629.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:17:33\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5082\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:17:36\n",
      "INFO:tensorflow:Saving dict for global step 5082: accuracy = 0.932083, cross_entropy_loss = 0.277163, global_step = 5082, loss = 0.574216\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5082\n",
      "INFO:tensorflow:Saving checkpoints for 5083 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.36931, step = 5083\n",
      "INFO:tensorflow:global_step/sec: 13.8004\n",
      "INFO:tensorflow:loss = 0.366584, step = 5183 (7.247 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8681\n",
      "INFO:tensorflow:loss = 0.351381, step = 5283 (7.211 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0016\n",
      "INFO:tensorflow:loss = 0.43524, step = 5383 (7.142 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5445 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.335691.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:18:03\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5445\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:18:06\n",
      "INFO:tensorflow:Saving dict for global step 5445: accuracy = 0.931244, cross_entropy_loss = 0.279956, global_step = 5445, loss = 0.576616\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5445\n",
      "INFO:tensorflow:Saving checkpoints for 5446 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.362247, step = 5446\n",
      "INFO:tensorflow:global_step/sec: 13.8304\n",
      "INFO:tensorflow:loss = 0.360722, step = 5546 (7.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8281\n",
      "INFO:tensorflow:loss = 0.34508, step = 5646 (7.232 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8925\n",
      "INFO:tensorflow:loss = 0.42401, step = 5746 (7.198 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5808 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.330205.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:18:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5808\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:18:37\n",
      "INFO:tensorflow:Saving dict for global step 5808: accuracy = 0.931774, cross_entropy_loss = 0.283058, global_step = 5808, loss = 0.579317\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-5808\n",
      "INFO:tensorflow:Saving checkpoints for 5809 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.355928, step = 5809\n",
      "INFO:tensorflow:global_step/sec: 13.7321\n",
      "INFO:tensorflow:loss = 0.355449, step = 5909 (7.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9106\n",
      "INFO:tensorflow:loss = 0.339678, step = 6009 (7.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8308\n",
      "INFO:tensorflow:loss = 0.413238, step = 6109 (7.230 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6171 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.32523.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:19:04\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6171\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:19:07\n",
      "INFO:tensorflow:Saving dict for global step 6171: accuracy = 0.931377, cross_entropy_loss = 0.28631, global_step = 6171, loss = 0.582163\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6171\n",
      "INFO:tensorflow:Saving checkpoints for 6172 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.35006, step = 6172\n",
      "INFO:tensorflow:global_step/sec: 13.8776\n",
      "INFO:tensorflow:loss = 0.350653, step = 6272 (7.207 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9248\n",
      "INFO:tensorflow:loss = 0.334948, step = 6372 (7.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9853\n",
      "INFO:tensorflow:loss = 0.40306, step = 6472 (7.151 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6534 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.320755.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:19:35\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6534\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:19:38\n",
      "INFO:tensorflow:Saving dict for global step 6534: accuracy = 0.931289, cross_entropy_loss = 0.28977, global_step = 6534, loss = 0.585212\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6534\n",
      "INFO:tensorflow:Saving checkpoints for 6535 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.344584, step = 6535\n",
      "INFO:tensorflow:global_step/sec: 13.7522\n",
      "INFO:tensorflow:loss = 0.346283, step = 6635 (7.273 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7358\n",
      "INFO:tensorflow:loss = 0.33075, step = 6735 (7.280 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6904\n",
      "INFO:tensorflow:loss = 0.393629, step = 6835 (7.305 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6897 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.316761.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:20:06\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6897\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:20:08\n",
      "INFO:tensorflow:Saving dict for global step 6897: accuracy = 0.931244, cross_entropy_loss = 0.293385, global_step = 6897, loss = 0.588413\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-6897\n",
      "INFO:tensorflow:Saving checkpoints for 6898 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.339457, step = 6898\n",
      "INFO:tensorflow:global_step/sec: 13.8827\n",
      "INFO:tensorflow:loss = 0.342267, step = 6998 (7.204 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.898\n",
      "INFO:tensorflow:loss = 0.326965, step = 7098 (7.195 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.93\n",
      "INFO:tensorflow:loss = 0.385029, step = 7198 (7.179 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7260 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.313205.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:20:36\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7260\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:20:39\n",
      "INFO:tensorflow:Saving dict for global step 7260: accuracy = 0.930715, cross_entropy_loss = 0.297137, global_step = 7260, loss = 0.591747\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7260\n",
      "INFO:tensorflow:Saving checkpoints for 7261 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33466, step = 7261\n",
      "INFO:tensorflow:global_step/sec: 13.6802\n",
      "INFO:tensorflow:loss = 0.338552, step = 7361 (7.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6991\n",
      "INFO:tensorflow:loss = 0.323517, step = 7461 (7.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6948\n",
      "INFO:tensorflow:loss = 0.377301, step = 7561 (7.302 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7623 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.310039.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:21:07\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7623\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:21:10\n",
      "INFO:tensorflow:Saving dict for global step 7623: accuracy = 0.930759, cross_entropy_loss = 0.301003, global_step = 7623, loss = 0.595193\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7623\n",
      "INFO:tensorflow:Saving checkpoints for 7624 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.330193, step = 7624\n",
      "INFO:tensorflow:global_step/sec: 13.8631\n",
      "INFO:tensorflow:loss = 0.335099, step = 7724 (7.215 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9785\n",
      "INFO:tensorflow:loss = 0.320356, step = 7824 (7.153 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7231\n",
      "INFO:tensorflow:loss = 0.370416, step = 7924 (7.287 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7986 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.30721.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:21:38\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7986\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:21:41\n",
      "INFO:tensorflow:Saving dict for global step 7986: accuracy = 0.930671, cross_entropy_loss = 0.304968, global_step = 7986, loss = 0.598737\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-7986\n",
      "INFO:tensorflow:Saving checkpoints for 7987 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.326075, step = 7987\n",
      "INFO:tensorflow:global_step/sec: 13.5698\n",
      "INFO:tensorflow:loss = 0.331879, step = 8087 (7.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5818\n",
      "INFO:tensorflow:loss = 0.317444, step = 8187 (7.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.603\n",
      "INFO:tensorflow:loss = 0.364304, step = 8287 (7.351 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8349 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.304668.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:22:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-8349\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:22:11\n",
      "INFO:tensorflow:Saving dict for global step 8349: accuracy = 0.930362, cross_entropy_loss = 0.309009, global_step = 8349, loss = 0.602357\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-8349\n",
      "INFO:tensorflow:Saving checkpoints for 8350 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.32232, step = 8350\n",
      "INFO:tensorflow:global_step/sec: 13.8707\n",
      "INFO:tensorflow:loss = 0.32887, step = 8450 (7.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8936\n",
      "INFO:tensorflow:loss = 0.314753, step = 8550 (7.198 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.931\n",
      "INFO:tensorflow:loss = 0.358886, step = 8650 (7.178 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8712 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.302371.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:22:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-8712\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:22:42\n",
      "INFO:tensorflow:Saving dict for global step 8712: accuracy = 0.930229, cross_entropy_loss = 0.313098, global_step = 8712, loss = 0.606023\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-8712\n",
      "INFO:tensorflow:Saving checkpoints for 8713 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.31892, step = 8713\n",
      "INFO:tensorflow:global_step/sec: 14.1172\n",
      "INFO:tensorflow:loss = 0.326051, step = 8813 (7.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1137\n",
      "INFO:tensorflow:loss = 0.312257, step = 8913 (7.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1346\n",
      "INFO:tensorflow:loss = 0.354093, step = 9013 (7.075 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9075 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.300286.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:23:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9075\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:23:12\n",
      "INFO:tensorflow:Saving dict for global step 9075: accuracy = 0.929965, cross_entropy_loss = 0.317209, global_step = 9075, loss = 0.609712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9075\n",
      "INFO:tensorflow:Saving checkpoints for 9076 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.315851, step = 9076\n",
      "INFO:tensorflow:global_step/sec: 14.1815\n",
      "INFO:tensorflow:loss = 0.323405, step = 9176 (7.053 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2238\n",
      "INFO:tensorflow:loss = 0.309933, step = 9276 (7.030 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2255\n",
      "INFO:tensorflow:loss = 0.349864, step = 9376 (7.030 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9438 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.298381.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:23:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9438\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:23:42\n",
      "INFO:tensorflow:Saving dict for global step 9438: accuracy = 0.929744, cross_entropy_loss = 0.321331, global_step = 9438, loss = 0.613413\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9438\n",
      "INFO:tensorflow:Saving checkpoints for 9439 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.313081, step = 9439\n",
      "INFO:tensorflow:global_step/sec: 14.0206\n",
      "INFO:tensorflow:loss = 0.320918, step = 9539 (7.133 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.0487\n",
      "INFO:tensorflow:loss = 0.307764, step = 9639 (7.118 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.345\n",
      "INFO:tensorflow:loss = 0.346141, step = 9739 (6.971 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9801 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.296625.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:24:09\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9801\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:24:12\n",
      "INFO:tensorflow:Saving dict for global step 9801: accuracy = 0.929921, cross_entropy_loss = 0.325471, global_step = 9801, loss = 0.617132\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-9801\n",
      "INFO:tensorflow:Saving checkpoints for 9802 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.310581, step = 9802\n",
      "INFO:tensorflow:global_step/sec: 14.332\n",
      "INFO:tensorflow:loss = 0.318576, step = 9902 (6.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.3788\n",
      "INFO:tensorflow:loss = 0.305737, step = 10002 (6.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.4302\n",
      "INFO:tensorflow:loss = 0.342856, step = 10102 (6.930 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10164 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.294996.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:24:39\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10164\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:24:42\n",
      "INFO:tensorflow:Saving dict for global step 10164: accuracy = 0.929347, cross_entropy_loss = 0.32963, global_step = 10164, loss = 0.62087\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10164\n",
      "INFO:tensorflow:Saving checkpoints for 10165 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.308318, step = 10165\n",
      "INFO:tensorflow:global_step/sec: 14.1955\n",
      "INFO:tensorflow:loss = 0.316366, step = 10265 (7.045 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1163\n",
      "INFO:tensorflow:loss = 0.303843, step = 10365 (7.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.2649\n",
      "INFO:tensorflow:loss = 0.339944, step = 10465 (7.010 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10527 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.293477.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:25:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10527\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:25:11\n",
      "INFO:tensorflow:Saving dict for global step 10527: accuracy = 0.929303, cross_entropy_loss = 0.3338, global_step = 10527, loss = 0.624621\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10527\n",
      "INFO:tensorflow:Saving checkpoints for 10528 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.306254, step = 10528\n",
      "INFO:tensorflow:global_step/sec: 13.591\n",
      "INFO:tensorflow:loss = 0.314274, step = 10628 (7.359 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5571\n",
      "INFO:tensorflow:loss = 0.302077, step = 10728 (7.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0701\n",
      "INFO:tensorflow:loss = 0.337353, step = 10828 (7.651 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10890 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.29206.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:25:40\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10890\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:25:44\n",
      "INFO:tensorflow:Saving dict for global step 10890: accuracy = 0.92917, cross_entropy_loss = 0.337971, global_step = 10890, loss = 0.628374\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-10890\n",
      "INFO:tensorflow:Saving checkpoints for 10891 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30436, step = 10891\n",
      "INFO:tensorflow:global_step/sec: 12.5152\n",
      "INFO:tensorflow:loss = 0.312295, step = 10991 (7.991 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.725\n",
      "INFO:tensorflow:loss = 0.300433, step = 11091 (7.858 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.7145\n",
      "INFO:tensorflow:loss = 0.335038, step = 11191 (7.865 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11253 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.290736.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:26:14\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11253\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:26:17\n",
      "INFO:tensorflow:Saving dict for global step 11253: accuracy = 0.928906, cross_entropy_loss = 0.342135, global_step = 11253, loss = 0.632121\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11253\n",
      "INFO:tensorflow:Saving checkpoints for 11254 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.302609, step = 11254\n",
      "INFO:tensorflow:global_step/sec: 13.4831\n",
      "INFO:tensorflow:loss = 0.310427, step = 11354 (7.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4388\n",
      "INFO:tensorflow:loss = 0.298901, step = 11454 (7.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4759\n",
      "INFO:tensorflow:loss = 0.332952, step = 11554 (7.421 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11616 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.289497.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:26:45\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11616\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:26:48\n",
      "INFO:tensorflow:Saving dict for global step 11616: accuracy = 0.928332, cross_entropy_loss = 0.346275, global_step = 11616, loss = 0.635844\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11616\n",
      "INFO:tensorflow:Saving checkpoints for 11617 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.300984, step = 11617\n",
      "INFO:tensorflow:global_step/sec: 13.7098\n",
      "INFO:tensorflow:loss = 0.30866, step = 11717 (7.295 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7294\n",
      "INFO:tensorflow:loss = 0.29747, step = 11817 (7.283 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6955\n",
      "INFO:tensorflow:loss = 0.33106, step = 11917 (7.302 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11979 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.288338.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:27:16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11979\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:27:19\n",
      "INFO:tensorflow:Saving dict for global step 11979: accuracy = 0.928111, cross_entropy_loss = 0.350383, global_step = 11979, loss = 0.639537\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-11979\n",
      "INFO:tensorflow:Saving checkpoints for 11980 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.299464, step = 11980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 13.7848\n",
      "INFO:tensorflow:loss = 0.306988, step = 12080 (7.256 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8434\n",
      "INFO:tensorflow:loss = 0.296131, step = 12180 (7.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8327\n",
      "INFO:tensorflow:loss = 0.329329, step = 12280 (7.229 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12342 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.287248.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:27:46\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-12342\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:27:49\n",
      "INFO:tensorflow:Saving dict for global step 12342: accuracy = 0.928067, cross_entropy_loss = 0.354458, global_step = 12342, loss = 0.643197\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-12342\n",
      "INFO:tensorflow:Saving checkpoints for 12343 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.298034, step = 12343\n",
      "INFO:tensorflow:global_step/sec: 14.0835\n",
      "INFO:tensorflow:loss = 0.305403, step = 12443 (7.101 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.9978\n",
      "INFO:tensorflow:loss = 0.294875, step = 12543 (7.144 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7464\n",
      "INFO:tensorflow:loss = 0.327728, step = 12643 (7.275 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12705 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.286219.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:28:17\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-12705\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:28:20\n",
      "INFO:tensorflow:Saving dict for global step 12705: accuracy = 0.928155, cross_entropy_loss = 0.358495, global_step = 12705, loss = 0.646819\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-12705\n",
      "INFO:tensorflow:Saving checkpoints for 12706 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.296679, step = 12706\n",
      "INFO:tensorflow:global_step/sec: 13.9535\n",
      "INFO:tensorflow:loss = 0.303901, step = 12806 (7.168 sec)\n",
      "INFO:tensorflow:global_step/sec: 14.1304\n",
      "INFO:tensorflow:loss = 0.293691, step = 12906 (7.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.7975\n",
      "INFO:tensorflow:loss = 0.326231, step = 13006 (7.248 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13068 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.285245.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:28:47\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13068\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:28:50\n",
      "INFO:tensorflow:Saving dict for global step 13068: accuracy = 0.928023, cross_entropy_loss = 0.362489, global_step = 13068, loss = 0.650399\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13068\n",
      "INFO:tensorflow:Saving checkpoints for 13069 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.295383, step = 13069\n",
      "INFO:tensorflow:global_step/sec: 13.3651\n",
      "INFO:tensorflow:loss = 0.302476, step = 13169 (7.484 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3942\n",
      "INFO:tensorflow:loss = 0.29257, step = 13269 (7.465 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1625\n",
      "INFO:tensorflow:loss = 0.324813, step = 13369 (7.597 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13431 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.284318.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:29:19\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13431\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:29:22\n",
      "INFO:tensorflow:Saving dict for global step 13431: accuracy = 0.927979, cross_entropy_loss = 0.366436, global_step = 13431, loss = 0.653932\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13431\n",
      "INFO:tensorflow:Saving checkpoints for 13432 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.294133, step = 13432\n",
      "INFO:tensorflow:global_step/sec: 13.7062\n",
      "INFO:tensorflow:loss = 0.301119, step = 13532 (7.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8321\n",
      "INFO:tensorflow:loss = 0.291504, step = 13632 (7.229 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6828\n",
      "INFO:tensorflow:loss = 0.323454, step = 13732 (7.309 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13794 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.283435.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:29:50\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13794\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:29:53\n",
      "INFO:tensorflow:Saving dict for global step 13794: accuracy = 0.927758, cross_entropy_loss = 0.370333, global_step = 13794, loss = 0.657414\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-13794\n",
      "INFO:tensorflow:Saving checkpoints for 13795 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.292923, step = 13795\n",
      "INFO:tensorflow:global_step/sec: 13.5505\n",
      "INFO:tensorflow:loss = 0.299815, step = 13895 (7.381 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.8161\n",
      "INFO:tensorflow:loss = 0.290487, step = 13995 (7.238 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6233\n",
      "INFO:tensorflow:loss = 0.322142, step = 14095 (7.341 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14157 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.282589.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:30:21\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14157\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:30:24\n",
      "INFO:tensorflow:Saving dict for global step 14157: accuracy = 0.927626, cross_entropy_loss = 0.37418, global_step = 14157, loss = 0.660847\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14157\n",
      "INFO:tensorflow:Saving checkpoints for 14158 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.291759, step = 14158\n",
      "INFO:tensorflow:global_step/sec: 13.06\n",
      "INFO:tensorflow:loss = 0.298556, step = 14258 (7.658 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0868\n",
      "INFO:tensorflow:loss = 0.289512, step = 14358 (7.641 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1795\n",
      "INFO:tensorflow:loss = 0.320875, step = 14458 (7.588 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14520 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.281777.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:30:53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14520\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:30:56\n",
      "INFO:tensorflow:Saving dict for global step 14520: accuracy = 0.927626, cross_entropy_loss = 0.377977, global_step = 14520, loss = 0.664231\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14520\n",
      "INFO:tensorflow:Saving checkpoints for 14521 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.290643, step = 14521\n",
      "INFO:tensorflow:global_step/sec: 13.5371\n",
      "INFO:tensorflow:loss = 0.297335, step = 14621 (7.388 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4846\n",
      "INFO:tensorflow:loss = 0.288575, step = 14721 (7.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3379\n",
      "INFO:tensorflow:loss = 0.319648, step = 14821 (7.497 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 14883 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280995.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:31:24\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14883\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:31:27\n",
      "INFO:tensorflow:Saving dict for global step 14883: accuracy = 0.927449, cross_entropy_loss = 0.381726, global_step = 14883, loss = 0.667566\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-14883\n",
      "INFO:tensorflow:Saving checkpoints for 14884 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.289574, step = 14884\n",
      "INFO:tensorflow:global_step/sec: 12.6131\n",
      "INFO:tensorflow:loss = 0.296151, step = 14984 (7.929 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8277\n",
      "INFO:tensorflow:loss = 0.28767, step = 15084 (7.796 sec)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:global_step/sec: 12.7529\n",
      "INFO:tensorflow:loss = 0.31846, step = 15184 (7.841 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15246 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.280241.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:31:57\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15246\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:32:00\n",
      "INFO:tensorflow:Saving dict for global step 15246: accuracy = 0.927317, cross_entropy_loss = 0.385427, global_step = 15246, loss = 0.670854\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15246\n",
      "INFO:tensorflow:Saving checkpoints for 15247 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.288547, step = 15247\n",
      "INFO:tensorflow:global_step/sec: 12.7598\n",
      "INFO:tensorflow:loss = 0.295001, step = 15347 (7.838 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.8464\n",
      "INFO:tensorflow:loss = 0.286796, step = 15447 (7.784 sec)\n",
      "INFO:tensorflow:global_step/sec: 12.9248\n",
      "INFO:tensorflow:loss = 0.317306, step = 15547 (7.737 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15609 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.27951.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:32:30\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15609\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:32:33\n",
      "INFO:tensorflow:Saving dict for global step 15609: accuracy = 0.927273, cross_entropy_loss = 0.38908, global_step = 15609, loss = 0.674093\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15609\n",
      "INFO:tensorflow:Saving checkpoints for 15610 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.287559, step = 15610\n",
      "INFO:tensorflow:global_step/sec: 13.3802\n",
      "INFO:tensorflow:loss = 0.293883, step = 15710 (7.475 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.0072\n",
      "INFO:tensorflow:loss = 0.28595, step = 15810 (7.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3496\n",
      "INFO:tensorflow:loss = 0.316183, step = 15910 (7.490 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 15972 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278802.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:33:02\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15972\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:33:05\n",
      "INFO:tensorflow:Saving dict for global step 15972: accuracy = 0.927052, cross_entropy_loss = 0.392684, global_step = 15972, loss = 0.677284\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-15972\n",
      "INFO:tensorflow:Saving checkpoints for 15973 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.286605, step = 15973\n",
      "INFO:tensorflow:global_step/sec: 13.3906\n",
      "INFO:tensorflow:loss = 0.292794, step = 16073 (7.469 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2916\n",
      "INFO:tensorflow:loss = 0.285128, step = 16173 (7.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3934\n",
      "INFO:tensorflow:loss = 0.315086, step = 16273 (7.466 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16335 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.278111.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:33:34\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-16335\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:33:36\n",
      "INFO:tensorflow:Saving dict for global step 16335: accuracy = 0.92692, cross_entropy_loss = 0.396236, global_step = 16335, loss = 0.680424\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-16335\n",
      "INFO:tensorflow:Saving checkpoints for 16336 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.285678, step = 16336\n",
      "INFO:tensorflow:global_step/sec: 13.6958\n",
      "INFO:tensorflow:loss = 0.291729, step = 16436 (7.303 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.4411\n",
      "INFO:tensorflow:loss = 0.284328, step = 16536 (7.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.3762\n",
      "INFO:tensorflow:loss = 0.314012, step = 16636 (7.476 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 16698 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.277434.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:34:04\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-16698\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:34:07\n",
      "INFO:tensorflow:Saving dict for global step 16698: accuracy = 0.926699, cross_entropy_loss = 0.39973, global_step = 16698, loss = 0.683506\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-16698\n",
      "INFO:tensorflow:Saving checkpoints for 16699 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.284769, step = 16699\n",
      "INFO:tensorflow:global_step/sec: 13.1063\n",
      "INFO:tensorflow:loss = 0.290686, step = 16799 (7.631 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2123\n",
      "INFO:tensorflow:loss = 0.283545, step = 16899 (7.568 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.2636\n",
      "INFO:tensorflow:loss = 0.312963, step = 16999 (7.540 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17061 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.276766.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:34:37\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17061\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:34:40\n",
      "INFO:tensorflow:Saving dict for global step 17061: accuracy = 0.926522, cross_entropy_loss = 0.403158, global_step = 17061, loss = 0.686523\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17061\n",
      "INFO:tensorflow:Saving checkpoints for 17062 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.283872, step = 17062\n",
      "INFO:tensorflow:global_step/sec: 13.1589\n",
      "INFO:tensorflow:loss = 0.289664, step = 17162 (7.601 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1743\n",
      "INFO:tensorflow:loss = 0.282777, step = 17262 (7.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.1924\n",
      "INFO:tensorflow:loss = 0.311938, step = 17362 (7.580 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17424 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.276109.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:35:08\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17424\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:35:11\n",
      "INFO:tensorflow:Saving dict for global step 17424: accuracy = 0.926567, cross_entropy_loss = 0.406522, global_step = 17424, loss = 0.689478\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17424\n",
      "INFO:tensorflow:Saving checkpoints for 17425 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282998, step = 17425\n",
      "INFO:tensorflow:global_step/sec: 13.5099\n",
      "INFO:tensorflow:loss = 0.288673, step = 17525 (7.403 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.5263\n",
      "INFO:tensorflow:loss = 0.282025, step = 17625 (7.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.722\n",
      "INFO:tensorflow:loss = 0.310928, step = 17725 (7.288 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 17787 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.275468.\n",
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:35:40\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17787\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:35:42\n",
      "INFO:tensorflow:Saving dict for global step 17787: accuracy = 0.926478, cross_entropy_loss = 0.409828, global_step = 17787, loss = 0.692374\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-17787\n",
      "INFO:tensorflow:Saving checkpoints for 17788 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.282155, step = 17788\n",
      "INFO:tensorflow:global_step/sec: 13.5731\n",
      "INFO:tensorflow:loss = 0.287717, step = 17888 (7.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6335\n",
      "INFO:tensorflow:loss = 0.281293, step = 17988 (7.335 sec)\n",
      "INFO:tensorflow:global_step/sec: 13.6649\n",
      "INFO:tensorflow:loss = 0.309924, step = 18088 (7.318 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 18150 into /tmp/tmpbzxc23gc/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.27484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:36:10\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-18150\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:36:13\n",
      "INFO:tensorflow:Saving dict for global step 18150: accuracy = 0.926522, cross_entropy_loss = 0.413076, global_step = 18150, loss = 0.695214\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/device:GPU:0'):\n",
    "    for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "        model1.train(input_fn=train_input_fn)\n",
    "        eval_metrics = model1.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u = list(conf_score_u_train) + list(conf_score_u_test)\n",
    "conf_score_u = np.reshape(conf_score_u,newshape = (len(conf_score_u),))\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_sent, \"ns\":u_ns , \"conf\": conf_score_u},\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions = model1.predict(input_fn=predict_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-18150\n"
     ]
    }
   ],
   "source": [
    "predictions = list(predictions)\n",
    "pred_label_U = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    pred_label_U.append(predictions[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-04-16-20:40:12\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpbzxc23gc/model.ckpt-18150\n",
      "INFO:tensorflow:Finished evaluation at 2018-04-16-20:40:15\n",
      "INFO:tensorflow:Saving dict for global step 18150: accuracy = 0.926522, cross_entropy_loss = 0.413076, global_step = 18150, loss = 0.695214\n",
      "Accuracy on test set: 92.65%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.92652249,\n",
       " 'cross_entropy_loss': 0.41307649,\n",
       " 'global_step': 18150,\n",
       " 'loss': 0.69521439}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model1.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U = pd.DataFrame(data = pred_label_U, columns = ['pred_labels'])\n",
    "pred_label_U.to_csv(\"predictions_12April.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U = pd.read_csv(\"predictions_12April.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0     113299\n",
       "pred_labels    113299\n",
       "dtype: int64"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         1\n",
       "pred_labels    27771\n",
       "dtype: int64"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 2].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         1\n",
       "pred_labels    19037\n",
       "dtype: int64"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 1].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Unnamed: 0         1\n",
       "pred_labels    66491\n",
       "dtype: int64"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label_U[pred_label_U == 0].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_0 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 0:\n",
    "        pred_label_U_0.append(1)\n",
    "    else:\n",
    "        pred_label_U_0.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_1 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 1:\n",
    "        pred_label_U_1.append(1)\n",
    "    else:\n",
    "        pred_label_U_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U_2 = []\n",
    "\n",
    "for i in range(len(pred_label_U)):\n",
    "    if pred_label_U.loc[i]['pred_labels'] == 2:\n",
    "        pred_label_U_2.append(1)\n",
    "    else:\n",
    "        pred_label_U_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_0'] = pred_label_U_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_1'] = pred_label_U_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U['label_2'] = pred_label_U_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = U_sent.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "\n",
    "input_x = tf.placeholder(tf.float32,shape=[None,feature_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_y = tf.placeholder(tf.float32,shape=[None,num_classes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "hold_prob = tf.placeholder(tf.float32, name=\"hold_prob\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model CNN and evluating it\n",
    "# Now we define our model\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(name = 'W_embed', shape=[V,embed_dim],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_)\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        \n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation,    \n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                             , bias_initializer=tf.zeros_initializer(), name = 'Hidden_%d'%i)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.nn.dropout(h_, keep_prob=dropout_rate)\n",
    "            \n",
    "        #if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(inputs = h_, rate=dropout_rate, training = is_training)\n",
    "        \n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A CNN for text classification.\n",
    "    Uses an embedding layer, followed by a convolutional, max-pooling and softmax layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, sequence_length, num_classes, vocab_size,\n",
    "        embedding_size, filter_heights, filter_widths,num_filters,channels_in,channels_out, init_scale,l2_reg_lambda):\n",
    "\n",
    "        assert(len(filter_widths)==num_filters)\n",
    "        assert(len(filter_heights)==num_filters)\n",
    "        assert(len(channels_in)==num_filters)\n",
    "        assert(len(channels_out)==num_filters)\n",
    "        assert(channels_in[0]==1)\n",
    "        \n",
    "        # Placeholders for input, output and dropout\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_x\")\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name=\"input_y\")\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name=\"dropout_keep_prob\")\n",
    "        \n",
    "        # Keeping track of l2 regularization loss (optional)\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        # Embedding layer\n",
    "        with tf.name_scope(\"embedding_CNN\"):\n",
    "            W_embedding = tf.get_variable(name = 'W_embedding', shape=[vocab_size,embedding_size],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "            self.xs = tf.nn.embedding_lookup(W_embedding, self.input_x)\n",
    "            self.xs_expanded = tf.reshape(self.xs,[-1,sequence_length,embedding_size,1])\n",
    "            \n",
    "        # Create a convolution + maxpool layer for each filter size\n",
    "        pooled_outputs = []\n",
    "        i = 1\n",
    "        x = self.xs_expanded\n",
    "        for filter_h, filter_w, c_in, c_out in zip(filter_heights,filter_widths,channels_in,channels_out):\n",
    "            with tf.name_scope(\"conv-maxpool-%s\" % i):\n",
    "                # Convolution Layer\n",
    "                filter_shape = [filter_h, filter_w, c_in , c_out]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[c_out]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(x,W,strides=[1,1,1,1],padding=\"SAME\",name=\"conv\")\n",
    "                # Apply nonlinearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name=\"relu\")\n",
    "                # Maxpooling over the outputs\n",
    "                pooled = tf.nn.max_pool(h,ksize=[1,embedding_size-filter_w+1, 1, 1],strides=[1, 1, 1, 1],padding=\"SAME\",name=\"pool\")\n",
    "                x = pooled\n",
    "                i = i + 1\n",
    "                pooled_outputs.append(pooled)\n",
    "        \n",
    "        length = len(pooled_outputs)\n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = c_out*sequence_length*embedding_size\n",
    "        self.h_pool_flat = tf.reshape(pooled_outputs[length-1], [-1, num_filters_total])\n",
    "        \n",
    "        # Add dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        # Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope(\"output\"):\n",
    "            W = tf.get_variable(\n",
    "                \"W\",\n",
    "                shape=[num_filters_total, num_classes],\n",
    "                initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        # Calculate mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(logits=self.scores, labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "\n",
    "        # Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions, tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions, \"float\"), name=\"accuracy\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final padded sentences\n",
    "X_sent_test, ns = utils.pad_np_array(X_ids, max_len=max_len2, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label = pd.DataFrame(data = Y_label, columns = ['labels'])\n",
    "label_0 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 0:\n",
    "        label_0.append(1)\n",
    "    else:\n",
    "        label_0.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_1 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 1:\n",
    "        label_1.append(1)\n",
    "    else:\n",
    "        label_1.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_2 = []\n",
    "\n",
    "for i in range(len(Y_label)):\n",
    "    if Y_label.loc[i]['labels'] == 2:\n",
    "        label_2.append(1)\n",
    "    else:\n",
    "        label_2.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_label['label_0'] = label_0\n",
    "Y_label['label_1'] = label_1\n",
    "Y_label['label_2'] = label_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label_0</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>570</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>571</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>582</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>585</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>587</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>591</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>593</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>595</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>597</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>599</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>600 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     label_0  label_1  label_2\n",
       "0          1        0        0\n",
       "1          1        0        0\n",
       "2          1        0        0\n",
       "3          1        0        0\n",
       "4          1        0        0\n",
       "5          1        0        0\n",
       "6          1        0        0\n",
       "7          1        0        0\n",
       "8          1        0        0\n",
       "9          1        0        0\n",
       "10         1        0        0\n",
       "11         1        0        0\n",
       "12         1        0        0\n",
       "13         1        0        0\n",
       "14         1        0        0\n",
       "15         1        0        0\n",
       "16         1        0        0\n",
       "17         1        0        0\n",
       "18         1        0        0\n",
       "19         1        0        0\n",
       "20         1        0        0\n",
       "21         1        0        0\n",
       "22         1        0        0\n",
       "23         1        0        0\n",
       "24         1        0        0\n",
       "25         1        0        0\n",
       "26         1        0        0\n",
       "27         1        0        0\n",
       "28         1        0        0\n",
       "29         1        0        0\n",
       "..       ...      ...      ...\n",
       "570        1        0        0\n",
       "571        1        0        0\n",
       "572        1        0        0\n",
       "573        1        0        0\n",
       "574        1        0        0\n",
       "575        1        0        0\n",
       "576        1        0        0\n",
       "577        1        0        0\n",
       "578        1        0        0\n",
       "579        1        0        0\n",
       "580        1        0        0\n",
       "581        1        0        0\n",
       "582        1        0        0\n",
       "583        1        0        0\n",
       "584        1        0        0\n",
       "585        1        0        0\n",
       "586        1        0        0\n",
       "587        1        0        0\n",
       "588        1        0        0\n",
       "589        1        0        0\n",
       "590        1        0        0\n",
       "591        1        0        0\n",
       "592        1        0        0\n",
       "593        1        0        0\n",
       "594        1        0        0\n",
       "595        1        0        0\n",
       "596        1        0        0\n",
       "597        1        0        0\n",
       "598        1        0        0\n",
       "599        1        0        0\n",
       "\n",
       "[600 rows x 3 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_label.iloc[0:600][['label_0','label_1','label_2']]         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ramya_girish/political_bias/political_bias/runs/1523911375\n",
      "\n",
      "2018-04-16T20:43:02.009741: step 1, loss 1.39029, acc 0.4\n",
      "2018-04-16T20:43:02.516896: step 2, loss 11.3052, acc 0.6\n",
      "2018-04-16T20:43:03.018875: step 3, loss 7.95248, acc 0.57\n",
      "2018-04-16T20:43:03.521031: step 4, loss 2.76381, acc 0.29\n",
      "2018-04-16T20:43:04.023169: step 5, loss 3.49105, acc 0.3\n",
      "2018-04-16T20:43:04.523649: step 6, loss 5.41642, acc 0.22\n",
      "2018-04-16T20:43:05.024019: step 7, loss 2.98517, acc 0.16\n",
      "2018-04-16T20:43:05.522870: step 8, loss 2.38735, acc 0.61\n",
      "2018-04-16T20:43:06.021641: step 9, loss 2.71408, acc 0.64\n",
      "2018-04-16T20:43:06.521300: step 10, loss 2.91001, acc 0.59\n",
      "2018-04-16T20:43:07.020237: step 11, loss 2.57654, acc 0.54\n",
      "2018-04-16T20:43:07.516527: step 12, loss 1.65551, acc 0.68\n",
      "2018-04-16T20:43:08.013202: step 13, loss 1.74936, acc 0.59\n",
      "2018-04-16T20:43:08.509969: step 14, loss 1.87808, acc 0.28\n",
      "2018-04-16T20:43:09.006923: step 15, loss 1.67429, acc 0.23\n",
      "2018-04-16T20:43:09.503159: step 16, loss 1.62453, acc 0.26\n",
      "2018-04-16T20:43:10.000108: step 17, loss 1.41526, acc 0.21\n",
      "2018-04-16T20:43:10.496367: step 18, loss 1.35554, acc 0.53\n",
      "2018-04-16T20:43:10.992128: step 19, loss 1.24028, acc 0.58\n",
      "2018-04-16T20:43:11.486599: step 20, loss 1.18893, acc 0.61\n",
      "2018-04-16T20:43:11.981671: step 21, loss 1.14704, acc 0.67\n",
      "2018-04-16T20:43:12.475754: step 22, loss 1.25295, acc 0.56\n",
      "2018-04-16T20:43:12.970440: step 23, loss 1.23203, acc 0.56\n",
      "2018-04-16T20:43:13.466909: step 24, loss 1.19696, acc 0.63\n",
      "2018-04-16T20:43:13.962025: step 25, loss 1.25529, acc 0.52\n",
      "2018-04-16T20:43:14.456229: step 26, loss 1.14057, acc 0.66\n",
      "2018-04-16T20:43:14.950609: step 27, loss 1.25427, acc 0.57\n",
      "2018-04-16T20:43:15.444527: step 28, loss 1.35869, acc 0.47\n",
      "2018-04-16T20:43:15.936027: step 29, loss 1.22049, acc 0.59\n",
      "2018-04-16T20:43:16.426131: step 30, loss 1.29174, acc 0.49\n",
      "2018-04-16T20:43:16.917202: step 31, loss 1.21943, acc 0.58\n",
      "2018-04-16T20:43:17.407200: step 32, loss 1.23745, acc 0.55\n",
      "2018-04-16T20:43:17.898848: step 33, loss 1.26827, acc 0.52\n",
      "2018-04-16T20:43:18.390441: step 34, loss 1.14007, acc 0.65\n",
      "2018-04-16T20:43:18.879008: step 35, loss 1.24071, acc 0.51\n",
      "2018-04-16T20:43:19.368402: step 36, loss 1.18193, acc 0.58\n",
      "2018-04-16T20:43:19.858689: step 37, loss 1.18652, acc 0.57\n",
      "2018-04-16T20:43:20.348368: step 38, loss 1.18473, acc 0.57\n",
      "2018-04-16T20:43:20.838053: step 39, loss 1.21777, acc 0.52\n",
      "2018-04-16T20:43:21.327861: step 40, loss 1.15359, acc 0.6\n",
      "2018-04-16T20:43:21.818009: step 41, loss 1.15094, acc 0.6\n",
      "2018-04-16T20:43:22.306955: step 42, loss 1.24003, acc 0.48\n",
      "2018-04-16T20:43:22.795795: step 43, loss 1.12288, acc 0.63\n",
      "2018-04-16T20:43:23.284058: step 44, loss 1.10205, acc 0.64\n",
      "2018-04-16T20:43:23.771457: step 45, loss 1.13286, acc 0.6\n",
      "2018-04-16T20:43:24.257847: step 46, loss 1.16169, acc 0.57\n",
      "2018-04-16T20:43:24.745850: step 47, loss 1.13289, acc 0.59\n",
      "2018-04-16T20:43:25.234079: step 48, loss 1.14517, acc 0.57\n",
      "2018-04-16T20:43:25.720777: step 49, loss 1.16373, acc 0.56\n",
      "2018-04-16T20:43:26.208729: step 50, loss 1.13063, acc 0.58\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:43:27.994569: step 50, loss 1.13794, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-50\n",
      "\n",
      "2018-04-16T20:43:28.602758: step 51, loss 1.12164, acc 0.58\n",
      "2018-04-16T20:43:29.089392: step 52, loss 1.14057, acc 0.56\n",
      "2018-04-16T20:43:29.575648: step 53, loss 1.12009, acc 0.57\n",
      "2018-04-16T20:43:30.060765: step 54, loss 1.1693, acc 0.53\n",
      "2018-04-16T20:43:30.547167: step 55, loss 1.08966, acc 0.59\n",
      "2018-04-16T20:43:31.034621: step 56, loss 1.01125, acc 0.68\n",
      "2018-04-16T20:43:31.520668: step 57, loss 1.10228, acc 0.6\n",
      "2018-04-16T20:43:32.007835: step 58, loss 1.07386, acc 0.62\n",
      "2018-04-16T20:43:32.496016: step 59, loss 1.00455, acc 0.67\n",
      "2018-04-16T20:43:32.982900: step 60, loss 1.13536, acc 0.57\n",
      "2018-04-16T20:43:33.470437: step 61, loss 1.10623, acc 0.57\n",
      "2018-04-16T20:43:33.957577: step 62, loss 0.974727, acc 0.68\n",
      "2018-04-16T20:43:34.446102: step 63, loss 1.01201, acc 0.65\n",
      "2018-04-16T20:43:34.932871: step 64, loss 1.09743, acc 0.58\n",
      "2018-04-16T20:43:35.419351: step 65, loss 1.08392, acc 0.61\n",
      "2018-04-16T20:43:35.905871: step 66, loss 1.11659, acc 0.57\n",
      "2018-04-16T20:43:36.391214: step 67, loss 1.12768, acc 0.56\n",
      "2018-04-16T20:43:36.878075: step 68, loss 1.0633, acc 0.59\n",
      "2018-04-16T20:43:37.365058: step 69, loss 1.0675, acc 0.61\n",
      "2018-04-16T20:43:37.853185: step 70, loss 1.03584, acc 0.6\n",
      "2018-04-16T20:43:38.339656: step 71, loss 1.04748, acc 0.6\n",
      "2018-04-16T20:43:38.826209: step 72, loss 1.0647, acc 0.59\n",
      "2018-04-16T20:43:39.312600: step 73, loss 1.02399, acc 0.63\n",
      "2018-04-16T20:43:39.800577: step 74, loss 0.972445, acc 0.67\n",
      "2018-04-16T20:43:40.286285: step 75, loss 1.04847, acc 0.59\n",
      "2018-04-16T20:43:40.773717: step 76, loss 1.14619, acc 0.51\n",
      "2018-04-16T20:43:41.261022: step 77, loss 1.01931, acc 0.62\n",
      "2018-04-16T20:43:41.749127: step 78, loss 0.969545, acc 0.67\n",
      "2018-04-16T20:43:42.235659: step 79, loss 1.10242, acc 0.54\n",
      "2018-04-16T20:43:42.722438: step 80, loss 1.05889, acc 0.58\n",
      "2018-04-16T20:43:43.210993: step 81, loss 1.08319, acc 0.58\n",
      "2018-04-16T20:43:43.697513: step 82, loss 1.03261, acc 0.59\n",
      "2018-04-16T20:43:44.184450: step 83, loss 1.01538, acc 0.6\n",
      "2018-04-16T20:43:44.671080: step 84, loss 1.08338, acc 0.54\n",
      "2018-04-16T20:43:45.159021: step 85, loss 1.06437, acc 0.58\n",
      "2018-04-16T20:43:45.645692: step 86, loss 1.0712, acc 0.58\n",
      "2018-04-16T20:43:46.132698: step 87, loss 1.00421, acc 0.63\n",
      "2018-04-16T20:43:46.621688: step 88, loss 1.2205, acc 0.39\n",
      "2018-04-16T20:43:47.108129: step 89, loss 1.05711, acc 0.56\n",
      "2018-04-16T20:43:47.595792: step 90, loss 1.13726, acc 0.49\n",
      "2018-04-16T20:43:48.083976: step 91, loss 1.09649, acc 0.54\n",
      "2018-04-16T20:43:48.572966: step 92, loss 1.11467, acc 0.48\n",
      "2018-04-16T20:43:49.060037: step 93, loss 1.07997, acc 0.56\n",
      "2018-04-16T20:43:49.546857: step 94, loss 0.988494, acc 0.68\n",
      "2018-04-16T20:43:50.034545: step 95, loss 0.998317, acc 0.64\n",
      "2018-04-16T20:43:50.523567: step 96, loss 1.06793, acc 0.57\n",
      "2018-04-16T20:43:51.009588: step 97, loss 1.01435, acc 0.6\n",
      "2018-04-16T20:43:51.497281: step 98, loss 1.09581, acc 0.51\n",
      "2018-04-16T20:43:51.985155: step 99, loss 1.01664, acc 0.58\n",
      "2018-04-16T20:43:52.472758: step 100, loss 1.08048, acc 0.5\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:43:53.529967: step 100, loss 1.04295, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-100\n",
      "\n",
      "2018-04-16T20:43:54.139368: step 101, loss 1.04999, acc 0.55\n",
      "2018-04-16T20:43:54.626627: step 102, loss 1.07451, acc 0.54\n",
      "2018-04-16T20:43:55.114253: step 103, loss 1.05814, acc 0.55\n",
      "2018-04-16T20:43:55.602103: step 104, loss 0.975688, acc 0.61\n",
      "2018-04-16T20:43:56.088211: step 105, loss 1.07133, acc 0.52\n",
      "2018-04-16T20:43:56.574730: step 106, loss 1.0151, acc 0.59\n",
      "2018-04-16T20:43:57.062336: step 107, loss 0.919933, acc 0.69\n",
      "2018-04-16T20:43:57.547550: step 108, loss 1.11118, acc 0.47\n",
      "2018-04-16T20:43:58.033333: step 109, loss 1.03813, acc 0.56\n",
      "2018-04-16T20:43:58.520026: step 110, loss 0.963433, acc 0.64\n",
      "2018-04-16T20:43:59.008101: step 111, loss 0.912602, acc 0.69\n",
      "2018-04-16T20:43:59.495640: step 112, loss 1.08022, acc 0.54\n",
      "2018-04-16T20:43:59.983302: step 113, loss 1.08756, acc 0.51\n",
      "2018-04-16T20:44:00.472119: step 114, loss 1.07179, acc 0.54\n",
      "2018-04-16T20:44:00.960278: step 115, loss 1.05687, acc 0.53\n",
      "2018-04-16T20:44:01.448308: step 116, loss 1.03123, acc 0.57\n",
      "2018-04-16T20:44:01.936141: step 117, loss 1.04524, acc 0.57\n",
      "2018-04-16T20:44:02.423806: step 118, loss 1.03932, acc 0.57\n",
      "2018-04-16T20:44:02.911173: step 119, loss 0.955089, acc 0.64\n",
      "2018-04-16T20:44:03.397707: step 120, loss 1.01874, acc 0.59\n",
      "2018-04-16T20:44:03.885361: step 121, loss 1.01045, acc 0.59\n",
      "2018-04-16T20:44:04.373116: step 122, loss 0.969157, acc 0.62\n",
      "2018-04-16T20:44:04.859972: step 123, loss 0.96961, acc 0.61\n",
      "2018-04-16T20:44:05.346470: step 124, loss 0.95361, acc 0.64\n",
      "2018-04-16T20:44:05.832806: step 125, loss 0.998455, acc 0.59\n",
      "2018-04-16T20:44:06.321931: step 126, loss 0.983513, acc 0.6\n",
      "2018-04-16T20:44:06.809291: step 127, loss 1.01637, acc 0.57\n",
      "2018-04-16T20:44:07.296875: step 128, loss 1.02439, acc 0.56\n",
      "2018-04-16T20:44:07.785738: step 129, loss 1.08274, acc 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:44:08.274798: step 130, loss 1.0154, acc 0.57\n",
      "2018-04-16T20:44:08.763939: step 131, loss 0.938944, acc 0.63\n",
      "2018-04-16T20:44:09.249735: step 132, loss 1.06762, acc 0.51\n",
      "2018-04-16T20:44:09.736167: step 133, loss 0.996629, acc 0.58\n",
      "2018-04-16T20:44:10.224549: step 134, loss 1.088, acc 0.49\n",
      "2018-04-16T20:44:10.712383: step 135, loss 0.975097, acc 0.61\n",
      "2018-04-16T20:44:11.200913: step 136, loss 0.908997, acc 0.67\n",
      "2018-04-16T20:44:11.688850: step 137, loss 1.07322, acc 0.5\n",
      "2018-04-16T20:44:12.176486: step 138, loss 0.939516, acc 0.63\n",
      "2018-04-16T20:44:12.662890: step 139, loss 0.947468, acc 0.62\n",
      "2018-04-16T20:44:13.149844: step 140, loss 0.944363, acc 0.62\n",
      "2018-04-16T20:44:13.639495: step 141, loss 1.00068, acc 0.55\n",
      "2018-04-16T20:44:14.128061: step 142, loss 1.0548, acc 0.54\n",
      "2018-04-16T20:44:14.614825: step 143, loss 1.02315, acc 0.54\n",
      "2018-04-16T20:44:15.102096: step 144, loss 0.940664, acc 0.61\n",
      "2018-04-16T20:44:15.589785: step 145, loss 0.925949, acc 0.65\n",
      "2018-04-16T20:44:16.077986: step 146, loss 0.961367, acc 0.61\n",
      "2018-04-16T20:44:16.564599: step 147, loss 0.865365, acc 0.7\n",
      "2018-04-16T20:44:17.051597: step 148, loss 1.03553, acc 0.59\n",
      "2018-04-16T20:44:17.538587: step 149, loss 0.897757, acc 0.67\n",
      "2018-04-16T20:44:18.026447: step 150, loss 1.02868, acc 0.53\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:44:19.084160: step 150, loss 1.01074, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-150\n",
      "\n",
      "2018-04-16T20:44:19.703153: step 151, loss 1.05513, acc 0.53\n",
      "2018-04-16T20:44:20.189980: step 152, loss 0.981665, acc 0.56\n",
      "2018-04-16T20:44:20.677937: step 153, loss 0.937981, acc 0.65\n",
      "2018-04-16T20:44:21.165324: step 154, loss 0.99702, acc 0.58\n",
      "2018-04-16T20:44:21.651612: step 155, loss 1.03006, acc 0.54\n",
      "2018-04-16T20:44:22.139610: step 156, loss 0.983811, acc 0.59\n",
      "2018-04-16T20:44:22.627286: step 157, loss 0.972524, acc 0.59\n",
      "2018-04-16T20:44:23.114390: step 158, loss 1.01285, acc 0.57\n",
      "2018-04-16T20:44:23.602124: step 159, loss 0.888175, acc 0.68\n",
      "2018-04-16T20:44:24.088991: step 160, loss 0.974432, acc 0.58\n",
      "2018-04-16T20:44:24.575879: step 161, loss 0.884137, acc 0.68\n",
      "2018-04-16T20:44:25.063811: step 162, loss 1.01366, acc 0.55\n",
      "2018-04-16T20:44:25.550954: step 163, loss 0.997446, acc 0.57\n",
      "2018-04-16T20:44:26.037881: step 164, loss 0.988494, acc 0.58\n",
      "2018-04-16T20:44:26.523444: step 165, loss 1.06741, acc 0.5\n",
      "2018-04-16T20:44:27.010742: step 166, loss 0.967132, acc 0.6\n",
      "2018-04-16T20:44:27.496920: step 167, loss 0.967391, acc 0.58\n",
      "2018-04-16T20:44:27.983140: step 168, loss 0.948873, acc 0.61\n",
      "2018-04-16T20:44:28.472284: step 169, loss 0.915526, acc 0.63\n",
      "2018-04-16T20:44:28.959482: step 170, loss 0.969512, acc 0.58\n",
      "2018-04-16T20:44:29.447332: step 171, loss 1.0541, acc 0.52\n",
      "2018-04-16T20:44:29.934996: step 172, loss 0.988866, acc 0.55\n",
      "2018-04-16T20:44:30.421329: step 173, loss 1.09956, acc 0.48\n",
      "2018-04-16T20:44:30.908201: step 174, loss 1.07449, acc 0.5\n",
      "2018-04-16T20:44:31.395492: step 175, loss 0.977801, acc 0.58\n",
      "2018-04-16T20:44:31.882432: step 176, loss 1.02089, acc 0.54\n",
      "2018-04-16T20:44:32.368757: step 177, loss 0.964008, acc 0.59\n",
      "2018-04-16T20:44:32.858802: step 178, loss 0.967112, acc 0.59\n",
      "2018-04-16T20:44:33.346613: step 179, loss 0.906597, acc 0.68\n",
      "2018-04-16T20:44:33.833270: step 180, loss 0.998783, acc 0.54\n",
      "2018-04-16T20:44:34.321507: step 181, loss 0.926512, acc 0.62\n",
      "2018-04-16T20:44:34.810010: step 182, loss 0.951167, acc 0.61\n",
      "2018-04-16T20:44:35.297285: step 183, loss 1.01018, acc 0.53\n",
      "2018-04-16T20:44:35.785757: step 184, loss 0.953369, acc 0.6\n",
      "2018-04-16T20:44:36.272210: step 185, loss 0.966391, acc 0.57\n",
      "2018-04-16T20:44:36.759875: step 186, loss 1.02543, acc 0.54\n",
      "2018-04-16T20:44:37.248863: step 187, loss 0.913604, acc 0.65\n",
      "2018-04-16T20:44:37.735924: step 188, loss 0.974279, acc 0.55\n",
      "2018-04-16T20:44:38.223588: step 189, loss 0.974141, acc 0.57\n",
      "2018-04-16T20:44:38.711491: step 190, loss 0.856037, acc 0.67\n",
      "2018-04-16T20:44:39.201199: step 191, loss 0.979362, acc 0.57\n",
      "2018-04-16T20:44:39.688381: step 192, loss 1.02354, acc 0.55\n",
      "2018-04-16T20:44:40.175520: step 193, loss 0.981308, acc 0.6\n",
      "2018-04-16T20:44:40.663840: step 194, loss 1.00408, acc 0.58\n",
      "2018-04-16T20:44:41.150750: step 195, loss 1.0213, acc 0.54\n",
      "2018-04-16T20:44:41.639176: step 196, loss 0.860538, acc 0.7\n",
      "2018-04-16T20:44:42.126008: step 197, loss 1.03374, acc 0.54\n",
      "2018-04-16T20:44:42.614255: step 198, loss 0.943273, acc 0.59\n",
      "2018-04-16T20:44:43.101958: step 199, loss 0.989435, acc 0.56\n",
      "2018-04-16T20:44:43.588466: step 200, loss 0.987051, acc 0.54\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:44:44.646378: step 200, loss 0.98909, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-200\n",
      "\n",
      "2018-04-16T20:44:45.252149: step 201, loss 0.960679, acc 0.6\n",
      "2018-04-16T20:44:45.740705: step 202, loss 0.995324, acc 0.59\n",
      "2018-04-16T20:44:46.228969: step 203, loss 1.03843, acc 0.52\n",
      "2018-04-16T20:44:46.716027: step 204, loss 0.993616, acc 0.56\n",
      "2018-04-16T20:44:47.203496: step 205, loss 1.02979, acc 0.51\n",
      "2018-04-16T20:44:47.689831: step 206, loss 0.955384, acc 0.59\n",
      "2018-04-16T20:44:48.176488: step 207, loss 0.994548, acc 0.56\n",
      "2018-04-16T20:44:48.663641: step 208, loss 0.934166, acc 0.62\n",
      "2018-04-16T20:44:49.151131: step 209, loss 0.987717, acc 0.57\n",
      "2018-04-16T20:44:49.638316: step 210, loss 0.984698, acc 0.56\n",
      "2018-04-16T20:44:50.125410: step 211, loss 0.982038, acc 0.57\n",
      "2018-04-16T20:44:50.612477: step 212, loss 0.963057, acc 0.61\n",
      "2018-04-16T20:44:51.099922: step 213, loss 0.944066, acc 0.62\n",
      "2018-04-16T20:44:51.588511: step 214, loss 0.929149, acc 0.62\n",
      "2018-04-16T20:44:52.075575: step 215, loss 0.895806, acc 0.64\n",
      "2018-04-16T20:44:52.563197: step 216, loss 1.01759, acc 0.54\n",
      "2018-04-16T20:44:53.052198: step 217, loss 1.03121, acc 0.51\n",
      "2018-04-16T20:44:53.541219: step 218, loss 0.974206, acc 0.57\n",
      "2018-04-16T20:44:54.027535: step 219, loss 0.993805, acc 0.58\n",
      "2018-04-16T20:44:54.515116: step 220, loss 0.958764, acc 0.61\n",
      "2018-04-16T20:44:55.002821: step 221, loss 0.981627, acc 0.57\n",
      "2018-04-16T20:44:55.489857: step 222, loss 0.970991, acc 0.59\n",
      "2018-04-16T20:44:55.976985: step 223, loss 0.932744, acc 0.61\n",
      "2018-04-16T20:44:56.463670: step 224, loss 0.950491, acc 0.61\n",
      "2018-04-16T20:44:56.950889: step 225, loss 0.966613, acc 0.57\n",
      "2018-04-16T20:44:57.438061: step 226, loss 1.00787, acc 0.55\n",
      "2018-04-16T20:44:57.926109: step 227, loss 0.962339, acc 0.58\n",
      "2018-04-16T20:44:58.412619: step 228, loss 0.978893, acc 0.57\n",
      "2018-04-16T20:44:58.899621: step 229, loss 0.91246, acc 0.63\n",
      "2018-04-16T20:44:59.388568: step 230, loss 0.970007, acc 0.57\n",
      "2018-04-16T20:44:59.876676: step 231, loss 0.978672, acc 0.57\n",
      "2018-04-16T20:45:00.364465: step 232, loss 1.04525, acc 0.5\n",
      "2018-04-16T20:45:00.853372: step 233, loss 1.01635, acc 0.53\n",
      "2018-04-16T20:45:01.340921: step 234, loss 0.994773, acc 0.57\n",
      "2018-04-16T20:45:01.827915: step 235, loss 0.953437, acc 0.6\n",
      "2018-04-16T20:45:02.314700: step 236, loss 0.967344, acc 0.58\n",
      "2018-04-16T20:45:02.803067: step 237, loss 0.998363, acc 0.56\n",
      "2018-04-16T20:45:03.290408: step 238, loss 0.973744, acc 0.6\n",
      "2018-04-16T20:45:03.777423: step 239, loss 0.966874, acc 0.57\n",
      "2018-04-16T20:45:04.265065: step 240, loss 0.875273, acc 0.69\n",
      "2018-04-16T20:45:04.754111: step 241, loss 1.01017, acc 0.55\n",
      "2018-04-16T20:45:05.243760: step 242, loss 0.99159, acc 0.55\n",
      "2018-04-16T20:45:05.731547: step 243, loss 0.993826, acc 0.55\n",
      "2018-04-16T20:45:06.218951: step 244, loss 1.00833, acc 0.55\n",
      "2018-04-16T20:45:06.706656: step 245, loss 1.00202, acc 0.56\n",
      "2018-04-16T20:45:07.194173: step 246, loss 0.933695, acc 0.62\n",
      "2018-04-16T20:45:07.680977: step 247, loss 0.868499, acc 0.67\n",
      "2018-04-16T20:45:08.167709: step 248, loss 0.983109, acc 0.57\n",
      "2018-04-16T20:45:08.654404: step 249, loss 1.00446, acc 0.56\n",
      "2018-04-16T20:45:09.143299: step 250, loss 1.0522, acc 0.51\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:45:10.203536: step 250, loss 0.982453, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-250\n",
      "\n",
      "2018-04-16T20:45:10.806840: step 251, loss 0.964847, acc 0.6\n",
      "2018-04-16T20:45:11.293435: step 252, loss 0.963015, acc 0.58\n",
      "2018-04-16T20:45:11.781222: step 253, loss 0.965131, acc 0.58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:45:12.267710: step 254, loss 1.00628, acc 0.54\n",
      "2018-04-16T20:45:12.754739: step 255, loss 0.951986, acc 0.59\n",
      "2018-04-16T20:45:13.242995: step 256, loss 0.96315, acc 0.59\n",
      "2018-04-16T20:45:13.730943: step 257, loss 0.956107, acc 0.6\n",
      "2018-04-16T20:45:14.219068: step 258, loss 1.00301, acc 0.55\n",
      "2018-04-16T20:45:14.707338: step 259, loss 1.00894, acc 0.55\n",
      "2018-04-16T20:45:15.195876: step 260, loss 0.991132, acc 0.56\n",
      "2018-04-16T20:45:15.683920: step 261, loss 1.05364, acc 0.47\n",
      "2018-04-16T20:45:16.170893: step 262, loss 0.940114, acc 0.6\n",
      "2018-04-16T20:45:16.658866: step 263, loss 0.981324, acc 0.57\n",
      "2018-04-16T20:45:17.146833: step 264, loss 0.931152, acc 0.64\n",
      "2018-04-16T20:45:17.636265: step 265, loss 0.917951, acc 0.65\n",
      "2018-04-16T20:45:18.124534: step 266, loss 1.0198, acc 0.53\n",
      "2018-04-16T20:45:18.611843: step 267, loss 0.9814, acc 0.57\n",
      "2018-04-16T20:45:19.100056: step 268, loss 0.908722, acc 0.64\n",
      "2018-04-16T20:45:19.589058: step 269, loss 1.00357, acc 0.54\n",
      "2018-04-16T20:45:20.076267: step 270, loss 0.901045, acc 0.63\n",
      "2018-04-16T20:45:20.563056: step 271, loss 0.931268, acc 0.61\n",
      "2018-04-16T20:45:21.051704: step 272, loss 1.01602, acc 0.53\n",
      "2018-04-16T20:45:21.539724: step 273, loss 0.894455, acc 0.64\n",
      "2018-04-16T20:45:22.027049: step 274, loss 1.01413, acc 0.57\n",
      "2018-04-16T20:45:22.515802: step 275, loss 0.905343, acc 0.64\n",
      "2018-04-16T20:45:23.004211: step 276, loss 0.918456, acc 0.62\n",
      "2018-04-16T20:45:23.493433: step 277, loss 0.949434, acc 0.58\n",
      "2018-04-16T20:45:23.981586: step 278, loss 0.948987, acc 0.59\n",
      "2018-04-16T20:45:24.470846: step 279, loss 0.9822, acc 0.58\n",
      "2018-04-16T20:45:24.957344: step 280, loss 1.01436, acc 0.55\n",
      "2018-04-16T20:45:25.445705: step 281, loss 1.03355, acc 0.53\n",
      "2018-04-16T20:45:25.933425: step 282, loss 0.904523, acc 0.64\n",
      "2018-04-16T20:45:26.421424: step 283, loss 0.865796, acc 0.68\n",
      "2018-04-16T20:45:26.908701: step 284, loss 0.95336, acc 0.6\n",
      "2018-04-16T20:45:27.396281: step 285, loss 0.973516, acc 0.58\n",
      "2018-04-16T20:45:27.883959: step 286, loss 0.960309, acc 0.59\n",
      "2018-04-16T20:45:28.371468: step 287, loss 0.882381, acc 0.65\n",
      "2018-04-16T20:45:28.858866: step 288, loss 1.0374, acc 0.54\n",
      "2018-04-16T20:45:29.346118: step 289, loss 0.970064, acc 0.58\n",
      "2018-04-16T20:45:29.834513: step 290, loss 0.989722, acc 0.55\n",
      "2018-04-16T20:45:30.322108: step 291, loss 0.9679, acc 0.58\n",
      "2018-04-16T20:45:30.809519: step 292, loss 1.03652, acc 0.52\n",
      "2018-04-16T20:45:31.297597: step 293, loss 0.935161, acc 0.61\n",
      "2018-04-16T20:45:31.786029: step 294, loss 0.995342, acc 0.54\n",
      "2018-04-16T20:45:32.273996: step 295, loss 1.12551, acc 0.4\n",
      "2018-04-16T20:45:32.762693: step 296, loss 1.05347, acc 0.49\n",
      "2018-04-16T20:45:33.252058: step 297, loss 0.951581, acc 0.62\n",
      "2018-04-16T20:45:33.740035: step 298, loss 0.995396, acc 0.58\n",
      "2018-04-16T20:45:34.229446: step 299, loss 1.02313, acc 0.54\n",
      "2018-04-16T20:45:34.718042: step 300, loss 0.965231, acc 0.62\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:45:35.775386: step 300, loss 0.990738, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-300\n",
      "\n",
      "2018-04-16T20:45:36.389659: step 301, loss 1.03174, acc 0.49\n",
      "2018-04-16T20:45:36.877927: step 302, loss 0.978634, acc 0.56\n",
      "2018-04-16T20:45:37.366838: step 303, loss 0.959117, acc 0.59\n",
      "2018-04-16T20:45:37.854111: step 304, loss 0.955346, acc 0.62\n",
      "2018-04-16T20:45:38.341880: step 305, loss 0.969511, acc 0.57\n",
      "2018-04-16T20:45:38.830219: step 306, loss 0.920574, acc 0.61\n",
      "2018-04-16T20:45:39.318129: step 307, loss 0.926039, acc 0.61\n",
      "2018-04-16T20:45:39.805935: step 308, loss 0.961609, acc 0.57\n",
      "2018-04-16T20:45:40.294094: step 309, loss 0.967869, acc 0.58\n",
      "2018-04-16T20:45:40.781981: step 310, loss 0.951462, acc 0.59\n",
      "2018-04-16T20:45:41.267918: step 311, loss 1.01928, acc 0.56\n",
      "2018-04-16T20:45:41.754437: step 312, loss 0.855168, acc 0.66\n",
      "2018-04-16T20:45:42.245450: step 313, loss 1.03762, acc 0.54\n",
      "2018-04-16T20:45:42.733370: step 314, loss 0.874792, acc 0.67\n",
      "2018-04-16T20:45:43.221025: step 315, loss 0.958989, acc 0.59\n",
      "2018-04-16T20:45:43.709272: step 316, loss 0.869619, acc 0.66\n",
      "2018-04-16T20:45:44.198621: step 317, loss 0.903808, acc 0.63\n",
      "2018-04-16T20:45:44.684439: step 318, loss 0.955869, acc 0.59\n",
      "2018-04-16T20:45:45.171465: step 319, loss 0.92891, acc 0.61\n",
      "2018-04-16T20:45:45.659214: step 320, loss 1.02733, acc 0.54\n",
      "2018-04-16T20:45:46.145461: step 321, loss 0.943074, acc 0.59\n",
      "2018-04-16T20:45:46.633463: step 322, loss 0.954118, acc 0.59\n",
      "2018-04-16T20:45:47.122167: step 323, loss 0.896121, acc 0.65\n",
      "2018-04-16T20:45:47.608765: step 324, loss 0.931618, acc 0.64\n",
      "2018-04-16T20:45:48.098377: step 325, loss 0.940119, acc 0.61\n",
      "2018-04-16T20:45:48.586399: step 326, loss 0.925833, acc 0.6\n",
      "2018-04-16T20:45:49.074299: step 327, loss 0.959792, acc 0.59\n",
      "2018-04-16T20:45:49.563127: step 328, loss 0.902566, acc 0.63\n",
      "2018-04-16T20:45:50.051399: step 329, loss 0.773927, acc 0.77\n",
      "2018-04-16T20:45:50.538971: step 330, loss 0.933893, acc 0.59\n",
      "2018-04-16T20:45:51.027178: step 331, loss 0.921778, acc 0.62\n",
      "2018-04-16T20:45:51.514438: step 332, loss 0.999721, acc 0.57\n",
      "2018-04-16T20:45:52.001446: step 333, loss 0.992499, acc 0.57\n",
      "2018-04-16T20:45:52.487822: step 334, loss 1.00998, acc 0.55\n",
      "2018-04-16T20:45:52.974954: step 335, loss 0.954089, acc 0.6\n",
      "2018-04-16T20:45:53.462372: step 336, loss 0.932268, acc 0.62\n",
      "2018-04-16T20:45:53.950567: step 337, loss 0.90654, acc 0.64\n",
      "2018-04-16T20:45:54.439800: step 338, loss 0.913895, acc 0.63\n",
      "2018-04-16T20:45:54.928331: step 339, loss 0.890692, acc 0.67\n",
      "2018-04-16T20:45:55.414747: step 340, loss 0.966757, acc 0.58\n",
      "2018-04-16T20:45:55.901377: step 341, loss 0.996823, acc 0.53\n",
      "2018-04-16T20:45:56.389455: step 342, loss 0.875346, acc 0.67\n",
      "2018-04-16T20:45:56.878884: step 343, loss 0.929804, acc 0.63\n",
      "2018-04-16T20:45:57.367900: step 344, loss 1.00391, acc 0.54\n",
      "2018-04-16T20:45:57.855209: step 345, loss 0.975305, acc 0.57\n",
      "2018-04-16T20:45:58.343245: step 346, loss 0.943645, acc 0.59\n",
      "2018-04-16T20:45:58.831691: step 347, loss 0.960587, acc 0.57\n",
      "2018-04-16T20:45:59.321256: step 348, loss 0.987863, acc 0.56\n",
      "2018-04-16T20:45:59.808531: step 349, loss 1.0384, acc 0.49\n",
      "2018-04-16T20:46:00.298679: step 350, loss 0.977189, acc 0.56\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:46:01.395063: step 350, loss 0.979689, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-350\n",
      "\n",
      "2018-04-16T20:46:02.010255: step 351, loss 0.955562, acc 0.6\n",
      "2018-04-16T20:46:02.498447: step 352, loss 0.862777, acc 0.7\n",
      "2018-04-16T20:46:02.987028: step 353, loss 0.976931, acc 0.55\n",
      "2018-04-16T20:46:03.475491: step 354, loss 0.928148, acc 0.61\n",
      "2018-04-16T20:46:03.963225: step 355, loss 0.870023, acc 0.7\n",
      "2018-04-16T20:46:04.449290: step 356, loss 1.03224, acc 0.53\n",
      "2018-04-16T20:46:04.936139: step 357, loss 1.08881, acc 0.5\n",
      "2018-04-16T20:46:05.424277: step 358, loss 0.874062, acc 0.65\n",
      "2018-04-16T20:46:05.912063: step 359, loss 0.896718, acc 0.63\n",
      "2018-04-16T20:46:06.398960: step 360, loss 0.918839, acc 0.62\n",
      "2018-04-16T20:46:06.885586: step 361, loss 0.923642, acc 0.6\n",
      "2018-04-16T20:46:07.372381: step 362, loss 1.01621, acc 0.54\n",
      "2018-04-16T20:46:07.860262: step 363, loss 0.987811, acc 0.56\n",
      "2018-04-16T20:46:08.347100: step 364, loss 1.08554, acc 0.48\n",
      "2018-04-16T20:46:08.837176: step 365, loss 0.973201, acc 0.56\n",
      "2018-04-16T20:46:09.326355: step 366, loss 0.837472, acc 0.71\n",
      "2018-04-16T20:46:09.814065: step 367, loss 0.915864, acc 0.63\n",
      "2018-04-16T20:46:10.302415: step 368, loss 0.938716, acc 0.62\n",
      "2018-04-16T20:46:10.790270: step 369, loss 0.933666, acc 0.62\n",
      "2018-04-16T20:46:11.278031: step 370, loss 1.0386, acc 0.5\n",
      "2018-04-16T20:46:11.765795: step 371, loss 0.855729, acc 0.69\n",
      "2018-04-16T20:46:12.254097: step 372, loss 0.967833, acc 0.58\n",
      "2018-04-16T20:46:12.743097: step 373, loss 0.930486, acc 0.61\n",
      "2018-04-16T20:46:13.231871: step 374, loss 0.91442, acc 0.61\n",
      "2018-04-16T20:46:13.719669: step 375, loss 0.929693, acc 0.63\n",
      "2018-04-16T20:46:14.207545: step 376, loss 0.996172, acc 0.55\n",
      "2018-04-16T20:46:14.694701: step 377, loss 0.906959, acc 0.63\n",
      "2018-04-16T20:46:15.183642: step 378, loss 1.01823, acc 0.53\n",
      "2018-04-16T20:46:15.671520: step 379, loss 0.963114, acc 0.58\n",
      "2018-04-16T20:46:16.160001: step 380, loss 0.942413, acc 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:46:16.645968: step 381, loss 0.880899, acc 0.66\n",
      "2018-04-16T20:46:17.134908: step 382, loss 0.948663, acc 0.58\n",
      "2018-04-16T20:46:17.622314: step 383, loss 0.976013, acc 0.56\n",
      "2018-04-16T20:46:18.109634: step 384, loss 0.946924, acc 0.59\n",
      "2018-04-16T20:46:18.597039: step 385, loss 0.993382, acc 0.52\n",
      "2018-04-16T20:46:19.084236: step 386, loss 1.03216, acc 0.48\n",
      "2018-04-16T20:46:19.571693: step 387, loss 0.935271, acc 0.61\n",
      "2018-04-16T20:46:20.061165: step 388, loss 1.0159, acc 0.51\n",
      "2018-04-16T20:46:20.549051: step 389, loss 0.949406, acc 0.62\n",
      "2018-04-16T20:46:21.036793: step 390, loss 0.965707, acc 0.56\n",
      "2018-04-16T20:46:21.522914: step 391, loss 0.967186, acc 0.53\n",
      "2018-04-16T20:46:22.012239: step 392, loss 0.953824, acc 0.63\n",
      "2018-04-16T20:46:22.499404: step 393, loss 0.923805, acc 0.59\n",
      "2018-04-16T20:46:22.987638: step 394, loss 0.914874, acc 0.64\n",
      "2018-04-16T20:46:23.475665: step 395, loss 0.857701, acc 0.68\n",
      "2018-04-16T20:46:23.964223: step 396, loss 0.883201, acc 0.66\n",
      "2018-04-16T20:46:24.451770: step 397, loss 0.902587, acc 0.64\n",
      "2018-04-16T20:46:24.940422: step 398, loss 0.989872, acc 0.57\n",
      "2018-04-16T20:46:25.429727: step 399, loss 0.947989, acc 0.6\n",
      "2018-04-16T20:46:25.917409: step 400, loss 0.874124, acc 0.65\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:46:27.001304: step 400, loss 0.989119, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-400\n",
      "\n",
      "2018-04-16T20:46:27.613901: step 401, loss 1.02396, acc 0.55\n",
      "2018-04-16T20:46:28.102408: step 402, loss 0.857265, acc 0.66\n",
      "2018-04-16T20:46:28.591329: step 403, loss 0.982568, acc 0.57\n",
      "2018-04-16T20:46:29.080095: step 404, loss 0.997403, acc 0.55\n",
      "2018-04-16T20:46:29.568323: step 405, loss 1.01063, acc 0.52\n",
      "2018-04-16T20:46:30.058173: step 406, loss 0.837118, acc 0.69\n",
      "2018-04-16T20:46:30.549932: step 407, loss 0.852473, acc 0.69\n",
      "2018-04-16T20:46:31.039654: step 408, loss 0.965752, acc 0.56\n",
      "2018-04-16T20:46:31.530943: step 409, loss 0.928359, acc 0.6\n",
      "2018-04-16T20:46:32.020211: step 410, loss 0.961788, acc 0.56\n",
      "2018-04-16T20:46:32.510168: step 411, loss 1.01315, acc 0.5\n",
      "2018-04-16T20:46:33.003146: step 412, loss 0.934897, acc 0.58\n",
      "2018-04-16T20:46:33.491975: step 413, loss 0.968765, acc 0.57\n",
      "2018-04-16T20:46:33.981076: step 414, loss 0.941559, acc 0.62\n",
      "2018-04-16T20:46:34.469667: step 415, loss 0.939218, acc 0.6\n",
      "2018-04-16T20:46:34.959845: step 416, loss 0.895094, acc 0.62\n",
      "2018-04-16T20:46:35.451217: step 417, loss 0.928275, acc 0.58\n",
      "2018-04-16T20:46:35.940665: step 418, loss 0.93903, acc 0.56\n",
      "2018-04-16T20:46:36.431767: step 419, loss 0.902991, acc 0.62\n",
      "2018-04-16T20:46:36.923135: step 420, loss 0.992118, acc 0.52\n",
      "2018-04-16T20:46:37.413254: step 421, loss 0.986726, acc 0.55\n",
      "2018-04-16T20:46:37.904949: step 422, loss 0.980704, acc 0.55\n",
      "2018-04-16T20:46:38.396486: step 423, loss 0.883987, acc 0.63\n",
      "2018-04-16T20:46:38.887972: step 424, loss 0.981229, acc 0.52\n",
      "2018-04-16T20:46:39.378691: step 425, loss 0.924224, acc 0.57\n",
      "2018-04-16T20:46:39.867103: step 426, loss 0.801414, acc 0.7\n",
      "2018-04-16T20:46:40.357557: step 427, loss 0.957618, acc 0.54\n",
      "2018-04-16T20:46:40.846189: step 428, loss 0.897819, acc 0.6\n",
      "2018-04-16T20:46:41.335568: step 429, loss 0.911433, acc 0.58\n",
      "2018-04-16T20:46:41.826355: step 430, loss 0.98162, acc 0.5\n",
      "2018-04-16T20:46:42.319381: step 431, loss 0.916757, acc 0.57\n",
      "2018-04-16T20:46:42.807841: step 432, loss 0.919625, acc 0.62\n",
      "2018-04-16T20:46:43.299399: step 433, loss 0.832649, acc 0.67\n",
      "2018-04-16T20:46:43.790965: step 434, loss 0.866049, acc 0.63\n",
      "2018-04-16T20:46:44.281502: step 435, loss 0.902358, acc 0.61\n",
      "2018-04-16T20:46:44.773511: step 436, loss 0.889925, acc 0.58\n",
      "2018-04-16T20:46:45.262780: step 437, loss 0.80205, acc 0.67\n",
      "2018-04-16T20:46:45.753227: step 438, loss 0.799193, acc 0.66\n",
      "2018-04-16T20:46:46.244539: step 439, loss 0.795808, acc 0.66\n",
      "2018-04-16T20:46:46.735562: step 440, loss 0.795915, acc 0.66\n",
      "2018-04-16T20:46:47.226698: step 441, loss 0.905728, acc 0.58\n",
      "2018-04-16T20:46:47.717819: step 442, loss 0.941498, acc 0.55\n",
      "2018-04-16T20:46:48.207912: step 443, loss 0.821944, acc 0.62\n",
      "2018-04-16T20:46:48.701264: step 444, loss 0.923356, acc 0.53\n",
      "2018-04-16T20:46:49.191839: step 445, loss 0.860638, acc 0.58\n",
      "2018-04-16T20:46:49.683302: step 446, loss 0.863124, acc 0.56\n",
      "2018-04-16T20:46:50.173521: step 447, loss 0.887833, acc 0.58\n",
      "2018-04-16T20:46:50.665026: step 448, loss 0.862047, acc 0.61\n",
      "2018-04-16T20:46:51.154970: step 449, loss 0.852333, acc 0.54\n",
      "2018-04-16T20:46:51.646502: step 450, loss 0.883277, acc 0.67\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:46:52.706488: step 450, loss 0.870398, acc 0.574956\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-450\n",
      "\n",
      "2018-04-16T20:46:53.320122: step 451, loss 0.943567, acc 0.55\n",
      "2018-04-16T20:46:53.811738: step 452, loss 0.847461, acc 0.57\n",
      "2018-04-16T20:46:54.302656: step 453, loss 0.787854, acc 0.67\n",
      "2018-04-16T20:46:54.792598: step 454, loss 0.854627, acc 0.53\n",
      "2018-04-16T20:46:55.283723: step 455, loss 0.847404, acc 0.58\n",
      "2018-04-16T20:46:55.773286: step 456, loss 0.758593, acc 0.66\n",
      "2018-04-16T20:46:56.266349: step 457, loss 0.914978, acc 0.52\n",
      "2018-04-16T20:46:56.758283: step 458, loss 0.85846, acc 0.55\n",
      "2018-04-16T20:46:57.249715: step 459, loss 0.799731, acc 0.62\n",
      "2018-04-16T20:46:57.742381: step 460, loss 0.838054, acc 0.59\n",
      "2018-04-16T20:46:58.234996: step 461, loss 0.781736, acc 0.61\n",
      "2018-04-16T20:46:58.728441: step 462, loss 0.920149, acc 0.49\n",
      "2018-04-16T20:46:59.221077: step 463, loss 0.814314, acc 0.6\n",
      "2018-04-16T20:46:59.710797: step 464, loss 0.78068, acc 0.6\n",
      "2018-04-16T20:47:00.203949: step 465, loss 0.873641, acc 0.51\n",
      "2018-04-16T20:47:00.695230: step 466, loss 0.713371, acc 0.72\n",
      "2018-04-16T20:47:01.188101: step 467, loss 0.840734, acc 0.72\n",
      "2018-04-16T20:47:01.679126: step 468, loss 0.916362, acc 0.63\n",
      "2018-04-16T20:47:02.170250: step 469, loss 0.761054, acc 0.81\n",
      "2018-04-16T20:47:02.661751: step 470, loss 0.766824, acc 0.79\n",
      "2018-04-16T20:47:03.153848: step 471, loss 0.792717, acc 0.71\n",
      "2018-04-16T20:47:03.646558: step 472, loss 0.704657, acc 0.82\n",
      "2018-04-16T20:47:04.137905: step 473, loss 0.689265, acc 0.75\n",
      "2018-04-16T20:47:04.629769: step 474, loss 0.803331, acc 0.7\n",
      "2018-04-16T20:47:05.119679: step 475, loss 0.817763, acc 0.61\n",
      "2018-04-16T20:47:05.609643: step 476, loss 0.792358, acc 0.59\n",
      "2018-04-16T20:47:06.100964: step 477, loss 0.772987, acc 0.7\n",
      "2018-04-16T20:47:06.592492: step 478, loss 0.750551, acc 0.66\n",
      "2018-04-16T20:47:07.083472: step 479, loss 0.817399, acc 0.7\n",
      "2018-04-16T20:47:07.575464: step 480, loss 0.670509, acc 0.85\n",
      "2018-04-16T20:47:08.066469: step 481, loss 0.755019, acc 0.78\n",
      "2018-04-16T20:47:08.557627: step 482, loss 0.776603, acc 0.72\n",
      "2018-04-16T20:47:09.048670: step 483, loss 0.661408, acc 0.85\n",
      "2018-04-16T20:47:09.540367: step 484, loss 0.806679, acc 0.69\n",
      "2018-04-16T20:47:10.032582: step 485, loss 0.631347, acc 0.8\n",
      "2018-04-16T20:47:10.523773: step 486, loss 0.785712, acc 0.64\n",
      "2018-04-16T20:47:11.015380: step 487, loss 0.752662, acc 0.72\n",
      "2018-04-16T20:47:11.506929: step 488, loss 0.698989, acc 0.73\n",
      "2018-04-16T20:47:11.998551: step 489, loss 0.736693, acc 0.72\n",
      "2018-04-16T20:47:12.491644: step 490, loss 0.731239, acc 0.73\n",
      "2018-04-16T20:47:12.984891: step 491, loss 0.761202, acc 0.75\n",
      "2018-04-16T20:47:13.477917: step 492, loss 0.741166, acc 0.72\n",
      "2018-04-16T20:47:13.971199: step 493, loss 0.704813, acc 0.69\n",
      "2018-04-16T20:47:14.465132: step 494, loss 0.728716, acc 0.73\n",
      "2018-04-16T20:47:14.960023: step 495, loss 0.676448, acc 0.77\n",
      "2018-04-16T20:47:15.454738: step 496, loss 0.735614, acc 0.72\n",
      "2018-04-16T20:47:15.949786: step 497, loss 0.661724, acc 0.82\n",
      "2018-04-16T20:47:16.444541: step 498, loss 0.647778, acc 0.78\n",
      "2018-04-16T20:47:16.938658: step 499, loss 0.67221, acc 0.72\n",
      "2018-04-16T20:47:17.434216: step 500, loss 0.625117, acc 0.77\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:47:18.495724: step 500, loss 0.658876, acc 0.737213\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-500\n",
      "\n",
      "2018-04-16T20:47:19.115613: step 501, loss 0.671305, acc 0.77\n",
      "2018-04-16T20:47:19.610208: step 502, loss 0.661185, acc 0.75\n",
      "2018-04-16T20:47:20.106197: step 503, loss 0.628188, acc 0.74\n",
      "2018-04-16T20:47:20.600976: step 504, loss 0.600253, acc 0.77\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:47:21.097632: step 505, loss 0.589824, acc 0.78\n",
      "2018-04-16T20:47:21.592325: step 506, loss 0.61951, acc 0.78\n",
      "2018-04-16T20:47:22.087380: step 507, loss 0.687424, acc 0.75\n",
      "2018-04-16T20:47:22.582522: step 508, loss 0.633855, acc 0.74\n",
      "2018-04-16T20:47:23.078651: step 509, loss 0.599565, acc 0.78\n",
      "2018-04-16T20:47:23.574532: step 510, loss 0.634571, acc 0.72\n",
      "2018-04-16T20:47:24.071785: step 511, loss 0.623186, acc 0.76\n",
      "2018-04-16T20:47:24.566325: step 512, loss 0.739762, acc 0.72\n",
      "2018-04-16T20:47:25.062040: step 513, loss 0.662085, acc 0.72\n",
      "2018-04-16T20:47:25.557745: step 514, loss 0.731041, acc 0.72\n",
      "2018-04-16T20:47:26.053558: step 515, loss 0.657628, acc 0.78\n",
      "2018-04-16T20:47:26.549624: step 516, loss 0.697166, acc 0.71\n",
      "2018-04-16T20:47:27.043720: step 517, loss 0.685644, acc 0.74\n",
      "2018-04-16T20:47:27.537640: step 518, loss 0.605948, acc 0.76\n",
      "2018-04-16T20:47:28.031256: step 519, loss 0.625811, acc 0.77\n",
      "2018-04-16T20:47:28.526487: step 520, loss 0.518421, acc 0.82\n",
      "2018-04-16T20:47:29.022150: step 521, loss 0.669138, acc 0.74\n",
      "2018-04-16T20:47:29.517450: step 522, loss 0.7087, acc 0.68\n",
      "2018-04-16T20:47:30.011106: step 523, loss 0.599318, acc 0.76\n",
      "2018-04-16T20:47:30.505027: step 524, loss 0.671466, acc 0.74\n",
      "2018-04-16T20:47:31.001138: step 525, loss 0.629477, acc 0.74\n",
      "2018-04-16T20:47:31.494847: step 526, loss 0.603852, acc 0.77\n",
      "2018-04-16T20:47:31.989934: step 527, loss 0.607293, acc 0.8\n",
      "2018-04-16T20:47:32.485422: step 528, loss 0.663645, acc 0.71\n",
      "2018-04-16T20:47:32.977932: step 529, loss 0.592894, acc 0.8\n",
      "2018-04-16T20:47:33.470626: step 530, loss 0.637315, acc 0.71\n",
      "2018-04-16T20:47:33.964600: step 531, loss 0.675731, acc 0.74\n",
      "2018-04-16T20:47:34.458183: step 532, loss 0.640743, acc 0.81\n",
      "2018-04-16T20:47:34.949680: step 533, loss 0.564066, acc 0.77\n",
      "2018-04-16T20:47:35.441121: step 534, loss 0.684384, acc 0.7\n",
      "2018-04-16T20:47:35.934957: step 535, loss 0.587977, acc 0.78\n",
      "2018-04-16T20:47:36.429645: step 536, loss 0.586416, acc 0.8\n",
      "2018-04-16T20:47:36.922155: step 537, loss 0.641894, acc 0.72\n",
      "2018-04-16T20:47:37.416240: step 538, loss 0.587461, acc 0.73\n",
      "2018-04-16T20:47:37.909512: step 539, loss 0.528026, acc 0.79\n",
      "2018-04-16T20:47:38.402442: step 540, loss 0.52515, acc 0.8\n",
      "2018-04-16T20:47:38.895990: step 541, loss 0.588644, acc 0.81\n",
      "2018-04-16T20:47:39.389783: step 542, loss 0.650696, acc 0.71\n",
      "2018-04-16T20:47:39.883521: step 543, loss 0.561331, acc 0.77\n",
      "2018-04-16T20:47:40.378087: step 544, loss 0.641521, acc 0.73\n",
      "2018-04-16T20:47:40.873249: step 545, loss 0.589069, acc 0.74\n",
      "2018-04-16T20:47:41.369062: step 546, loss 0.543651, acc 0.78\n",
      "2018-04-16T20:47:41.864201: step 547, loss 0.611078, acc 0.76\n",
      "2018-04-16T20:47:42.359663: step 548, loss 0.585802, acc 0.74\n",
      "2018-04-16T20:47:42.853747: step 549, loss 0.621394, acc 0.79\n",
      "2018-04-16T20:47:43.348790: step 550, loss 0.67459, acc 0.8\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:47:44.411400: step 550, loss 0.564558, acc 0.767196\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-550\n",
      "\n",
      "2018-04-16T20:47:45.036865: step 551, loss 0.61601, acc 0.72\n",
      "2018-04-16T20:47:45.533130: step 552, loss 0.593811, acc 0.8\n",
      "2018-04-16T20:47:46.029237: step 553, loss 0.624317, acc 0.74\n",
      "2018-04-16T20:47:46.525422: step 554, loss 0.579717, acc 0.74\n",
      "2018-04-16T20:47:47.022211: step 555, loss 0.574005, acc 0.73\n",
      "2018-04-16T20:47:47.518385: step 556, loss 0.548247, acc 0.79\n",
      "2018-04-16T20:47:48.014909: step 557, loss 0.614777, acc 0.74\n",
      "2018-04-16T20:47:48.510443: step 558, loss 0.522165, acc 0.81\n",
      "2018-04-16T20:47:49.006708: step 559, loss 0.498256, acc 0.91\n",
      "2018-04-16T20:47:49.503701: step 560, loss 0.501138, acc 0.78\n",
      "2018-04-16T20:47:50.000187: step 561, loss 0.471623, acc 0.87\n",
      "2018-04-16T20:47:50.495232: step 562, loss 0.526856, acc 0.85\n",
      "2018-04-16T20:47:50.992237: step 563, loss 0.570049, acc 0.84\n",
      "2018-04-16T20:47:51.489934: step 564, loss 0.603879, acc 0.79\n",
      "2018-04-16T20:47:51.986350: step 565, loss 0.613649, acc 0.72\n",
      "2018-04-16T20:47:52.481438: step 566, loss 0.57446, acc 0.74\n",
      "2018-04-16T20:47:52.977895: step 567, loss 0.57236, acc 0.75\n",
      "2018-04-16T20:47:53.473749: step 568, loss 0.546151, acc 0.77\n",
      "2018-04-16T20:47:53.969772: step 569, loss 0.551176, acc 0.81\n",
      "2018-04-16T20:47:54.464988: step 570, loss 0.610733, acc 0.79\n",
      "2018-04-16T20:47:54.961142: step 571, loss 0.577475, acc 0.79\n",
      "2018-04-16T20:47:55.455803: step 572, loss 0.564655, acc 0.8\n",
      "2018-04-16T20:47:55.951292: step 573, loss 0.499759, acc 0.83\n",
      "2018-04-16T20:47:56.445651: step 574, loss 0.54731, acc 0.85\n",
      "2018-04-16T20:47:56.940009: step 575, loss 0.460165, acc 0.82\n",
      "2018-04-16T20:47:57.434231: step 576, loss 0.594034, acc 0.82\n",
      "2018-04-16T20:47:57.930430: step 577, loss 0.563938, acc 0.75\n",
      "2018-04-16T20:47:58.425728: step 578, loss 0.49372, acc 0.78\n",
      "2018-04-16T20:47:58.918861: step 579, loss 0.562391, acc 0.83\n",
      "2018-04-16T20:47:59.414224: step 580, loss 0.543927, acc 0.85\n",
      "2018-04-16T20:47:59.909288: step 581, loss 0.533966, acc 0.81\n",
      "2018-04-16T20:48:00.403552: step 582, loss 0.521441, acc 0.81\n",
      "2018-04-16T20:48:00.897587: step 583, loss 0.61133, acc 0.81\n",
      "2018-04-16T20:48:01.392184: step 584, loss 0.54909, acc 0.8\n",
      "2018-04-16T20:48:01.886806: step 585, loss 0.721365, acc 0.78\n",
      "2018-04-16T20:48:02.382264: step 586, loss 0.524182, acc 0.85\n",
      "2018-04-16T20:48:02.877893: step 587, loss 0.426893, acc 0.84\n",
      "2018-04-16T20:48:03.373318: step 588, loss 0.491483, acc 0.81\n",
      "2018-04-16T20:48:03.867863: step 589, loss 0.529438, acc 0.77\n",
      "2018-04-16T20:48:04.364481: step 590, loss 0.495997, acc 0.81\n",
      "2018-04-16T20:48:04.860371: step 591, loss 0.453131, acc 0.83\n",
      "2018-04-16T20:48:05.356884: step 592, loss 0.535677, acc 0.85\n",
      "2018-04-16T20:48:05.856472: step 593, loss 0.516961, acc 0.77\n",
      "2018-04-16T20:48:06.363382: step 594, loss 0.543424, acc 0.81\n",
      "2018-04-16T20:48:06.859356: step 595, loss 0.398492, acc 0.86\n",
      "2018-04-16T20:48:07.354754: step 596, loss 0.436989, acc 0.88\n",
      "2018-04-16T20:48:07.851171: step 597, loss 0.570393, acc 0.79\n",
      "2018-04-16T20:48:08.347884: step 598, loss 0.492859, acc 0.83\n",
      "2018-04-16T20:48:08.844622: step 599, loss 0.434265, acc 0.88\n",
      "2018-04-16T20:48:09.340193: step 600, loss 0.529732, acc 0.74\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:48:10.401476: step 600, loss 0.438451, acc 0.869489\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-600\n",
      "\n",
      "2018-04-16T20:48:11.020528: step 601, loss 0.462361, acc 0.85\n",
      "2018-04-16T20:48:11.516901: step 602, loss 0.48898, acc 0.86\n",
      "2018-04-16T20:48:12.013411: step 603, loss 0.480647, acc 0.85\n",
      "2018-04-16T20:48:12.508574: step 604, loss 0.477035, acc 0.81\n",
      "2018-04-16T20:48:13.004807: step 605, loss 0.398753, acc 0.88\n",
      "2018-04-16T20:48:13.501998: step 606, loss 0.548362, acc 0.79\n",
      "2018-04-16T20:48:13.998418: step 607, loss 0.508429, acc 0.81\n",
      "2018-04-16T20:48:14.493051: step 608, loss 0.504812, acc 0.85\n",
      "2018-04-16T20:48:14.989424: step 609, loss 0.351388, acc 0.93\n",
      "2018-04-16T20:48:15.485494: step 610, loss 0.439827, acc 0.86\n",
      "2018-04-16T20:48:15.982501: step 611, loss 0.401823, acc 0.87\n",
      "2018-04-16T20:48:16.480609: step 612, loss 0.418144, acc 0.85\n",
      "2018-04-16T20:48:16.978277: step 613, loss 0.331411, acc 0.93\n",
      "2018-04-16T20:48:17.474012: step 614, loss 0.428706, acc 0.84\n",
      "2018-04-16T20:48:17.971162: step 615, loss 0.448392, acc 0.86\n",
      "2018-04-16T20:48:18.466486: step 616, loss 0.534569, acc 0.86\n",
      "2018-04-16T20:48:18.961664: step 617, loss 0.467156, acc 0.84\n",
      "2018-04-16T20:48:19.457140: step 618, loss 0.412819, acc 0.91\n",
      "2018-04-16T20:48:19.951978: step 619, loss 0.49666, acc 0.88\n",
      "2018-04-16T20:48:20.450441: step 620, loss 0.391894, acc 0.91\n",
      "2018-04-16T20:48:20.946087: step 621, loss 0.454032, acc 0.88\n",
      "2018-04-16T20:48:21.442862: step 622, loss 0.449459, acc 0.83\n",
      "2018-04-16T20:48:21.936715: step 623, loss 0.564188, acc 0.87\n",
      "2018-04-16T20:48:22.432446: step 624, loss 0.428289, acc 0.88\n",
      "2018-04-16T20:48:22.927508: step 625, loss 0.436086, acc 0.89\n",
      "2018-04-16T20:48:23.422979: step 626, loss 0.445826, acc 0.86\n",
      "2018-04-16T20:48:23.919332: step 627, loss 0.350705, acc 0.93\n",
      "2018-04-16T20:48:24.414255: step 628, loss 0.332627, acc 0.94\n",
      "2018-04-16T20:48:24.910708: step 629, loss 0.478404, acc 0.84\n",
      "2018-04-16T20:48:25.407277: step 630, loss 0.371582, acc 0.89\n",
      "2018-04-16T20:48:25.905023: step 631, loss 0.389612, acc 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:48:26.402192: step 632, loss 0.306608, acc 0.95\n",
      "2018-04-16T20:48:26.899506: step 633, loss 0.379185, acc 0.87\n",
      "2018-04-16T20:48:27.396951: step 634, loss 0.388155, acc 0.9\n",
      "2018-04-16T20:48:27.891803: step 635, loss 0.372776, acc 0.89\n",
      "2018-04-16T20:48:28.388032: step 636, loss 0.364359, acc 0.85\n",
      "2018-04-16T20:48:28.883977: step 637, loss 0.401916, acc 0.86\n",
      "2018-04-16T20:48:29.381391: step 638, loss 0.382723, acc 0.89\n",
      "2018-04-16T20:48:29.878865: step 639, loss 0.655065, acc 0.78\n",
      "2018-04-16T20:48:30.374679: step 640, loss 0.43935, acc 0.88\n",
      "2018-04-16T20:48:30.869513: step 641, loss 0.458839, acc 0.88\n",
      "2018-04-16T20:48:31.364769: step 642, loss 0.440415, acc 0.87\n",
      "2018-04-16T20:48:31.859697: step 643, loss 0.405816, acc 0.9\n",
      "2018-04-16T20:48:32.355510: step 644, loss 0.527001, acc 0.84\n",
      "2018-04-16T20:48:32.851016: step 645, loss 0.390896, acc 0.92\n",
      "2018-04-16T20:48:33.344086: step 646, loss 0.417749, acc 0.87\n",
      "2018-04-16T20:48:33.838102: step 647, loss 0.39252, acc 0.92\n",
      "2018-04-16T20:48:34.332087: step 648, loss 0.367585, acc 0.9\n",
      "2018-04-16T20:48:34.826662: step 649, loss 0.425744, acc 0.9\n",
      "2018-04-16T20:48:35.320829: step 650, loss 0.319295, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:48:36.378426: step 650, loss 0.396879, acc 0.93298\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-650\n",
      "\n",
      "2018-04-16T20:48:36.998547: step 651, loss 0.402512, acc 0.88\n",
      "2018-04-16T20:48:37.493077: step 652, loss 0.364757, acc 0.93\n",
      "2018-04-16T20:48:37.986904: step 653, loss 0.424259, acc 0.89\n",
      "2018-04-16T20:48:38.481048: step 654, loss 0.410474, acc 0.89\n",
      "2018-04-16T20:48:38.975791: step 655, loss 0.354351, acc 0.9\n",
      "2018-04-16T20:48:39.469989: step 656, loss 0.391064, acc 0.86\n",
      "2018-04-16T20:48:39.965750: step 657, loss 0.372069, acc 0.9\n",
      "2018-04-16T20:48:40.460972: step 658, loss 0.35643, acc 0.91\n",
      "2018-04-16T20:48:40.955538: step 659, loss 0.389139, acc 0.9\n",
      "2018-04-16T20:48:41.450174: step 660, loss 0.524457, acc 0.85\n",
      "2018-04-16T20:48:41.945404: step 661, loss 0.481557, acc 0.85\n",
      "2018-04-16T20:48:42.438735: step 662, loss 0.322672, acc 0.87\n",
      "2018-04-16T20:48:42.933236: step 663, loss 0.29363, acc 0.95\n",
      "2018-04-16T20:48:43.428468: step 664, loss 0.371372, acc 0.89\n",
      "2018-04-16T20:48:43.923077: step 665, loss 0.330817, acc 0.94\n",
      "2018-04-16T20:48:44.416767: step 666, loss 0.273323, acc 0.92\n",
      "2018-04-16T20:48:44.912797: step 667, loss 0.441346, acc 0.86\n",
      "2018-04-16T20:48:45.407792: step 668, loss 0.355854, acc 0.88\n",
      "2018-04-16T20:48:45.903711: step 669, loss 0.332632, acc 0.91\n",
      "2018-04-16T20:48:46.400476: step 670, loss 0.352695, acc 0.88\n",
      "2018-04-16T20:48:46.896573: step 671, loss 0.431874, acc 0.82\n",
      "2018-04-16T20:48:47.391639: step 672, loss 0.322231, acc 0.9\n",
      "2018-04-16T20:48:47.887652: step 673, loss 0.344641, acc 0.88\n",
      "2018-04-16T20:48:48.383110: step 674, loss 0.335624, acc 0.92\n",
      "2018-04-16T20:48:48.880465: step 675, loss 0.271069, acc 0.97\n",
      "2018-04-16T20:48:49.373603: step 676, loss 0.335944, acc 0.92\n",
      "2018-04-16T20:48:49.870309: step 677, loss 0.307581, acc 0.96\n",
      "2018-04-16T20:48:50.365539: step 678, loss 0.442942, acc 0.83\n",
      "2018-04-16T20:48:50.861022: step 679, loss 0.33717, acc 0.9\n",
      "2018-04-16T20:48:51.356125: step 680, loss 0.319165, acc 0.94\n",
      "2018-04-16T20:48:51.850818: step 681, loss 0.274131, acc 0.92\n",
      "2018-04-16T20:48:52.345488: step 682, loss 0.390091, acc 0.89\n",
      "2018-04-16T20:48:52.839673: step 683, loss 0.445377, acc 0.84\n",
      "2018-04-16T20:48:53.335493: step 684, loss 0.256965, acc 0.93\n",
      "2018-04-16T20:48:53.831220: step 685, loss 0.350646, acc 0.9\n",
      "2018-04-16T20:48:54.326251: step 686, loss 0.267682, acc 0.93\n",
      "2018-04-16T20:48:54.820158: step 687, loss 0.471901, acc 0.85\n",
      "2018-04-16T20:48:55.315437: step 688, loss 0.353863, acc 0.91\n",
      "2018-04-16T20:48:55.809883: step 689, loss 0.337193, acc 0.88\n",
      "2018-04-16T20:48:56.305444: step 690, loss 0.392135, acc 0.87\n",
      "2018-04-16T20:48:56.801862: step 691, loss 0.233817, acc 0.93\n",
      "2018-04-16T20:48:57.297262: step 692, loss 0.276471, acc 0.92\n",
      "2018-04-16T20:48:57.791518: step 693, loss 0.396925, acc 0.89\n",
      "2018-04-16T20:48:58.287440: step 694, loss 0.282583, acc 0.93\n",
      "2018-04-16T20:48:58.782164: step 695, loss 0.357637, acc 0.88\n",
      "2018-04-16T20:48:59.276802: step 696, loss 0.404432, acc 0.89\n",
      "2018-04-16T20:48:59.771576: step 697, loss 0.321103, acc 0.91\n",
      "2018-04-16T20:49:00.266715: step 698, loss 0.295457, acc 0.92\n",
      "2018-04-16T20:49:00.761083: step 699, loss 0.208812, acc 0.97\n",
      "2018-04-16T20:49:01.257996: step 700, loss 0.353311, acc 0.88\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:49:02.318983: step 700, loss 0.282549, acc 0.932981\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-700\n",
      "\n",
      "2018-04-16T20:49:02.940659: step 701, loss 0.385371, acc 0.89\n",
      "2018-04-16T20:49:03.436477: step 702, loss 0.26608, acc 0.93\n",
      "2018-04-16T20:49:03.932093: step 703, loss 0.444824, acc 0.84\n",
      "2018-04-16T20:49:04.428109: step 704, loss 0.332797, acc 0.89\n",
      "2018-04-16T20:49:04.925225: step 705, loss 0.277003, acc 0.91\n",
      "2018-04-16T20:49:05.420175: step 706, loss 0.380583, acc 0.89\n",
      "2018-04-16T20:49:05.916102: step 707, loss 0.320052, acc 0.92\n",
      "2018-04-16T20:49:06.411604: step 708, loss 0.281156, acc 0.92\n",
      "2018-04-16T20:49:06.906869: step 709, loss 0.300832, acc 0.92\n",
      "2018-04-16T20:49:07.402364: step 710, loss 0.383366, acc 0.89\n",
      "2018-04-16T20:49:07.897043: step 711, loss 0.307758, acc 0.92\n",
      "2018-04-16T20:49:08.394148: step 712, loss 0.36507, acc 0.89\n",
      "2018-04-16T20:49:08.889334: step 713, loss 0.334561, acc 0.9\n",
      "2018-04-16T20:49:09.385184: step 714, loss 0.508372, acc 0.86\n",
      "2018-04-16T20:49:09.879305: step 715, loss 0.291441, acc 0.94\n",
      "2018-04-16T20:49:10.372771: step 716, loss 0.501808, acc 0.84\n",
      "2018-04-16T20:49:10.867277: step 717, loss 0.364225, acc 0.88\n",
      "2018-04-16T20:49:11.360945: step 718, loss 0.393582, acc 0.87\n",
      "2018-04-16T20:49:11.856995: step 719, loss 0.558666, acc 0.83\n",
      "2018-04-16T20:49:12.353132: step 720, loss 0.291638, acc 0.9\n",
      "2018-04-16T20:49:12.848931: step 721, loss 0.308289, acc 0.93\n",
      "2018-04-16T20:49:13.343149: step 722, loss 0.403322, acc 0.88\n",
      "2018-04-16T20:49:13.838137: step 723, loss 0.36111, acc 0.92\n",
      "2018-04-16T20:49:14.333519: step 724, loss 0.239098, acc 0.95\n",
      "2018-04-16T20:49:14.828550: step 725, loss 0.331341, acc 0.93\n",
      "2018-04-16T20:49:15.322694: step 726, loss 0.330786, acc 0.91\n",
      "2018-04-16T20:49:15.816072: step 727, loss 0.323682, acc 0.9\n",
      "2018-04-16T20:49:16.310604: step 728, loss 0.301263, acc 0.94\n",
      "2018-04-16T20:49:16.804352: step 729, loss 0.305264, acc 0.92\n",
      "2018-04-16T20:49:17.298919: step 730, loss 0.40999, acc 0.88\n",
      "2018-04-16T20:49:17.794593: step 731, loss 0.31685, acc 0.96\n",
      "2018-04-16T20:49:18.288257: step 732, loss 0.259342, acc 0.94\n",
      "2018-04-16T20:49:18.781530: step 733, loss 0.284798, acc 0.95\n",
      "2018-04-16T20:49:19.274404: step 734, loss 0.263664, acc 0.95\n",
      "2018-04-16T20:49:19.768414: step 735, loss 0.267328, acc 0.95\n",
      "2018-04-16T20:49:20.263587: step 736, loss 0.26421, acc 0.92\n",
      "2018-04-16T20:49:20.758850: step 737, loss 0.587939, acc 0.88\n",
      "2018-04-16T20:49:21.255937: step 738, loss 0.51377, acc 0.84\n",
      "2018-04-16T20:49:21.748175: step 739, loss 0.397333, acc 0.93\n",
      "2018-04-16T20:49:22.243693: step 740, loss 0.399229, acc 0.89\n",
      "2018-04-16T20:49:22.737552: step 741, loss 0.30931, acc 0.92\n",
      "2018-04-16T20:49:23.230306: step 742, loss 0.26367, acc 0.94\n",
      "2018-04-16T20:49:23.724350: step 743, loss 0.278105, acc 0.92\n",
      "2018-04-16T20:49:24.217518: step 744, loss 0.310904, acc 0.92\n",
      "2018-04-16T20:49:24.713037: step 745, loss 0.3093, acc 0.91\n",
      "2018-04-16T20:49:25.209014: step 746, loss 0.321076, acc 0.89\n",
      "2018-04-16T20:49:25.703379: step 747, loss 0.283856, acc 0.91\n",
      "2018-04-16T20:49:26.198633: step 748, loss 0.252474, acc 0.94\n",
      "2018-04-16T20:49:26.692904: step 749, loss 0.288772, acc 0.92\n",
      "2018-04-16T20:49:27.187316: step 750, loss 0.327484, acc 0.91\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:49:28.247376: step 750, loss 0.276499, acc 0.938272\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-750\n",
      "\n",
      "2018-04-16T20:49:28.869422: step 751, loss 0.37079, acc 0.91\n",
      "2018-04-16T20:49:29.363911: step 752, loss 0.299568, acc 0.94\n",
      "2018-04-16T20:49:29.857932: step 753, loss 0.221746, acc 0.96\n",
      "2018-04-16T20:49:30.355381: step 754, loss 0.270994, acc 0.92\n",
      "2018-04-16T20:49:30.851638: step 755, loss 0.301565, acc 0.92\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:49:31.346519: step 756, loss 0.242374, acc 0.95\n",
      "2018-04-16T20:49:31.841498: step 757, loss 0.370989, acc 0.91\n",
      "2018-04-16T20:49:32.336519: step 758, loss 0.209367, acc 0.93\n",
      "2018-04-16T20:49:32.830580: step 759, loss 0.331918, acc 0.89\n",
      "2018-04-16T20:49:33.326226: step 760, loss 0.248656, acc 0.94\n",
      "2018-04-16T20:49:33.822571: step 761, loss 0.325892, acc 0.86\n",
      "2018-04-16T20:49:34.318882: step 762, loss 0.269731, acc 0.92\n",
      "2018-04-16T20:49:34.815887: step 763, loss 0.320498, acc 0.9\n",
      "2018-04-16T20:49:35.310235: step 764, loss 0.351942, acc 0.93\n",
      "2018-04-16T20:49:35.805304: step 765, loss 0.33705, acc 0.92\n",
      "2018-04-16T20:49:36.300761: step 766, loss 0.284868, acc 0.93\n",
      "2018-04-16T20:49:36.795530: step 767, loss 0.256533, acc 0.92\n",
      "2018-04-16T20:49:37.291787: step 768, loss 0.357518, acc 0.9\n",
      "2018-04-16T20:49:37.786033: step 769, loss 0.344065, acc 0.92\n",
      "2018-04-16T20:49:38.280958: step 770, loss 0.218964, acc 0.95\n",
      "2018-04-16T20:49:38.775808: step 771, loss 0.279708, acc 0.93\n",
      "2018-04-16T20:49:39.273768: step 772, loss 0.212564, acc 0.94\n",
      "2018-04-16T20:49:39.770491: step 773, loss 0.293617, acc 0.91\n",
      "2018-04-16T20:49:40.268391: step 774, loss 0.223132, acc 0.94\n",
      "2018-04-16T20:49:40.764051: step 775, loss 0.259623, acc 0.94\n",
      "2018-04-16T20:49:41.260583: step 776, loss 0.223641, acc 0.96\n",
      "2018-04-16T20:49:41.757000: step 777, loss 0.266305, acc 0.94\n",
      "2018-04-16T20:49:42.254170: step 778, loss 0.245999, acc 0.93\n",
      "2018-04-16T20:49:42.751024: step 779, loss 0.485415, acc 0.85\n",
      "2018-04-16T20:49:43.246626: step 780, loss 0.208001, acc 0.97\n",
      "2018-04-16T20:49:43.744279: step 781, loss 0.337996, acc 0.92\n",
      "2018-04-16T20:49:44.239917: step 782, loss 0.355003, acc 0.91\n",
      "2018-04-16T20:49:44.737085: step 783, loss 0.22975, acc 0.95\n",
      "2018-04-16T20:49:45.235165: step 784, loss 0.23256, acc 0.92\n",
      "2018-04-16T20:49:45.731753: step 785, loss 0.33407, acc 0.9\n",
      "2018-04-16T20:49:46.229636: step 786, loss 0.266661, acc 0.92\n",
      "2018-04-16T20:49:46.727003: step 787, loss 0.3683, acc 0.87\n",
      "2018-04-16T20:49:47.223381: step 788, loss 0.207222, acc 0.96\n",
      "2018-04-16T20:49:47.721082: step 789, loss 0.259318, acc 0.97\n",
      "2018-04-16T20:49:48.215844: step 790, loss 0.39738, acc 0.86\n",
      "2018-04-16T20:49:48.710010: step 791, loss 0.291993, acc 0.94\n",
      "2018-04-16T20:49:49.207008: step 792, loss 0.454899, acc 0.88\n",
      "2018-04-16T20:49:49.701604: step 793, loss 0.232494, acc 0.94\n",
      "2018-04-16T20:49:50.194416: step 794, loss 0.315037, acc 0.92\n",
      "2018-04-16T20:49:50.687748: step 795, loss 0.331145, acc 0.92\n",
      "2018-04-16T20:49:51.180979: step 796, loss 0.397502, acc 0.91\n",
      "2018-04-16T20:49:51.674717: step 797, loss 0.391965, acc 0.88\n",
      "2018-04-16T20:49:52.167731: step 798, loss 0.362185, acc 0.89\n",
      "2018-04-16T20:49:52.661041: step 799, loss 0.280576, acc 0.91\n",
      "2018-04-16T20:49:53.153591: step 800, loss 0.247955, acc 0.97\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:49:54.240473: step 800, loss 0.244977, acc 0.940035\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-800\n",
      "\n",
      "2018-04-16T20:49:54.858496: step 801, loss 0.265964, acc 0.95\n",
      "2018-04-16T20:49:55.351497: step 802, loss 0.277481, acc 0.93\n",
      "2018-04-16T20:49:55.844686: step 803, loss 0.298409, acc 0.89\n",
      "2018-04-16T20:49:56.338130: step 804, loss 0.254265, acc 0.94\n",
      "2018-04-16T20:49:56.831207: step 805, loss 0.306934, acc 0.91\n",
      "2018-04-16T20:49:57.324310: step 806, loss 0.35417, acc 0.89\n",
      "2018-04-16T20:49:57.817645: step 807, loss 0.281678, acc 0.95\n",
      "2018-04-16T20:49:58.311465: step 808, loss 0.228289, acc 0.94\n",
      "2018-04-16T20:49:58.805559: step 809, loss 0.234423, acc 0.93\n",
      "2018-04-16T20:49:59.302551: step 810, loss 0.332112, acc 0.9\n",
      "2018-04-16T20:49:59.798869: step 811, loss 0.413396, acc 0.87\n",
      "2018-04-16T20:50:00.295461: step 812, loss 0.274143, acc 0.91\n",
      "2018-04-16T20:50:00.792043: step 813, loss 0.338095, acc 0.92\n",
      "2018-04-16T20:50:01.288687: step 814, loss 0.293242, acc 0.9\n",
      "2018-04-16T20:50:01.784559: step 815, loss 0.296301, acc 0.94\n",
      "2018-04-16T20:50:02.280173: step 816, loss 0.186451, acc 0.96\n",
      "2018-04-16T20:50:02.774927: step 817, loss 0.339386, acc 0.89\n",
      "2018-04-16T20:50:03.269668: step 818, loss 0.259759, acc 0.93\n",
      "2018-04-16T20:50:03.764108: step 819, loss 0.212826, acc 0.96\n",
      "2018-04-16T20:50:04.260380: step 820, loss 0.315262, acc 0.91\n",
      "2018-04-16T20:50:04.757919: step 821, loss 0.228357, acc 0.95\n",
      "2018-04-16T20:50:05.251884: step 822, loss 0.240839, acc 0.95\n",
      "2018-04-16T20:50:05.745133: step 823, loss 0.241056, acc 0.92\n",
      "2018-04-16T20:50:06.238064: step 824, loss 0.208293, acc 0.95\n",
      "2018-04-16T20:50:06.732870: step 825, loss 0.366142, acc 0.89\n",
      "2018-04-16T20:50:07.227634: step 826, loss 0.231473, acc 0.96\n",
      "2018-04-16T20:50:07.720613: step 827, loss 0.255506, acc 0.95\n",
      "2018-04-16T20:50:08.217457: step 828, loss 0.275267, acc 0.94\n",
      "2018-04-16T20:50:08.714434: step 829, loss 0.453537, acc 0.86\n",
      "2018-04-16T20:50:09.209249: step 830, loss 0.22781, acc 0.93\n",
      "2018-04-16T20:50:09.705326: step 831, loss 0.20353, acc 0.93\n",
      "2018-04-16T20:50:10.200058: step 832, loss 0.193242, acc 0.95\n",
      "2018-04-16T20:50:10.696062: step 833, loss 0.295346, acc 0.91\n",
      "2018-04-16T20:50:11.189633: step 834, loss 0.266266, acc 0.92\n",
      "2018-04-16T20:50:11.685602: step 835, loss 0.282355, acc 0.94\n",
      "2018-04-16T20:50:12.180952: step 836, loss 0.296451, acc 0.92\n",
      "2018-04-16T20:50:12.675998: step 837, loss 0.359221, acc 0.93\n",
      "2018-04-16T20:50:13.169449: step 838, loss 0.317317, acc 0.89\n",
      "2018-04-16T20:50:13.664660: step 839, loss 0.319906, acc 0.9\n",
      "2018-04-16T20:50:14.158871: step 840, loss 0.348688, acc 0.89\n",
      "2018-04-16T20:50:14.653536: step 841, loss 0.357485, acc 0.9\n",
      "2018-04-16T20:50:15.147659: step 842, loss 0.215801, acc 0.96\n",
      "2018-04-16T20:50:15.642343: step 843, loss 0.202308, acc 0.93\n",
      "2018-04-16T20:50:16.136210: step 844, loss 0.236913, acc 0.95\n",
      "2018-04-16T20:50:16.630051: step 845, loss 0.226206, acc 0.94\n",
      "2018-04-16T20:50:17.125263: step 846, loss 0.179207, acc 0.99\n",
      "2018-04-16T20:50:17.619876: step 847, loss 0.210599, acc 0.96\n",
      "2018-04-16T20:50:18.115531: step 848, loss 0.231435, acc 0.95\n",
      "2018-04-16T20:50:18.609524: step 849, loss 0.35807, acc 0.88\n",
      "2018-04-16T20:50:19.103503: step 850, loss 0.366221, acc 0.89\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:50:20.164287: step 850, loss 0.227908, acc 0.943563\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-850\n",
      "\n",
      "2018-04-16T20:50:20.786854: step 851, loss 0.25474, acc 0.96\n",
      "2018-04-16T20:50:21.282168: step 852, loss 0.175398, acc 0.96\n",
      "2018-04-16T20:50:21.777111: step 853, loss 0.254567, acc 0.93\n",
      "2018-04-16T20:50:22.272717: step 854, loss 0.29924, acc 0.92\n",
      "2018-04-16T20:50:22.767411: step 855, loss 0.299911, acc 0.91\n",
      "2018-04-16T20:50:23.262647: step 856, loss 0.260323, acc 0.93\n",
      "2018-04-16T20:50:23.756920: step 857, loss 0.321342, acc 0.92\n",
      "2018-04-16T20:50:24.251073: step 858, loss 0.210771, acc 0.95\n",
      "2018-04-16T20:50:24.746537: step 859, loss 0.260332, acc 0.94\n",
      "2018-04-16T20:50:25.240585: step 860, loss 0.268964, acc 0.92\n",
      "2018-04-16T20:50:25.733574: step 861, loss 0.395838, acc 0.86\n",
      "2018-04-16T20:50:26.226186: step 862, loss 0.601097, acc 0.8\n",
      "2018-04-16T20:50:26.718449: step 863, loss 0.374909, acc 0.87\n",
      "2018-04-16T20:50:27.211602: step 864, loss 0.308734, acc 0.92\n",
      "2018-04-16T20:50:27.705522: step 865, loss 0.262318, acc 0.91\n",
      "2018-04-16T20:50:28.199754: step 866, loss 0.173135, acc 0.95\n",
      "2018-04-16T20:50:28.694488: step 867, loss 0.295815, acc 0.94\n",
      "2018-04-16T20:50:29.189346: step 868, loss 0.248354, acc 0.94\n",
      "2018-04-16T20:50:29.683579: step 869, loss 0.258119, acc 0.91\n",
      "2018-04-16T20:50:30.178659: step 870, loss 0.278651, acc 0.94\n",
      "2018-04-16T20:50:30.673296: step 871, loss 0.309272, acc 0.92\n",
      "2018-04-16T20:50:31.171463: step 872, loss 0.394304, acc 0.88\n",
      "2018-04-16T20:50:31.668720: step 873, loss 0.208851, acc 0.97\n",
      "2018-04-16T20:50:32.164069: step 874, loss 0.256721, acc 0.94\n",
      "2018-04-16T20:50:32.659101: step 875, loss 0.244943, acc 0.96\n",
      "2018-04-16T20:50:33.155111: step 876, loss 0.190346, acc 0.96\n",
      "2018-04-16T20:50:33.650830: step 877, loss 0.232781, acc 0.94\n",
      "2018-04-16T20:50:34.145808: step 878, loss 0.219779, acc 0.96\n",
      "2018-04-16T20:50:34.642265: step 879, loss 0.356571, acc 0.93\n",
      "2018-04-16T20:50:35.139402: step 880, loss 0.270465, acc 0.9\n",
      "2018-04-16T20:50:35.636439: step 881, loss 0.311947, acc 0.9\n",
      "2018-04-16T20:50:36.131757: step 882, loss 0.358256, acc 0.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:50:36.628833: step 883, loss 0.20786, acc 0.94\n",
      "2018-04-16T20:50:37.123076: step 884, loss 0.372644, acc 0.89\n",
      "2018-04-16T20:50:37.618380: step 885, loss 0.184726, acc 0.96\n",
      "2018-04-16T20:50:38.114535: step 886, loss 0.231675, acc 0.93\n",
      "2018-04-16T20:50:38.610888: step 887, loss 0.191556, acc 0.97\n",
      "2018-04-16T20:50:39.106691: step 888, loss 0.188507, acc 0.96\n",
      "2018-04-16T20:50:39.599745: step 889, loss 0.295368, acc 0.91\n",
      "2018-04-16T20:50:40.095012: step 890, loss 0.281768, acc 0.93\n",
      "2018-04-16T20:50:40.589275: step 891, loss 0.291575, acc 0.9\n",
      "2018-04-16T20:50:41.083042: step 892, loss 0.465324, acc 0.89\n",
      "2018-04-16T20:50:41.577040: step 893, loss 0.341855, acc 0.93\n",
      "2018-04-16T20:50:42.069824: step 894, loss 0.312552, acc 0.9\n",
      "2018-04-16T20:50:42.563264: step 895, loss 0.235998, acc 0.93\n",
      "2018-04-16T20:50:43.055110: step 896, loss 0.286889, acc 0.91\n",
      "2018-04-16T20:50:43.547638: step 897, loss 0.336652, acc 0.89\n",
      "2018-04-16T20:50:44.040494: step 898, loss 0.324018, acc 0.9\n",
      "2018-04-16T20:50:44.534420: step 899, loss 0.235358, acc 0.94\n",
      "2018-04-16T20:50:45.027188: step 900, loss 0.226196, acc 0.95\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:50:46.090763: step 900, loss 0.234721, acc 0.943563\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-900\n",
      "\n",
      "2018-04-16T20:50:46.709167: step 901, loss 0.305742, acc 0.91\n",
      "2018-04-16T20:50:47.201702: step 902, loss 0.211433, acc 0.95\n",
      "2018-04-16T20:50:47.693652: step 903, loss 0.219337, acc 0.98\n",
      "2018-04-16T20:50:48.188107: step 904, loss 0.234054, acc 0.96\n",
      "2018-04-16T20:50:48.679921: step 905, loss 0.215086, acc 0.94\n",
      "2018-04-16T20:50:49.172727: step 906, loss 0.196556, acc 0.93\n",
      "2018-04-16T20:50:49.665680: step 907, loss 0.328705, acc 0.9\n",
      "2018-04-16T20:50:50.162160: step 908, loss 0.283718, acc 0.94\n",
      "2018-04-16T20:50:50.655566: step 909, loss 0.297436, acc 0.92\n",
      "2018-04-16T20:50:51.149955: step 910, loss 0.133758, acc 0.99\n",
      "2018-04-16T20:50:51.643789: step 911, loss 0.361932, acc 0.91\n",
      "2018-04-16T20:50:52.137945: step 912, loss 0.226952, acc 0.94\n",
      "2018-04-16T20:50:52.632730: step 913, loss 0.189878, acc 0.97\n",
      "2018-04-16T20:50:53.126539: step 914, loss 0.224976, acc 0.95\n",
      "2018-04-16T20:50:53.620597: step 915, loss 0.298417, acc 0.88\n",
      "2018-04-16T20:50:54.116440: step 916, loss 0.296387, acc 0.9\n",
      "2018-04-16T20:50:54.610333: step 917, loss 0.277722, acc 0.91\n",
      "2018-04-16T20:50:55.103251: step 918, loss 0.186731, acc 0.96\n",
      "2018-04-16T20:50:55.598882: step 919, loss 0.24107, acc 0.92\n",
      "2018-04-16T20:50:56.092311: step 920, loss 0.185802, acc 0.96\n",
      "2018-04-16T20:50:56.587025: step 921, loss 0.237715, acc 0.92\n",
      "2018-04-16T20:50:57.082083: step 922, loss 0.219281, acc 0.94\n",
      "2018-04-16T20:50:57.575692: step 923, loss 0.271438, acc 0.93\n",
      "2018-04-16T20:50:58.069784: step 924, loss 0.183721, acc 0.95\n",
      "2018-04-16T20:50:58.563289: step 925, loss 0.231782, acc 0.95\n",
      "2018-04-16T20:50:59.056694: step 926, loss 0.199453, acc 0.98\n",
      "2018-04-16T20:50:59.550540: step 927, loss 0.265966, acc 0.95\n",
      "2018-04-16T20:51:00.043269: step 928, loss 0.211766, acc 0.96\n",
      "2018-04-16T20:51:00.535862: step 929, loss 0.234457, acc 0.95\n",
      "2018-04-16T20:51:01.029926: step 930, loss 0.287343, acc 0.93\n",
      "2018-04-16T20:51:01.524152: step 931, loss 0.306259, acc 0.9\n",
      "2018-04-16T20:51:02.018479: step 932, loss 0.316416, acc 0.92\n",
      "2018-04-16T20:51:02.513690: step 933, loss 0.293764, acc 0.88\n",
      "2018-04-16T20:51:03.010090: step 934, loss 0.211092, acc 0.96\n",
      "2018-04-16T20:51:03.506871: step 935, loss 0.351528, acc 0.87\n",
      "2018-04-16T20:51:04.003599: step 936, loss 0.225061, acc 0.98\n",
      "2018-04-16T20:51:04.495142: step 937, loss 0.348965, acc 0.9\n",
      "2018-04-16T20:51:04.988788: step 938, loss 0.210497, acc 0.95\n",
      "2018-04-16T20:51:05.480168: step 939, loss 0.294013, acc 0.94\n",
      "2018-04-16T20:51:05.971899: step 940, loss 0.298736, acc 0.92\n",
      "2018-04-16T20:51:06.462201: step 941, loss 0.31404, acc 0.92\n",
      "2018-04-16T20:51:06.952782: step 942, loss 0.260713, acc 0.94\n",
      "2018-04-16T20:51:07.444036: step 943, loss 0.196799, acc 0.97\n",
      "2018-04-16T20:51:07.935681: step 944, loss 0.371663, acc 0.89\n",
      "2018-04-16T20:51:08.427679: step 945, loss 0.211666, acc 0.95\n",
      "2018-04-16T20:51:08.917349: step 946, loss 0.237836, acc 0.94\n",
      "2018-04-16T20:51:09.407519: step 947, loss 0.38241, acc 0.91\n",
      "2018-04-16T20:51:09.898053: step 948, loss 0.217829, acc 0.95\n",
      "2018-04-16T20:51:10.388667: step 949, loss 0.241541, acc 0.95\n",
      "2018-04-16T20:51:10.880075: step 950, loss 0.267768, acc 0.93\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:51:11.940407: step 950, loss 0.227793, acc 0.940035\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-950\n",
      "\n",
      "2018-04-16T20:51:12.561058: step 951, loss 0.256116, acc 0.93\n",
      "2018-04-16T20:51:13.051595: step 952, loss 0.3171, acc 0.93\n",
      "2018-04-16T20:51:13.543086: step 953, loss 0.24546, acc 0.93\n",
      "2018-04-16T20:51:14.035408: step 954, loss 0.426354, acc 0.9\n",
      "2018-04-16T20:51:14.528037: step 955, loss 0.219093, acc 0.93\n",
      "2018-04-16T20:51:15.019261: step 956, loss 0.280665, acc 0.93\n",
      "2018-04-16T20:51:15.511636: step 957, loss 0.27029, acc 0.94\n",
      "2018-04-16T20:51:16.003881: step 958, loss 0.23821, acc 0.94\n",
      "2018-04-16T20:51:16.496518: step 959, loss 0.183002, acc 0.96\n",
      "2018-04-16T20:51:16.987573: step 960, loss 0.316862, acc 0.92\n",
      "2018-04-16T20:51:17.480118: step 961, loss 0.163762, acc 0.96\n",
      "2018-04-16T20:51:17.973525: step 962, loss 0.229829, acc 0.92\n",
      "2018-04-16T20:51:18.466853: step 963, loss 0.368316, acc 0.87\n",
      "2018-04-16T20:51:18.960557: step 964, loss 0.29056, acc 0.92\n",
      "2018-04-16T20:51:19.452767: step 965, loss 0.21481, acc 0.97\n",
      "2018-04-16T20:51:19.945324: step 966, loss 0.160811, acc 0.97\n",
      "2018-04-16T20:51:20.437656: step 967, loss 0.299608, acc 0.9\n",
      "2018-04-16T20:51:20.928281: step 968, loss 0.16634, acc 0.95\n",
      "2018-04-16T20:51:21.419931: step 969, loss 0.206379, acc 0.97\n",
      "2018-04-16T20:51:21.912355: step 970, loss 0.227098, acc 0.91\n",
      "2018-04-16T20:51:22.403764: step 971, loss 0.241719, acc 0.95\n",
      "2018-04-16T20:51:22.894755: step 972, loss 0.236754, acc 0.93\n",
      "2018-04-16T20:51:23.387753: step 973, loss 0.21978, acc 0.95\n",
      "2018-04-16T20:51:23.880008: step 974, loss 0.164553, acc 0.96\n",
      "2018-04-16T20:51:24.370972: step 975, loss 0.321784, acc 0.9\n",
      "2018-04-16T20:51:24.864011: step 976, loss 0.199813, acc 0.95\n",
      "2018-04-16T20:51:25.356977: step 977, loss 0.3255, acc 0.9\n",
      "2018-04-16T20:51:25.849385: step 978, loss 0.210503, acc 0.96\n",
      "2018-04-16T20:51:26.342349: step 979, loss 0.23543, acc 0.94\n",
      "2018-04-16T20:51:26.835883: step 980, loss 0.177573, acc 0.96\n",
      "2018-04-16T20:51:27.327071: step 981, loss 0.285336, acc 0.93\n",
      "2018-04-16T20:51:27.818590: step 982, loss 0.235357, acc 0.96\n",
      "2018-04-16T20:51:28.310061: step 983, loss 0.262729, acc 0.94\n",
      "2018-04-16T20:51:28.802146: step 984, loss 0.23921, acc 0.95\n",
      "2018-04-16T20:51:29.294918: step 985, loss 0.251549, acc 0.92\n",
      "2018-04-16T20:51:29.786751: step 986, loss 0.3155, acc 0.91\n",
      "2018-04-16T20:51:30.278979: step 987, loss 0.257103, acc 0.94\n",
      "2018-04-16T20:51:30.771063: step 988, loss 0.123768, acc 0.99\n",
      "2018-04-16T20:51:31.264284: step 989, loss 0.196942, acc 0.94\n",
      "2018-04-16T20:51:31.755777: step 990, loss 0.260955, acc 0.89\n",
      "2018-04-16T20:51:32.247771: step 991, loss 0.183885, acc 0.94\n",
      "2018-04-16T20:51:32.739374: step 992, loss 0.225609, acc 0.93\n",
      "2018-04-16T20:51:33.231443: step 993, loss 0.314173, acc 0.92\n",
      "2018-04-16T20:51:33.724914: step 994, loss 0.186096, acc 0.96\n",
      "2018-04-16T20:51:34.217195: step 995, loss 0.229178, acc 0.96\n",
      "2018-04-16T20:51:34.709952: step 996, loss 0.228299, acc 0.95\n",
      "2018-04-16T20:51:35.202898: step 997, loss 0.303446, acc 0.93\n",
      "2018-04-16T20:51:35.697843: step 998, loss 0.12969, acc 0.98\n",
      "2018-04-16T20:51:36.191321: step 999, loss 0.180472, acc 0.94\n",
      "2018-04-16T20:51:36.683123: step 1000, loss 0.126508, acc 0.99\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:51:37.749411: step 1000, loss 0.1988, acc 0.938272\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-1000\n",
      "\n",
      "2018-04-16T20:51:38.368427: step 1001, loss 0.340577, acc 0.91\n",
      "2018-04-16T20:51:38.860586: step 1002, loss 0.136511, acc 0.99\n",
      "2018-04-16T20:51:39.353423: step 1003, loss 0.29224, acc 0.91\n",
      "2018-04-16T20:51:39.846776: step 1004, loss 0.167125, acc 0.96\n",
      "2018-04-16T20:51:40.340508: step 1005, loss 0.248273, acc 0.89\n",
      "2018-04-16T20:51:40.833216: step 1006, loss 0.228386, acc 0.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2018-04-16T20:51:41.327691: step 1007, loss 0.225469, acc 0.93\n",
      "2018-04-16T20:51:41.820842: step 1008, loss 0.181918, acc 0.96\n",
      "2018-04-16T20:51:42.313930: step 1009, loss 0.552669, acc 0.88\n",
      "2018-04-16T20:51:42.808580: step 1010, loss 0.283248, acc 0.92\n",
      "2018-04-16T20:51:43.301203: step 1011, loss 0.155572, acc 0.98\n",
      "2018-04-16T20:51:43.793433: step 1012, loss 0.279133, acc 0.92\n",
      "2018-04-16T20:51:44.286249: step 1013, loss 0.300521, acc 0.91\n",
      "2018-04-16T20:51:44.779676: step 1014, loss 0.216778, acc 0.94\n",
      "2018-04-16T20:51:45.271644: step 1015, loss 0.267163, acc 0.94\n",
      "2018-04-16T20:51:45.763962: step 1016, loss 0.169658, acc 0.96\n",
      "2018-04-16T20:51:46.255966: step 1017, loss 0.234004, acc 0.95\n",
      "2018-04-16T20:51:46.746834: step 1018, loss 0.186452, acc 0.97\n",
      "2018-04-16T20:51:47.238125: step 1019, loss 0.273036, acc 0.95\n",
      "2018-04-16T20:51:47.729753: step 1020, loss 0.293845, acc 0.9\n",
      "2018-04-16T20:51:48.219291: step 1021, loss 0.231793, acc 0.95\n",
      "2018-04-16T20:51:48.710420: step 1022, loss 0.256665, acc 0.94\n",
      "2018-04-16T20:51:49.200296: step 1023, loss 0.360574, acc 0.91\n",
      "2018-04-16T20:51:49.690597: step 1024, loss 0.371993, acc 0.91\n",
      "2018-04-16T20:51:50.181983: step 1025, loss 0.177233, acc 0.96\n",
      "2018-04-16T20:51:50.674068: step 1026, loss 0.337876, acc 0.89\n",
      "2018-04-16T20:51:51.166246: step 1027, loss 0.208927, acc 0.94\n",
      "2018-04-16T20:51:51.658510: step 1028, loss 0.248309, acc 0.95\n",
      "2018-04-16T20:51:52.150392: step 1029, loss 0.142977, acc 0.97\n",
      "2018-04-16T20:51:52.640920: step 1030, loss 0.276995, acc 0.95\n",
      "2018-04-16T20:51:53.131449: step 1031, loss 0.281149, acc 0.92\n",
      "2018-04-16T20:51:53.623125: step 1032, loss 0.227744, acc 0.94\n",
      "2018-04-16T20:51:54.115416: step 1033, loss 0.19623, acc 0.96\n",
      "2018-04-16T20:51:54.605381: step 1034, loss 0.206244, acc 0.94\n",
      "2018-04-16T20:51:55.096348: step 1035, loss 0.144129, acc 0.96\n",
      "2018-04-16T20:51:55.587408: step 1036, loss 0.154926, acc 0.98\n",
      "2018-04-16T20:51:56.078488: step 1037, loss 0.202161, acc 0.94\n",
      "2018-04-16T20:51:56.570289: step 1038, loss 0.178035, acc 0.95\n",
      "2018-04-16T20:51:57.062159: step 1039, loss 0.220255, acc 0.96\n",
      "2018-04-16T20:51:57.552925: step 1040, loss 0.264031, acc 0.95\n",
      "2018-04-16T20:51:58.044263: step 1041, loss 0.247744, acc 0.92\n",
      "2018-04-16T20:51:58.535241: step 1042, loss 0.30208, acc 0.92\n",
      "2018-04-16T20:51:59.027092: step 1043, loss 0.209237, acc 0.94\n",
      "2018-04-16T20:51:59.518155: step 1044, loss 0.269069, acc 0.91\n",
      "2018-04-16T20:52:00.010126: step 1045, loss 0.245696, acc 0.93\n",
      "2018-04-16T20:52:00.500019: step 1046, loss 0.288587, acc 0.9\n",
      "2018-04-16T20:52:00.991322: step 1047, loss 0.248622, acc 0.93\n",
      "2018-04-16T20:52:01.483827: step 1048, loss 0.22453, acc 0.94\n",
      "2018-04-16T20:52:01.975539: step 1049, loss 0.405019, acc 0.91\n",
      "2018-04-16T20:52:02.466725: step 1050, loss 0.222076, acc 0.94\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:52:03.546749: step 1050, loss 0.197932, acc 0.941799\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-1050\n",
      "\n",
      "2018-04-16T20:52:04.164307: step 1051, loss 0.254615, acc 0.94\n",
      "2018-04-16T20:52:04.657109: step 1052, loss 0.185412, acc 0.96\n",
      "2018-04-16T20:52:05.148446: step 1053, loss 0.263971, acc 0.93\n",
      "2018-04-16T20:52:05.640443: step 1054, loss 0.241943, acc 0.92\n",
      "2018-04-16T20:52:06.132933: step 1055, loss 0.243221, acc 0.94\n",
      "2018-04-16T20:52:06.624316: step 1056, loss 0.299204, acc 0.92\n",
      "2018-04-16T20:52:07.115071: step 1057, loss 0.149254, acc 0.97\n",
      "2018-04-16T20:52:07.607244: step 1058, loss 0.268409, acc 0.93\n",
      "2018-04-16T20:52:08.098149: step 1059, loss 0.244439, acc 0.94\n",
      "2018-04-16T20:52:08.589693: step 1060, loss 0.153989, acc 0.98\n",
      "2018-04-16T20:52:09.079448: step 1061, loss 0.304804, acc 0.88\n",
      "2018-04-16T20:52:09.571299: step 1062, loss 0.265443, acc 0.93\n",
      "2018-04-16T20:52:10.062381: step 1063, loss 0.312107, acc 0.95\n",
      "2018-04-16T20:52:10.554928: step 1064, loss 0.229108, acc 0.97\n",
      "2018-04-16T20:52:11.045471: step 1065, loss 0.242916, acc 0.96\n",
      "2018-04-16T20:52:11.537036: step 1066, loss 0.236621, acc 0.95\n",
      "2018-04-16T20:52:12.027473: step 1067, loss 0.20021, acc 0.94\n",
      "2018-04-16T20:52:12.518762: step 1068, loss 0.190722, acc 0.96\n",
      "2018-04-16T20:52:13.009302: step 1069, loss 0.200677, acc 0.95\n",
      "2018-04-16T20:52:13.499588: step 1070, loss 0.172782, acc 0.98\n",
      "2018-04-16T20:52:13.990260: step 1071, loss 0.209118, acc 0.93\n",
      "2018-04-16T20:52:14.478805: step 1072, loss 0.18096, acc 0.95\n",
      "2018-04-16T20:52:14.968691: step 1073, loss 0.295914, acc 0.92\n",
      "2018-04-16T20:52:15.458651: step 1074, loss 0.200139, acc 0.95\n",
      "2018-04-16T20:52:15.948923: step 1075, loss 0.315206, acc 0.91\n",
      "2018-04-16T20:52:16.440484: step 1076, loss 0.180082, acc 0.94\n",
      "2018-04-16T20:52:16.931216: step 1077, loss 0.170613, acc 0.95\n",
      "2018-04-16T20:52:17.422053: step 1078, loss 0.225471, acc 0.95\n",
      "2018-04-16T20:52:17.912490: step 1079, loss 0.237456, acc 0.92\n",
      "2018-04-16T20:52:18.401096: step 1080, loss 0.205973, acc 0.98\n",
      "2018-04-16T20:52:18.893066: step 1081, loss 0.238529, acc 0.95\n",
      "2018-04-16T20:52:19.382925: step 1082, loss 0.237368, acc 0.91\n",
      "2018-04-16T20:52:19.873908: step 1083, loss 0.238712, acc 0.95\n",
      "2018-04-16T20:52:20.365402: step 1084, loss 0.180732, acc 0.97\n",
      "2018-04-16T20:52:20.856441: step 1085, loss 0.138821, acc 0.96\n",
      "2018-04-16T20:52:21.347519: step 1086, loss 0.201662, acc 0.94\n",
      "2018-04-16T20:52:21.836784: step 1087, loss 0.267621, acc 0.95\n",
      "2018-04-16T20:52:22.329048: step 1088, loss 0.275798, acc 0.93\n",
      "2018-04-16T20:52:22.821203: step 1089, loss 0.129602, acc 0.98\n",
      "2018-04-16T20:52:23.313203: step 1090, loss 0.126382, acc 0.98\n",
      "2018-04-16T20:52:23.804711: step 1091, loss 0.250025, acc 0.95\n",
      "2018-04-16T20:52:24.296393: step 1092, loss 0.374944, acc 0.89\n",
      "2018-04-16T20:52:24.790285: step 1093, loss 0.252436, acc 0.95\n",
      "2018-04-16T20:52:25.283652: step 1094, loss 0.305915, acc 0.92\n",
      "2018-04-16T20:52:25.776344: step 1095, loss 0.164499, acc 0.97\n",
      "2018-04-16T20:52:26.267238: step 1096, loss 0.265464, acc 0.93\n",
      "2018-04-16T20:52:26.756688: step 1097, loss 0.190496, acc 0.96\n",
      "2018-04-16T20:52:27.248005: step 1098, loss 0.18424, acc 0.95\n",
      "2018-04-16T20:52:27.739839: step 1099, loss 0.177033, acc 0.96\n",
      "2018-04-16T20:52:28.231718: step 1100, loss 0.299413, acc 0.92\n",
      "\n",
      "Evaluation:\n",
      "2018-04-16T20:52:29.289330: step 1100, loss 0.225505, acc 0.940035\n",
      "\n",
      "Saved model checkpoint to /home/ramya_girish/political_bias/political_bias/runs/1523911375/checkpoints/model-1100\n",
      "\n",
      "2018-04-16T20:52:29.908871: step 1101, loss 0.28698, acc 0.91\n",
      "2018-04-16T20:52:30.399750: step 1102, loss 0.244117, acc 0.93\n",
      "2018-04-16T20:52:30.890762: step 1103, loss 0.345679, acc 0.88\n",
      "2018-04-16T20:52:31.382592: step 1104, loss 0.27025, acc 0.93\n",
      "2018-04-16T20:52:31.873736: step 1105, loss 0.138775, acc 0.98\n",
      "2018-04-16T20:52:32.364309: step 1106, loss 0.3549, acc 0.88\n",
      "2018-04-16T20:52:32.854853: step 1107, loss 0.241003, acc 0.95\n",
      "2018-04-16T20:52:33.345900: step 1108, loss 0.187999, acc 0.95\n",
      "2018-04-16T20:52:33.837871: step 1109, loss 0.260268, acc 0.9\n",
      "2018-04-16T20:52:34.328180: step 1110, loss 0.244474, acc 0.96\n",
      "2018-04-16T20:52:34.819273: step 1111, loss 0.303656, acc 0.9\n",
      "2018-04-16T20:52:35.310310: step 1112, loss 0.185088, acc 0.95\n",
      "2018-04-16T20:52:35.801920: step 1113, loss 0.199751, acc 0.95\n",
      "2018-04-16T20:52:36.292600: step 1114, loss 0.188616, acc 0.96\n",
      "2018-04-16T20:52:36.784027: step 1115, loss 0.297446, acc 0.93\n",
      "2018-04-16T20:52:37.275204: step 1116, loss 0.225551, acc 0.94\n",
      "2018-04-16T20:52:37.766778: step 1117, loss 0.309757, acc 0.93\n",
      "2018-04-16T20:52:38.257693: step 1118, loss 0.248015, acc 0.93\n",
      "2018-04-16T20:52:38.749151: step 1119, loss 0.367904, acc 0.9\n",
      "2018-04-16T20:52:39.239743: step 1120, loss 0.23354, acc 0.96\n",
      "2018-04-16T20:52:39.730862: step 1121, loss 0.2439, acc 0.92\n",
      "2018-04-16T20:52:40.222097: step 1122, loss 0.279932, acc 0.93\n",
      "2018-04-16T20:52:40.715291: step 1123, loss 0.19569, acc 0.94\n",
      "2018-04-16T20:52:41.207887: step 1124, loss 0.218634, acc 0.92\n",
      "2018-04-16T20:52:41.700264: step 1125, loss 0.237494, acc 0.93\n",
      "2018-04-16T20:52:42.192231: step 1126, loss 0.293433, acc 0.94\n",
      "2018-04-16T20:52:42.685038: step 1127, loss 0.138732, acc 0.97\n",
      "2018-04-16T20:52:43.012347: step 1128, loss 0.176524, acc 0.9375\n",
      "\n",
      "Test set accuracy:\n",
      "2018-04-16T20:52:44.899367: step 1128, loss 0.627292, acc 0.693333\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epoch = 1\n",
    "num_checkpoints = 5\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=feature_size, num_classes=3, vocab_size=V,\n",
    "        embedding_size=100, filter_heights=[5,5,5], filter_widths=[5,5,5],num_filters=3,channels_in=[1,3,3]\n",
    "              ,channels_out=[3,3,3]\n",
    "              ,init_scale=0.08, l2_reg_lambda=0.1)\n",
    "\n",
    "        \n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.summary.scalar(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.summary.scalar(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.summary.FileWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.summary.merge([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.summary.FileWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.global_variables(), max_to_keep=num_checkpoints)\n",
    "        \n",
    "        # Initialize all variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        def train_step(x_batch, y_batch):\n",
    "            \"\"\"\n",
    "            A single training step\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 0.5\n",
    "            }\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "            \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            \"\"\"\n",
    "            Evaluates model on a dev set\n",
    "            \"\"\"\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "            \n",
    "        from sklearn.model_selection import train_test_split \n",
    "\n",
    "        \n",
    "        U_train,U_dev,y_train, y_dev = train_test_split(U_sent, pred_label_U[['label_0','label_1','label_2']], \n",
    "                                                        test_size=0.005, random_state=42)\n",
    "        \n",
    "        for j in range(num_epoch):\n",
    "        \n",
    "            t0 = time.time()\n",
    "            total_batches = 0\n",
    "            total_examples = 0\n",
    "        \n",
    "            for (bx, by) in utils.multi_batch_generator(batch_size, U_train, y_train):\n",
    "                train_step(bx,by)\n",
    "                current_step = tf.train.global_step(sess, global_step)\n",
    "                if current_step % 50 == 0:\n",
    "                    print(\"\\nEvaluation:\")\n",
    "                    dev_step(U_dev, y_dev, writer=dev_summary_writer)\n",
    "                    print(\"\")\n",
    "                if current_step % 50 == 0:\n",
    "                    path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                    print(\"Saved model checkpoint to {}\\n\".format(path))\n",
    "                    \n",
    "        print(\"\\nTest set accuracy:\")\n",
    "        dev_step(X_sent_test[0:600], Y_label.iloc[0:600][['label_0','label_1','label_2']], writer=dev_summary_writer)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
