{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test basis /Users/ramyabalasubramaniam/Desktop/nyt_corpus/data/1987/01/01/01\n",
    "\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1987\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1987/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1987/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1987\n",
    "    file_name = \"nyt_1987_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':p})\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "frame = pd.DataFrame()\n",
    "list_ = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_.append(df)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = pd.concat(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubName</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Newsbody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>AAR CORP reports earnings for Qtr to Nov 30</td>\n",
       "      <td>*3*** COMPANY REPORTS ** *3*AAR CORP (NYSE) Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>AMERICAN CYTOGENETICS reports earnings for Qt...</td>\n",
       "      <td>*3*** COMPANY REPORTS ** *3*AMERICAN CYTOGENE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>APPLIED POWER reports earnings for Qtr to Nov 30</td>\n",
       "      <td>*3*** COMPANY REPORTS ** *3*APPLIED POWER (OT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>BARRETT RESOURCES reports earnings for Year t...</td>\n",
       "      <td>*3*** COMPANY REPORTS ** *3*BARRETT RESOURCES...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>CANAVERAL INTERNATIONAL reports earnings for ...</td>\n",
       "      <td>*3*** COMPANY REPORTS ** *3*CANAVERAL INTERNA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              PubName                                           Headline  \\\n",
       "0  The New York Times        AAR CORP reports earnings for Qtr to Nov 30   \n",
       "1  The New York Times   AMERICAN CYTOGENETICS reports earnings for Qt...   \n",
       "2  The New York Times   APPLIED POWER reports earnings for Qtr to Nov 30   \n",
       "3  The New York Times   BARRETT RESOURCES reports earnings for Year t...   \n",
       "4  The New York Times   CANAVERAL INTERNATIONAL reports earnings for ...   \n",
       "\n",
       "                                            Newsbody  \n",
       "0   *3*** COMPANY REPORTS ** *3*AAR CORP (NYSE) Q...  \n",
       "1   *3*** COMPANY REPORTS ** *3*AMERICAN CYTOGENE...  \n",
       "2   *3*** COMPANY REPORTS ** *3*APPLIED POWER (OT...  \n",
       "3   *3*** COMPANY REPORTS ** *3*BARRETT RESOURCES...  \n",
       "4   *3*** COMPANY REPORTS ** *3*CANAVERAL INTERNA...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1988\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1988/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1988/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1988\n",
    "    file_name = \"nyt_1988_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':str(p)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") \n",
    "\n",
    "frame_1988 = pd.DataFrame()\n",
    "list_1988 = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_1988.append(df)\n",
    "    \n",
    "frame_1988 = pd.concat(list_1988)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubName</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Newsbody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>104541</td>\n",
       "      <td>104405</td>\n",
       "      <td>104159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>1</td>\n",
       "      <td>94034</td>\n",
       "      <td>103391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Corrections</td>\n",
       "      <td>Replies for publication should be no more tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>104541</td>\n",
       "      <td>908</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   PubName      Headline  \\\n",
       "count               104541        104405   \n",
       "unique                   1         94034   \n",
       "top     The New York Times   Corrections   \n",
       "freq                104541           908   \n",
       "\n",
       "                                                 Newsbody  \n",
       "count                                              104159  \n",
       "unique                                             103391  \n",
       "top      Replies for publication should be no more tha...  \n",
       "freq                                                   46  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_1988.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_words = [\"republican \",\"democrat \",\"Pro-Choice\",\"bill \"\n",
    "              , \"congress \", \"abortion \", \"Pro-life\",\"Senate\",\"GOP\", \"caucus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame['Newsbody'])):\n",
    "    if not(frame.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "        if any(word in frame.iloc[i]['Newsbody'] for word in bias_words):\n",
    "            list_bias.append(frame.iloc[i])\n",
    "\n",
    "frame_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Government securities dealers spent yesterday's abbreviated session marking down prices of long-term Treasury bonds and raising those on intermediate maturities. As a partial explanation of yesterday's activity, dealers said they were expecting more institutional and other bond investors to shorten the maturities in their portfolios when they start to return to the market next week. The market was also unsettled by bearish economic news: The Government reported a record $19.2 billion trade deficit, and the dollar fell to a new six-year low against the West German mark. Sentiment was not helped by the continued rise in oil prices and the sharp $15-an-ounce surge in gold, which sent bullion prices above the $400 mark once again. Whether or not these factors signal inflation, the bane of bond holders, or are merely short-term phenomena, the bond market took the prudent course of adjusting positions yesterday. As a result, the offered price of the bellwether long Treasury bond, the 7.5 percent issue due in 2016, closed down 17/32, at 100 7/32, for a yield of 7.48 percent, compared with 7.44 percent the previous day. Also at 2 P.M., when trading in government securities ended yesterday, the price of long Treasury notes, such as the 7 1/4 percent issue of 1996, was down 8/32, or a quarter-point, at 100 5/32, and now yields 7.23 percent, compared with 7.20 percent the day before. But the shortening of maturities helped raise the price of the 6 1/4 percent issue of 1988 by 3/32, to 99 28/32, for a yield of 6.32 percent. The 6 5/8 percent issue of 1990 closed unchanged at 99 21/32, a yield of 6.72 percent, and the 7 percent notes of January 1994, which the Treasury auctioned Tuesday at an average yield of 7.09 percent, closed unchanged yesterday at an offered price of 99 18/32. As expected, the end of the quarter and the year caused a last-minute flurry of Treasury bill purchases by corporations, mutual funds and other institutions seeking to show large cash positions on their final accounts for 1986. This traditional ''window-dressing'' helped shave three basis points off the discount rate of 90-day Treasury bills, which closed at 5.68 percent; the six-month bill rate fell four basis points, to 5.64 percent, and the one-year bills closed at 5.63, down nine basis points. Also expected was the abnormally high Federal funds, or overnight interbank lending, rate, which opened at 30 percent and broke sharply to 9 percent just before the 2 P.M. close. Aside from the usual heavy demand for window-dressing and other funds on Dec. 31, yesterday was the end of the two-week bank reserve settlement period, when many banks scramble to meet various reserve requirements. Yesterday was also the settlement day for the $18 billion of notes auctioned by the Treasury last week. Dealers were surprised by the timing of the Fed's intervention in the market, which occurred at 10 A.M, or more than an hour earlier than normal. In any case, the Fed's direct injection of reserves into the banking system, effected by two-day system repurchase agreements, came when the funds rate was 20 percent. The credit market is closed today. Tomorrow, the credit market, which normally has no fixed closing hour, will again end at 2 P.M. CREDIT MARKETS\""
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_bias.iloc[1]['Newsbody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame_1988['Newsbody'])):\n",
    "    if isinstance(frame_1988.iloc[i]['Newsbody'],str):\n",
    "        if not(frame_1988.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "            if any(word in frame_1988.iloc[i]['Newsbody'] for word in bias_words):\n",
    "                list_bias.append(frame_1988.iloc[i])\n",
    "\n",
    "frame_1988_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PubName     7389\n",
       "Headline    7374\n",
       "Newsbody    7389\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_1988_bias.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Former Secretary of State Alexander M. Haig Jr. was released from a hospital early today after an overnight stay for treatment of an allergic reaction to drugs he had taken for a tooth problem. ''They gave him a clean bill of health,'' said a spokesman for Fairfax Hospital, Lon Walls. Mr. Haig, a candidate for the Republican Presidential nomination, had been treated Wednesday at Walter Reed Army Medical Center for an abscessed tooth. He had been given penicillin and Tylenol.\""
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_1988_bias.iloc[1]['Newsbody']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1989\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1989/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1989/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1989\n",
    "    file_name = \"nyt_1989_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':str(p)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") \n",
    "\n",
    "frame_1989 = pd.DataFrame()\n",
    "list_1989 = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_1989.append(df)\n",
    "    \n",
    "frame_1989 = pd.concat(list_1989)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame_1989['Newsbody'])):\n",
    "    if isinstance(frame_1989.iloc[i]['Newsbody'],str):\n",
    "        if not(frame_1989.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "            if any(word in frame_1989.iloc[i]['Newsbody'] for word in bias_words):\n",
    "                list_bias.append(frame_1989.iloc[i])\n",
    "\n",
    "frame_1989_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PubName</th>\n",
       "      <th>Headline</th>\n",
       "      <th>Newsbody</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>2d Investigation Touches Capital Mayor</td>\n",
       "      <td>Mayor Marion S. Barry Jr., already under inve...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Beef Dispute: Stakes High In Trade War</td>\n",
       "      <td>As foreign trade goes, the loss of $130 milli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>U.N. Council Unable To Adopt Guidelines For a...</td>\n",
       "      <td>The Security Council today failed to agree on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>U.S. Bank Board Adds Nine More Troubled Savin...</td>\n",
       "      <td>Closing out the year with a rapid-fire series...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>The New York Times</td>\n",
       "      <td>Albany Backs Camera Use To Catch Traffic Viol...</td>\n",
       "      <td>New York City has received approval to start ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               PubName                                           Headline  \\\n",
       "0   The New York Times             2d Investigation Touches Capital Mayor   \n",
       "32  The New York Times             Beef Dispute: Stakes High In Trade War   \n",
       "34  The New York Times   U.N. Council Unable To Adopt Guidelines For a...   \n",
       "36  The New York Times   U.S. Bank Board Adds Nine More Troubled Savin...   \n",
       "44  The New York Times   Albany Backs Camera Use To Catch Traffic Viol...   \n",
       "\n",
       "                                             Newsbody  \n",
       "0    Mayor Marion S. Barry Jr., already under inve...  \n",
       "32   As foreign trade goes, the loss of $130 milli...  \n",
       "34   The Security Council today failed to agree on...  \n",
       "36   Closing out the year with a rapid-fire series...  \n",
       "44   New York City has received approval to start ...  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_1989_bias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sentences = []\n",
    "\n",
    "for i in range(len(frame_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In the last 365 days, we've had problems: criticism of our new mural over the grand staircase, ''The Mets' Gary Carter Traps a Pop Fly at the Visitors' Dugout''; the $5,000 Windex bill for the Hall of Mirrors, and landscaping cost overruns for the 400 acres of Astroturf.\",\n",
       " 'As expected, the end of the quarter and the year caused a last-minute flurry of Treasury bill purchases by corporations, mutual funds and other institutions seeking to show large cash positions on their final accounts for 1986.',\n",
       " \"This traditional ''window-dressing'' helped shave three basis points off the discount rate of 90-day Treasury bills, which closed at 5.68 percent; the six-month bill rate fell four basis points, to 5.64 percent, and the one-year bills closed at 5.63, down nine basis points.\",\n",
       " 'Last year I was spot-on about the collapse of oil prices, wrong about the control of the Senate and almost everything else.',\n",
       " \" THE average consumer's telephone bill has risen by nearly 20 percent in the three years since the breakup of the Bell System in 1984, according to a report made public by the Consumer Federation of America.\",\n",
       " \"Arizona voters considered a bill that would have transferred regulation of phone rates to the Legislature and would simultaneously have deregulated most of the company's new ventures - primarily data transmission.\",\n",
       " \"At the Treasury's weekly bill auction, the average discount rate for a three-month bill rose one basis point, to 5.68 percent, its highest level since last August.\",\n",
       " \"His firm designed the Omaha Beach Memorial in Normandy, and in the early 1950's redesigned the House and Senate chambers in the Capitol in Washington.\",\n",
       " 'Fritz Schwarz, great-grandson of the toy company founder, served the nation well a decade ago as counsel to the Senate committee investigating intelligence abuses.',\n",
       " 'First they lost control of the Senate, then they watched Mr. Reagan become mired in controversy over the Iran arms deal and the diversion of profits to the Nicaraguan rebels.',\n",
       " \"''We have received lots of applications, and I mean lots,'' said a spokesman for Senator Daniel K. Inouye, who will head the Senate panel.\",\n",
       " \"In announcing this pocket veto, Mr. Reagan said he was committed to the clean water law but added, ''Unfortunately, this bill so far exceeds acceptable levels of intended budgetry commitments that I must withhold my approval.''\",\n",
       " 'Because the bill had been passed unanimously and had been strongly supported by business, labor, environmental and state and local government groups, most experts said that a written veto would have been easily overridden.',\n",
       " \"'Strong Alternative' Is Seen The Administration will now seek to have its own legislation introduced, providing what Mr. Thomas called a ''strong alternative'' to the bill Congress passed.\",\n",
       " 'But the new chairman of the Senate Environment and Public Works Committee, Quentin N. Burdick of North Dakota, announced today that a clean water bill identical with the one passed last year would be the first bill introduced in the Senate as the 100th Congress begins.',\n",
       " 'The bill may go to the floor of both chambers as early as the first week of the new session, according to staff aides on Capitol Hill.',\n",
       " 'By all accounts, the legislation continues to enjoy strong bipartisan support in both Houses; it already has 58 cosponsors in the Senate.',\n",
       " \"''We intend to push this bill through the Congress and back onto the President's desk as soon as possible,'' Senator Burdick, a Democrat, said today.\",\n",
       " \"Mr. Thomas said that the Administration's water bill would gradually eliminate the sewage treatment grants over eight years, as Congress wanted, instead of over four years as the President proposed last year.\",\n",
       " \"He said that the proposed bill would give states more flexibility in deciding whether they wanted to use the Federal money for outright grants to municipalities or to set up loan programs, but otherwise it left last fall's Congressional legislation unchanged.\"]"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame_1988_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_1988_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "38090"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bias_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame_1989_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_1989_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54356"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bias_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1990\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1990/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1990/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1990\n",
    "    file_name = \"nyt_1990_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':str(p)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") \n",
    "\n",
    "frame_1990 = pd.DataFrame()\n",
    "list_1990 = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_1990.append(df)\n",
    "    \n",
    "frame_1990 = pd.concat(list_1990)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame_1990['Newsbody'])):\n",
    "    if isinstance(frame_1990.iloc[i]['Newsbody'],str):\n",
    "        if not(frame_1990.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "            if any(word in frame_1990.iloc[i]['Newsbody'] for word in bias_words):\n",
    "                list_bias.append(frame_1990.iloc[i])\n",
    "\n",
    "frame_1990_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame_1990_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_1990_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73177"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bias_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1990\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1991/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1991/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1990\n",
    "    file_name = \"nyt_1991_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':str(p)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") \n",
    "\n",
    "frame_1991 = pd.DataFrame()\n",
    "list_1991 = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_1991.append(df)\n",
    "    \n",
    "frame_1991 = pd.concat(list_1991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame_1991['Newsbody'])):\n",
    "    if isinstance(frame_1991.iloc[i]['Newsbody'],str):\n",
    "        if not(frame_1991.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "            if any(word in frame_1991.iloc[i]['Newsbody'] for word in bias_words):\n",
    "                list_bias.append(frame_1991.iloc[i])\n",
    "\n",
    "frame_1991_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame_1991_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_1991_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93411"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bias_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(12):\n",
    "    base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "    # for creating a single csv file for every month in 1992\n",
    "    if i < 9:\n",
    "        base_path = base_path + str(\"/data/1992/0\") + str(i+1) \n",
    "    else:\n",
    "        base_path = base_path + str(\"/data/1992/\") + str(i+1) \n",
    "    \n",
    "    # creating file name for each month in 1992\n",
    "    file_name = \"nyt_1992_\" + str(i+1) + \".csv\"\n",
    "    \n",
    "    # creating the list of directories in each folder representing a month\n",
    "    dir_list = os.listdir(base_path)\n",
    "    \n",
    "    with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "        fieldnames = ['PubName','Headline','Newsbody']\n",
    "        csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "        csv_app.writeheader()\n",
    "        for single_dir in dir_list:\n",
    "            file_path = base_path + str('/') + str(single_dir) + str('/')\n",
    "            xml_file_list = os.listdir(file_path)\n",
    "            for xml_file in xml_file_list:\n",
    "                file = os.path.join(file_path,xml_file)\n",
    "                tree = ET.parse(file)\n",
    "                root = tree.getroot()\n",
    "                head = \"\"\n",
    "                p = \"\"\n",
    "                name = \"\"\n",
    "                for child in root:\n",
    "                    if child.tag == 'head':\n",
    "                        for child1 in child:\n",
    "                            if child1.tag == 'pubdata':\n",
    "                                t = child1.attrib\n",
    "                                name = t['name']\n",
    "                for child in root:\n",
    "                    if child.tag == 'body':\n",
    "                        for child1 in child:\n",
    "                            for child2 in child1:\n",
    "                                for child3 in child2:\n",
    "                                    if child3.tag == \"hl1\":\n",
    "                                        head = head + str(\" \" + child3.text)\n",
    "                        \n",
    "                                    elif child3.tag == \"p\" and child3.text.startswith(\"LEAD\"):\n",
    "                                        lead = child3.text\n",
    "                                    else:\n",
    "                                        p = p + str(\" \" + child3.text)\n",
    "                csv_app.writerow({'PubName':name,'Headline':head,'Newsbody':str(p)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "allFiles = glob.glob(path + \"/*.csv\") \n",
    "\n",
    "frame_1992 = pd.DataFrame()\n",
    "list_1992 = []\n",
    "for file_ in allFiles:\n",
    "    df = pd.read_csv(file_,index_col=None, header=0)\n",
    "    list_1992.append(df)\n",
    "    \n",
    "frame_1992 = pd.concat(list_1992)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_bias = []\n",
    "\n",
    "for i in range(len(frame_1992['Newsbody'])):\n",
    "    if isinstance(frame_1992.iloc[i]['Newsbody'],str):\n",
    "        if not(frame_1992.iloc[i]['Newsbody'].startswith(\" *3*** COMPANY REPORTS **\")):\n",
    "            if any(word in frame_1992.iloc[i]['Newsbody'] for word in bias_words):\n",
    "                list_bias.append(frame_1992.iloc[i])\n",
    "\n",
    "frame_1992_bias = pd.DataFrame(list_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(frame_1992_bias['Newsbody'])):\n",
    "    frame_list = tokenize.sent_tokenize(frame_1992_bias.iloc[i]['Newsbody'])\n",
    "    for sent in frame_list:\n",
    "        if any(word in sent for word in bias_words):\n",
    "            bias_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113299"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bias_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "\n",
    "for _ in range(len(bias_sentences)):\n",
    "    l = random.randint(1,2)\n",
    "    if l == 2:\n",
    "        labels.append('liberal')\n",
    "    else:\n",
    "        labels.append('conservative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.dirname(os.path.realpath(filename=\"project_notebook.ipynb\"))\n",
    "   \n",
    "    \n",
    "# creating file name for each month in 1992\n",
    "file_name = \"bias_sentences.csv\"\n",
    "    \n",
    "with open(file_name,\"w\",newline=\"\") as csv_file:\n",
    "    fieldnames = ['sentence','label']\n",
    "    csv_app = csv.DictWriter(csv_file,fieldnames=fieldnames)\n",
    "    csv_app.writeheader()\n",
    "    \n",
    "    for i in range(len(bias_sentences)):\n",
    "        csv_app.writerow({'sentence':bias_sentences[i],'label':labels[i]})    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_words = [\"republican \",\"democrat \",\"Pro-Choice\",\"bill \"\n",
    "              , \"congress \", \"abortion \", \"Pro-life\",\"Senate\",\"GOP\", \"caucus\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_sentences = pd.read_csv(\"bias_sentences.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To preserve his influence, he became more open to arguments from differing farming interests and even courted urban liberals by supporting a more generous approach to food stamps so as to win passage of a farm bill he could live with in 1973.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.iloc[120]['sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weak_annotator(sent, list1, list2):\n",
    "    ind1 = ind2 = len(sent)\n",
    "    if any(word in sent for word in list1):\n",
    "        ind1 = max([sent.lower().find(word) for word in list1])\n",
    "    elif any(word in sent for word in list2):\n",
    "        ind2 = max([sent.lower().find(word) for word in list2])\n",
    "    else:\n",
    "        ind1 = ind2 = len(sent)\n",
    "        \n",
    "    if ind1 < ind2:\n",
    "        return 'liberal'\n",
    "    elif ind2 < ind1:\n",
    "        return 'conservative'\n",
    "    else: \n",
    "        return 'neutral'       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'conservative'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences.iloc[100][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database = pd.read_csv(\"ibcData.csv\",sep='\\t', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([0, 1], dtype='int64')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['liberal', 'conservative', 'neutral'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database[1].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lib_list = ['liberal','democrat','pro-choice','agnostic','abortion','gay','freedom'\n",
    "            ,'climate change','secular','civil liberty','equality','regulation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_list = ['conservative','republican','pro-life','catholic','free market','tax cut','second amendment'\n",
    "            ,'fair trade','GOP',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_labels_ibc = []\n",
    " \n",
    "for i in range(ibc_database.count()[0]):\n",
    "    weak_labels_ibc.append(weak_annotator(ibc_database.iloc[i][0],lib_list,con_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4326"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(weak_labels_ibc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(bias_sentences.count()[0]):\n",
    "    bias_sentences.iloc[i][1] = weak_annotator(bias_sentences.iloc[i][0],lib_list,con_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    16256\n",
       "label       16256\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences[bias_sentences['label']=='liberal'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    1351\n",
       "label       1351\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences[bias_sentences['label']=='conservative'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence    95692\n",
       "label       95692\n",
       "dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_sentences[bias_sentences['label']=='neutral'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "492"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('liberal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "222"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('conservative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3612"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weak_labels_ibc.count('neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ibc_database.insert(len(ibc_database.columns), 'weak_label', weak_labels_ibc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>weak_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Forcing middle-class workers to bear a greater...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Because it would not be worthwhile to bring a ...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Indeed , Lind argues that high profits and hig...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>In fairness , it should be noted that he devot...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>liberal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Psychological tactics are social control techn...</td>\n",
       "      <td>liberal</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0        1 weak_label\n",
       "0  Forcing middle-class workers to bear a greater...  liberal    neutral\n",
       "1  Because it would not be worthwhile to bring a ...  liberal    neutral\n",
       "2  Indeed , Lind argues that high profits and hig...  liberal    neutral\n",
       "3  In fairness , it should be noted that he devot...  liberal    liberal\n",
       "4  Psychological tactics are social control techn...  liberal    neutral"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ibc_database.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os, sys, re, json, time, datetime, shutil\n",
    "import itertools, collections\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "assert(tf.__version__.startswith(\"1.4\"))\n",
    "\n",
    "# Helper libraries\n",
    "from common import utils, vocabulary, tf_embed_viz, treeviz\n",
    "from common import patched_numpy_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize IBC database\n",
    "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "X = ibc_database[0]\n",
    "U = bias_sentences['sentence']\n",
    "\n",
    "X_tokens = []\n",
    "U_tokens = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    X_tokens.append(tokenizer.tokenize(X[i]))\n",
    "    \n",
    "for i in range(len(U)):\n",
    "    U_tokens.append(tokenizer.tokenize(U[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# canonicalize IBC database\n",
    "\n",
    "X_tokens_canon = []\n",
    "U_tokens_canon = []\n",
    "\n",
    "for i in range(len(X_tokens)):\n",
    "    X_tokens_canon.append(utils.canonicalize_words(X_tokens[i]))\n",
    "    \n",
    "for i in range(len(U_tokens)):\n",
    "    U_tokens_canon.append(utils.canonicalize_words(U_tokens[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten the list of lists consisting of canonicalized sentences\n",
    "\n",
    "X_final = list(itertools.chain.from_iterable(X_tokens_canon))\n",
    "U_final = list(itertools.chain.from_iterable(U_tokens_canon))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15721\n",
      "54687\n"
     ]
    }
   ],
   "source": [
    "# number of unique tokens in database\n",
    "print(len(set(X_final)))\n",
    "print(len(set(U_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create unigrams \n",
    "\n",
    "unigram_counts_X = collections.Counter()\n",
    "unigram_counts_U = collections.Counter()\n",
    "\n",
    "for word in X_final:  \n",
    "    unigram_counts_X[word] += 1\n",
    "    \n",
    "for word in U_final:  \n",
    "    unigram_counts_U[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_X = unigram_counts_X.keys()\n",
    "\n",
    "counts_X = []\n",
    "for w in words_X:\n",
    "    counts_X.append(unigram_counts_X[w])\n",
    "    \n",
    "words_U = unigram_counts_U.keys()\n",
    "\n",
    "counts_U = []\n",
    "for w in words_U:\n",
    "    counts_U.append(unigram_counts_U[w])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorting the words according to their counts\n",
    "\n",
    "unigram_list_X = [x for _,x in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "unigram_count_X = [y for y,_ in sorted(zip(counts_X,words_X),  reverse=True)]\n",
    "\n",
    "unigram_list_U = [x for _,x in sorted(zip(counts_U,words_U),  reverse=True)]\n",
    "unigram_count_U = [y for y,_ in sorted(zip(counts_U,words_U),  reverse=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div class=\"bk-root\">\n",
       "        <a href=\"https://bokeh.pydata.org\" target=\"_blank\" class=\"bk-logo bk-logo-small bk-logo-notebook\"></a>\n",
       "        <span id=\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\">Loading BokehJS ...</span>\n",
       "    </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "(function(root) {\n",
       "  function now() {\n",
       "    return new Date();\n",
       "  }\n",
       "\n",
       "  var force = true;\n",
       "\n",
       "  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n",
       "    root._bokeh_onload_callbacks = [];\n",
       "    root._bokeh_is_loading = undefined;\n",
       "  }\n",
       "\n",
       "  var JS_MIME_TYPE = 'application/javascript';\n",
       "  var HTML_MIME_TYPE = 'text/html';\n",
       "  var EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n",
       "  var CLASS_NAME = 'output_bokeh rendered_html';\n",
       "\n",
       "  /**\n",
       "   * Render data to the DOM node\n",
       "   */\n",
       "  function render(props, node) {\n",
       "    var script = document.createElement(\"script\");\n",
       "    node.appendChild(script);\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when an output is cleared or removed\n",
       "   */\n",
       "  function handleClearOutput(event, handle) {\n",
       "    var cell = handle.cell;\n",
       "\n",
       "    var id = cell.output_area._bokeh_element_id;\n",
       "    var server_id = cell.output_area._bokeh_server_id;\n",
       "    // Clean up Bokeh references\n",
       "    if (id !== undefined) {\n",
       "      Bokeh.index[id].model.document.clear();\n",
       "      delete Bokeh.index[id];\n",
       "    }\n",
       "\n",
       "    if (server_id !== undefined) {\n",
       "      // Clean up Bokeh references\n",
       "      var cmd = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n",
       "      cell.notebook.kernel.execute(cmd, {\n",
       "        iopub: {\n",
       "          output: function(msg) {\n",
       "            var element_id = msg.content.text.trim();\n",
       "            Bokeh.index[element_id].model.document.clear();\n",
       "            delete Bokeh.index[element_id];\n",
       "          }\n",
       "        }\n",
       "      });\n",
       "      // Destroy server and session\n",
       "      var cmd = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n",
       "      cell.notebook.kernel.execute(cmd);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  /**\n",
       "   * Handle when a new output is added\n",
       "   */\n",
       "  function handleAddOutput(event, handle) {\n",
       "    var output_area = handle.output_area;\n",
       "    var output = handle.output;\n",
       "\n",
       "    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n",
       "    if ((output.output_type != \"display_data\") || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n",
       "      return\n",
       "    }\n",
       "\n",
       "    var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n",
       "\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n",
       "      toinsert[0].firstChild.textContent = output.data[JS_MIME_TYPE];\n",
       "      // store reference to embed id on output_area\n",
       "      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n",
       "    }\n",
       "    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n",
       "      var bk_div = document.createElement(\"div\");\n",
       "      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n",
       "      var script_attrs = bk_div.children[0].attributes;\n",
       "      for (var i = 0; i < script_attrs.length; i++) {\n",
       "        toinsert[0].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n",
       "      }\n",
       "      // store reference to server id on output_area\n",
       "      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n",
       "    }\n",
       "  }\n",
       "\n",
       "  function register_renderer(events, OutputArea) {\n",
       "\n",
       "    function append_mime(data, metadata, element) {\n",
       "      // create a DOM node to render to\n",
       "      var toinsert = this.create_output_subarea(\n",
       "        metadata,\n",
       "        CLASS_NAME,\n",
       "        EXEC_MIME_TYPE\n",
       "      );\n",
       "      this.keyboard_manager.register_events(toinsert);\n",
       "      // Render to node\n",
       "      var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n",
       "      render(props, toinsert[0]);\n",
       "      element.append(toinsert);\n",
       "      return toinsert\n",
       "    }\n",
       "\n",
       "    /* Handle when an output is cleared or removed */\n",
       "    events.on('clear_output.CodeCell', handleClearOutput);\n",
       "    events.on('delete.Cell', handleClearOutput);\n",
       "\n",
       "    /* Handle when a new output is added */\n",
       "    events.on('output_added.OutputArea', handleAddOutput);\n",
       "\n",
       "    /**\n",
       "     * Register the mime type and append_mime function with output_area\n",
       "     */\n",
       "    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n",
       "      /* Is output safe? */\n",
       "      safe: true,\n",
       "      /* Index of renderer in `output_area.display_order` */\n",
       "      index: 0\n",
       "    });\n",
       "  }\n",
       "\n",
       "  // register the mime type if in Jupyter Notebook environment and previously unregistered\n",
       "  if (root.Jupyter !== undefined) {\n",
       "    var events = require('base/js/events');\n",
       "    var OutputArea = require('notebook/js/outputarea').OutputArea;\n",
       "\n",
       "    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n",
       "      register_renderer(events, OutputArea);\n",
       "    }\n",
       "  }\n",
       "\n",
       "  \n",
       "  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n",
       "    root._bokeh_timeout = Date.now() + 5000;\n",
       "    root._bokeh_failed_load = false;\n",
       "  }\n",
       "\n",
       "  var NB_LOAD_WARNING = {'data': {'text/html':\n",
       "     \"<div style='background-color: #fdd'>\\n\"+\n",
       "     \"<p>\\n\"+\n",
       "     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n",
       "     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n",
       "     \"</p>\\n\"+\n",
       "     \"<ul>\\n\"+\n",
       "     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n",
       "     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n",
       "     \"</ul>\\n\"+\n",
       "     \"<code>\\n\"+\n",
       "     \"from bokeh.resources import INLINE\\n\"+\n",
       "     \"output_notebook(resources=INLINE)\\n\"+\n",
       "     \"</code>\\n\"+\n",
       "     \"</div>\"}};\n",
       "\n",
       "  function display_loaded() {\n",
       "    var el = document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\");\n",
       "    if (el != null) {\n",
       "      el.textContent = \"BokehJS is loading...\";\n",
       "    }\n",
       "    if (root.Bokeh !== undefined) {\n",
       "      if (el != null) {\n",
       "        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n",
       "      }\n",
       "    } else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(display_loaded, 100)\n",
       "    }\n",
       "  }\n",
       "\n",
       "\n",
       "  function run_callbacks() {\n",
       "    try {\n",
       "      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n",
       "    }\n",
       "    finally {\n",
       "      delete root._bokeh_onload_callbacks\n",
       "    }\n",
       "    console.info(\"Bokeh: all callbacks have finished\");\n",
       "  }\n",
       "\n",
       "  function load_libs(js_urls, callback) {\n",
       "    root._bokeh_onload_callbacks.push(callback);\n",
       "    if (root._bokeh_is_loading > 0) {\n",
       "      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n",
       "      return null;\n",
       "    }\n",
       "    if (js_urls == null || js_urls.length === 0) {\n",
       "      run_callbacks();\n",
       "      return null;\n",
       "    }\n",
       "    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n",
       "    root._bokeh_is_loading = js_urls.length;\n",
       "    for (var i = 0; i < js_urls.length; i++) {\n",
       "      var url = js_urls[i];\n",
       "      var s = document.createElement('script');\n",
       "      s.src = url;\n",
       "      s.async = false;\n",
       "      s.onreadystatechange = s.onload = function() {\n",
       "        root._bokeh_is_loading--;\n",
       "        if (root._bokeh_is_loading === 0) {\n",
       "          console.log(\"Bokeh: all BokehJS libraries loaded\");\n",
       "          run_callbacks()\n",
       "        }\n",
       "      };\n",
       "      s.onerror = function() {\n",
       "        console.warn(\"failed to load library \" + url);\n",
       "      };\n",
       "      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n",
       "      document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "    }\n",
       "  };var element = document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\");\n",
       "  if (element == null) {\n",
       "    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9440c0eb-b960-48bc-8834-a5ecfd2d3d1c' but no matching script tag was found. \")\n",
       "    return false;\n",
       "  }\n",
       "\n",
       "  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n",
       "\n",
       "  var inline_js = [\n",
       "    function(Bokeh) {\n",
       "      Bokeh.set_log_level(\"info\");\n",
       "    },\n",
       "    \n",
       "    function(Bokeh) {\n",
       "      \n",
       "    },\n",
       "    function(Bokeh) {\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n",
       "      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n",
       "    }\n",
       "  ];\n",
       "\n",
       "  function run_inline_js() {\n",
       "    \n",
       "    if ((root.Bokeh !== undefined) || (force === true)) {\n",
       "      for (var i = 0; i < inline_js.length; i++) {\n",
       "        inline_js[i].call(root, root.Bokeh);\n",
       "      }if (force === true) {\n",
       "        display_loaded();\n",
       "      }} else if (Date.now() < root._bokeh_timeout) {\n",
       "      setTimeout(run_inline_js, 100);\n",
       "    } else if (!root._bokeh_failed_load) {\n",
       "      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n",
       "      root._bokeh_failed_load = true;\n",
       "    } else if (force !== true) {\n",
       "      var cell = $(document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\")).parents('.cell').data().cell;\n",
       "      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n",
       "    }\n",
       "\n",
       "  }\n",
       "\n",
       "  if (root._bokeh_is_loading === 0) {\n",
       "    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n",
       "    run_inline_js();\n",
       "  } else {\n",
       "    load_libs(js_urls, function() {\n",
       "      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n",
       "      run_inline_js();\n",
       "    });\n",
       "  }\n",
       "}(window));"
      ],
      "application/vnd.bokehjs_load.v0+json": "\n(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  var force = true;\n\n  if (typeof (root._bokeh_onload_callbacks) === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\n  \n\n  \n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  var NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    var el = document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\");\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) { callback() });\n    }\n    finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.info(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(js_urls, callback) {\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.log(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.log(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = js_urls.length;\n    for (var i = 0; i < js_urls.length; i++) {\n      var url = js_urls[i];\n      var s = document.createElement('script');\n      s.src = url;\n      s.async = false;\n      s.onreadystatechange = s.onload = function() {\n        root._bokeh_is_loading--;\n        if (root._bokeh_is_loading === 0) {\n          console.log(\"Bokeh: all BokehJS libraries loaded\");\n          run_callbacks()\n        }\n      };\n      s.onerror = function() {\n        console.warn(\"failed to load library \" + url);\n      };\n      console.log(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.getElementsByTagName(\"head\")[0].appendChild(s);\n    }\n  };var element = document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\");\n  if (element == null) {\n    console.log(\"Bokeh: ERROR: autoload.js configured with elementid '9440c0eb-b960-48bc-8834-a5ecfd2d3d1c' but no matching script tag was found. \")\n    return false;\n  }\n\n  var js_urls = [\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.js\", \"https://cdn.pydata.org/bokeh/release/bokeh-gl-0.12.13.min.js\"];\n\n  var inline_js = [\n    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\n    \n    function(Bokeh) {\n      \n    },\n    function(Bokeh) {\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-widgets-0.12.13.min.css\");\n      console.log(\"Bokeh: injecting CSS: https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n      Bokeh.embed.inject_css(\"https://cdn.pydata.org/bokeh/release/bokeh-tables-0.12.13.min.css\");\n    }\n  ];\n\n  function run_inline_js() {\n    \n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (var i = 0; i < inline_js.length; i++) {\n        inline_js[i].call(root, root.Bokeh);\n      }if (force === true) {\n        display_loaded();\n      }} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      var cell = $(document.getElementById(\"9440c0eb-b960-48bc-8834-a5ecfd2d3d1c\")).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.log(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(js_urls, function() {\n      console.log(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the distribution of unigrams\n",
    "\n",
    "utils.require_package(\"bokeh\")\n",
    "import bokeh.plotting as bp\n",
    "from bokeh.models import HoverTool\n",
    "bp.output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_X, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist, bin_edges = np.histogram(a=unigram_count_U, bins=40, normed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"18b35974-3c4c-4502-831e-e1df049dab17\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"4bdf65b0-5f84-4356-af65-849ddb1d4e9f\":{\"roots\":{\"references\":[{\"attributes\":{\"callback\":null,\"column_names\":[\"top\",\"x\"],\"data\":{\"top\":[9943,9672,5550,5241,5165,4325,3054,2841,2762,1811,1593,1249,1106,1071,1046,1014,942,881,832,827,793,744,741,722,712,680,658,640,602,564,526,522,521,516,511,489,485,484,479,475,469,456,440,440,432,420,414,406,394,384,374,371,350,345,335,329,323,318,317,313,296,295,292,289,288,287,285,284,282,280,277,261,259,257,256,251,249,244,241,240,236,235,230,226,225,223,222,217,213,213,210,207,205,203,203,203,202,200,197,195],\"x\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]}},\"id\":\"57df6edd-ddb3-4761-816c-c5610da3a484\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"243e87e8-8fb8-472b-9a15-0c36dcf3d879\",\"type\":\"BasicTicker\"},{\"attributes\":{},\"id\":\"18cdef33-ff82-4ab7-b5d3-6a00f067450f\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"38399ab5-038c-42ff-bba0-47367d4cae0b\",\"type\":\"VBar\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c01f8b74-26a7-49f0-8f4b-8037399d1178\",\"type\":\"CategoricalTicker\"}},\"id\":\"b4644b91-e849-47e5-bb46-c034b9854ecc\",\"type\":\"Grid\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"272c5773-772e-4341-af64-8e56a9ddefe9\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"243e87e8-8fb8-472b-9a15-0c36dcf3d879\",\"type\":\"BasicTicker\"}},\"id\":\"c24d1b71-1b39-4541-9f11-bf48193e6861\",\"type\":\"LinearAxis\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"e89e6691-66ea-4cce-b7ce-92ee4e577a35\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"callback\":null,\"factors\":[\",\",\"the\",\"and\",\"to\",\"of\",\".\",\"a\",\"that\",\"in\",\"for\",\"is\",\"as\",\"it\",\"by\",\"on\",\"``\",\"are\",\"with\",\"''\",\"their\",\"'s\",\"they\",\"not\",\"have\",\"or\",\"be\",\"more\",\"but\",\"from\",\"who\",\"an\",\")\",\"government\",\"(\",\"because\",\"this\",\"we\",\"would\",\"was\",\"has\",\":\",\"our\",\"at\",\"--\",\"will\",\"which\",\"can\",\"people\",\"he\",\"than\",\"his\",\";\",\"if\",\"its\",\"all\",\"make\",\"about\",\"when\",\"economic\",\"other\",\"new\",\"DGDGDGDG\",\"one\",\"tax\",\"how\",\"so\",\"energy\",\"them\",\"were\",\"those\",\"care\",\"health\",\"social\",\"even\",\"only\",\"free\",\"been\",\"create\",\"public\",\"economy\",\"these\",\"do\",\"no\",\"while\",\"use\",\"such\",\"system\",\"money\",\"most\",\"good\",\"also\",\"out\",\"many\",\"over\",\"federal\",\"DGDG\",\"had\",\"there\",\"security\",\"up\"]},\"id\":\"6860ede1-af81-45c4-b831-68050112d691\",\"type\":\"FactorRange\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"0276df98-8374-4546-9a88-3542c0b42515\",\"type\":\"VBar\"},{\"attributes\":{\"overlay\":{\"id\":\"e89e6691-66ea-4cce-b7ce-92ee4e577a35\",\"type\":\"BoxAnnotation\"}},\"id\":\"70ae46d5-22f7-4ab3-9920-a3522ad40399\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"cf69646f-de19-49d7-9aa9-06b7431e16d9\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"c01f8b74-26a7-49f0-8f4b-8037399d1178\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"bd1db7d3-3520-47a0-b086-4f5bfa567dd1\",\"type\":\"VBar\"},{\"attributes\":{\"below\":[{\"id\":\"4c8f688b-ad23-42dd-addc-f32e33184b4e\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"c24d1b71-1b39-4541-9f11-bf48193e6861\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"4c8f688b-ad23-42dd-addc-f32e33184b4e\",\"type\":\"CategoricalAxis\"},{\"id\":\"b4644b91-e849-47e5-bb46-c034b9854ecc\",\"type\":\"Grid\"},{\"id\":\"c24d1b71-1b39-4541-9f11-bf48193e6861\",\"type\":\"LinearAxis\"},{\"id\":\"fbb249b3-3014-4882-9ea2-6bbcfab9876f\",\"type\":\"Grid\"},{\"id\":\"e89e6691-66ea-4cce-b7ce-92ee4e577a35\",\"type\":\"BoxAnnotation\"},{\"id\":\"dc7e4707-5776-49ba-9826-0347019ae6e9\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"4a67be8b-b54c-4382-9fa5-e5431ef5ad87\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"71071d2f-27de-41ec-adc2-ed7ce6234fc8\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"6860ede1-af81-45c4-b831-68050112d691\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"f5ac0a43-6add-46c5-bc81-54600737e2d4\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"23879bc8-71bb-40ac-a072-dc60f1d90ec4\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"37723ef0-e854-4dfd-9a00-9bdfd1db0f51\",\"type\":\"LinearScale\"}},\"id\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"formatter\":{\"id\":\"18cdef33-ff82-4ab7-b5d3-6a00f067450f\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"c01f8b74-26a7-49f0-8f4b-8037399d1178\",\"type\":\"CategoricalTicker\"}},\"id\":\"4c8f688b-ad23-42dd-addc-f32e33184b4e\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"cf69646f-de19-49d7-9aa9-06b7431e16d9\",\"type\":\"PanTool\"},{\"id\":\"c6a860e8-c8c4-4fa4-a944-4ceee97f9f45\",\"type\":\"WheelZoomTool\"},{\"id\":\"70ae46d5-22f7-4ab3-9920-a3522ad40399\",\"type\":\"BoxZoomTool\"},{\"id\":\"b6cab4cf-522a-4e6e-9737-06a9156ced3f\",\"type\":\"SaveTool\"},{\"id\":\"47952a0f-95e9-4c49-881b-1d939613eab3\",\"type\":\"ResetTool\"},{\"id\":\"896881a6-3828-45fc-b739-521dd9ae6865\",\"type\":\"HelpTool\"},{\"id\":\"de013ff3-03c0-4555-92fa-013e80372e4c\",\"type\":\"HoverTool\"}]},\"id\":\"71071d2f-27de-41ec-adc2-ed7ce6234fc8\",\"type\":\"Toolbar\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"4a67be8b-b54c-4382-9fa5-e5431ef5ad87\",\"type\":\"Title\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"dc7e4707-5776-49ba-9826-0347019ae6e9\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"de013ff3-03c0-4555-92fa-013e80372e4c\",\"type\":\"HoverTool\"},{\"attributes\":{},\"id\":\"272c5773-772e-4341-af64-8e56a9ddefe9\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{},\"id\":\"f5ac0a43-6add-46c5-bc81-54600737e2d4\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"57df6edd-ddb3-4761-816c-c5610da3a484\",\"type\":\"ColumnDataSource\"}},\"id\":\"c9c22d7e-10ca-44c5-92df-6dcd18548083\",\"type\":\"CDSView\"},{\"attributes\":{\"callback\":null,\"end\":11931.6,\"start\":0},\"id\":\"23879bc8-71bb-40ac-a072-dc60f1d90ec4\",\"type\":\"DataRange1d\"},{\"attributes\":{},\"id\":\"896881a6-3828-45fc-b739-521dd9ae6865\",\"type\":\"HelpTool\"},{\"attributes\":{},\"id\":\"47952a0f-95e9-4c49-881b-1d939613eab3\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"c6a860e8-c8c4-4fa4-a944-4ceee97f9f45\",\"type\":\"WheelZoomTool\"},{\"attributes\":{},\"id\":\"b6cab4cf-522a-4e6e-9737-06a9156ced3f\",\"type\":\"SaveTool\"},{\"attributes\":{},\"id\":\"37723ef0-e854-4dfd-9a00-9bdfd1db0f51\",\"type\":\"LinearScale\"},{\"attributes\":{\"data_source\":{\"id\":\"57df6edd-ddb3-4761-816c-c5610da3a484\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"0276df98-8374-4546-9a88-3542c0b42515\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"38399ab5-038c-42ff-bba0-47367d4cae0b\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"bd1db7d3-3520-47a0-b086-4f5bfa567dd1\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"c9c22d7e-10ca-44c5-92df-6dcd18548083\",\"type\":\"CDSView\"}},\"id\":\"dc7e4707-5776-49ba-9826-0347019ae6e9\",\"type\":\"GlyphRenderer\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"243e87e8-8fb8-472b-9a15-0c36dcf3d879\",\"type\":\"BasicTicker\"}},\"id\":\"fbb249b3-3014-4882-9ea2-6bbcfab9876f\",\"type\":\"Grid\"}],\"root_ids\":[\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.13\"}};\n",
       "  var render_items = [{\"docid\":\"4bdf65b0-5f84-4356-af65-849ddb1d4e9f\",\"elementid\":\"18b35974-3c4c-4502-831e-e1df049dab17\",\"modelid\":\"5f5781b4-8c3a-4d01-a2ca-4f933386bbf3\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "5f5781b4-8c3a-4d01-a2ca-4f933386bbf3"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_X[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_X[:nplot], width=0.4, top=unigram_count_X[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_X)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div class=\"bk-root\">\n",
       "    <div class=\"bk-plotdiv\" id=\"7c434da8-d39a-4891-aa77-e9388372d49e\"></div>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "(function(root) {\n",
       "  function embed_document(root) {\n",
       "    \n",
       "  var docs_json = {\"536a3271-9ad1-4b51-8e9b-c15f37431053\":{\"roots\":{\"references\":[{\"attributes\":{},\"id\":\"42d06a26-f1a6-4e74-8c18-0ea8738b1a5a\",\"type\":\"HelpTool\"},{\"attributes\":{\"data_source\":{\"id\":\"16faea65-9b9d-487e-adc7-2b2c36ba4a05\",\"type\":\"ColumnDataSource\"},\"glyph\":{\"id\":\"ee441056-a7ad-4f48-b783-f1a3d4212ff5\",\"type\":\"VBar\"},\"hover_glyph\":{\"id\":\"576624cb-7fc5-4007-835a-67afb2308a6e\",\"type\":\"VBar\"},\"muted_glyph\":null,\"nonselection_glyph\":{\"id\":\"e9baa29a-5b83-4a50-8fc1-00e293ab2bd0\",\"type\":\"VBar\"},\"selection_glyph\":null,\"view\":{\"id\":\"17073ca3-38a3-4138-b65b-509bbbf98615\",\"type\":\"CDSView\"}},\"id\":\"d841442b-dbf1-40dc-ae41-3db961dc2bd7\",\"type\":\"GlyphRenderer\"},{\"attributes\":{},\"id\":\"45d3e3af-065b-48de-8115-4d15c44feee6\",\"type\":\"LinearScale\"},{\"attributes\":{},\"id\":\"2ca0a367-ad38-4513-8bd5-4ad0b9448b31\",\"type\":\"CategoricalTickFormatter\"},{\"attributes\":{\"below\":[{\"id\":\"7d625b2e-3ac2-408c-a086-ae25797e966b\",\"type\":\"CategoricalAxis\"}],\"left\":[{\"id\":\"e1c8871a-fc55-4080-8ca4-cd3d0fb011ba\",\"type\":\"LinearAxis\"}],\"plot_width\":1000,\"renderers\":[{\"id\":\"7d625b2e-3ac2-408c-a086-ae25797e966b\",\"type\":\"CategoricalAxis\"},{\"id\":\"bda35ee6-03d9-4721-bcbf-d98fe730d727\",\"type\":\"Grid\"},{\"id\":\"e1c8871a-fc55-4080-8ca4-cd3d0fb011ba\",\"type\":\"LinearAxis\"},{\"id\":\"1a9aa086-a015-4bd4-94c9-5e563d828a5a\",\"type\":\"Grid\"},{\"id\":\"06708bdf-19a6-4b46-9d7e-6e99ec5a3bd3\",\"type\":\"BoxAnnotation\"},{\"id\":\"d841442b-dbf1-40dc-ae41-3db961dc2bd7\",\"type\":\"GlyphRenderer\"}],\"title\":{\"id\":\"fc2dc9a1-fe2f-45ed-b150-00d8f9ac1e60\",\"type\":\"Title\"},\"toolbar\":{\"id\":\"de139ed5-396d-499b-ab4e-285122f4957d\",\"type\":\"Toolbar\"},\"x_range\":{\"id\":\"9b020c3e-b300-446e-a130-3b8ebbd05319\",\"type\":\"FactorRange\"},\"x_scale\":{\"id\":\"037a6304-7f6e-4ee8-b7f1-3f1e40ae222b\",\"type\":\"CategoricalScale\"},\"y_range\":{\"id\":\"53b875d2-88ff-4c72-bd83-e81ea8369209\",\"type\":\"DataRange1d\"},\"y_scale\":{\"id\":\"45d3e3af-065b-48de-8115-4d15c44feee6\",\"type\":\"LinearScale\"}},\"id\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\",\"subtype\":\"Figure\",\"type\":\"Plot\"},{\"attributes\":{\"axis_label\":\"Count(w)\",\"formatter\":{\"id\":\"c1301320-91a9-40a5-871b-bf80426f5cfb\",\"type\":\"BasicTickFormatter\"},\"plot\":{\"id\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6d8eebdf-4416-43bf-8087-7df807294f99\",\"type\":\"BasicTicker\"}},\"id\":\"e1c8871a-fc55-4080-8ca4-cd3d0fb011ba\",\"type\":\"LinearAxis\"},{\"attributes\":{\"callback\":null,\"end\":315195.6,\"start\":0},\"id\":\"53b875d2-88ff-4c72-bd83-e81ea8369209\",\"type\":\"DataRange1d\"},{\"attributes\":{\"callback\":null,\"column_names\":[\"top\",\"x\"],\"data\":{\"top\":[262663,175107,112277,101315,97575,86820,70380,68361,59032,46670,44546,38023,34077,29305,26325,23835,21807,20233,19947,19424,18851,17921,17222,17030,16010,15420,15128,14976,14823,14735,14392,13661,13363,13160,12929,12329,12124,11678,11527,11527,11479,10724,9889,9341,8894,8630,8517,8503,8382,8310,8236,7688,7671,7642,7580,7428,7302,6925,6912,6435,6331,6101,6051,6030,5871,5867,5847,5705,5677,5584,5583,5326,5303,5277,5077,4877,4833,4785,4757,4679,4650,4629,4591,4584,4577,4567,4464,4424,4346,4296,4242,4172,4168,4088,4060,4025,3934,3927,3924,3862],\"x\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]}},\"id\":\"16faea65-9b9d-487e-adc7-2b2c36ba4a05\",\"type\":\"ColumnDataSource\"},{\"attributes\":{},\"id\":\"a2e77389-2bac-47c2-ae88-cd1605ada523\",\"type\":\"ResetTool\"},{\"attributes\":{},\"id\":\"6d8eebdf-4416-43bf-8087-7df807294f99\",\"type\":\"BasicTicker\"},{\"attributes\":{\"plot\":null,\"text\":\"\"},\"id\":\"fc2dc9a1-fe2f-45ed-b150-00d8f9ac1e60\",\"type\":\"Title\"},{\"attributes\":{\"dimension\":1,\"plot\":{\"id\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"6d8eebdf-4416-43bf-8087-7df807294f99\",\"type\":\"BasicTicker\"}},\"id\":\"1a9aa086-a015-4bd4-94c9-5e563d828a5a\",\"type\":\"Grid\"},{\"attributes\":{\"bottom_units\":\"screen\",\"fill_alpha\":{\"value\":0.5},\"fill_color\":{\"value\":\"lightgrey\"},\"left_units\":\"screen\",\"level\":\"overlay\",\"line_alpha\":{\"value\":1.0},\"line_color\":{\"value\":\"black\"},\"line_dash\":[4,4],\"line_width\":{\"value\":2},\"plot\":null,\"render_mode\":\"css\",\"right_units\":\"screen\",\"top_units\":\"screen\"},\"id\":\"06708bdf-19a6-4b46-9d7e-6e99ec5a3bd3\",\"type\":\"BoxAnnotation\"},{\"attributes\":{\"overlay\":{\"id\":\"06708bdf-19a6-4b46-9d7e-6e99ec5a3bd3\",\"type\":\"BoxAnnotation\"}},\"id\":\"7628e905-2936-4cbe-8401-3e088f81b8f8\",\"type\":\"BoxZoomTool\"},{\"attributes\":{},\"id\":\"c1301320-91a9-40a5-871b-bf80426f5cfb\",\"type\":\"BasicTickFormatter\"},{\"attributes\":{\"fill_alpha\":{\"value\":0.1},\"fill_color\":{\"value\":\"#1f77b4\"},\"line_alpha\":{\"value\":0.1},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"e9baa29a-5b83-4a50-8fc1-00e293ab2bd0\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"c39bde2a-3e76-4a62-a033-4f6ac767ba88\",\"type\":\"PanTool\"},{\"attributes\":{},\"id\":\"037a6304-7f6e-4ee8-b7f1-3f1e40ae222b\",\"type\":\"CategoricalScale\"},{\"attributes\":{\"source\":{\"id\":\"16faea65-9b9d-487e-adc7-2b2c36ba4a05\",\"type\":\"ColumnDataSource\"}},\"id\":\"17073ca3-38a3-4138-b65b-509bbbf98615\",\"type\":\"CDSView\"},{\"attributes\":{\"formatter\":{\"id\":\"2ca0a367-ad38-4513-8bd5-4ad0b9448b31\",\"type\":\"CategoricalTickFormatter\"},\"major_label_orientation\":\"vertical\",\"plot\":{\"id\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a3f7c38d-ab82-472e-949c-5408fd15d974\",\"type\":\"CategoricalTicker\"}},\"id\":\"7d625b2e-3ac2-408c-a086-ae25797e966b\",\"type\":\"CategoricalAxis\"},{\"attributes\":{\"fill_color\":{\"value\":\"firebrick\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"576624cb-7fc5-4007-835a-67afb2308a6e\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"046d8248-4060-4748-9833-ba78642cc188\",\"type\":\"WheelZoomTool\"},{\"attributes\":{\"active_drag\":\"auto\",\"active_inspect\":\"auto\",\"active_scroll\":\"auto\",\"active_tap\":\"auto\",\"tools\":[{\"id\":\"c39bde2a-3e76-4a62-a033-4f6ac767ba88\",\"type\":\"PanTool\"},{\"id\":\"046d8248-4060-4748-9833-ba78642cc188\",\"type\":\"WheelZoomTool\"},{\"id\":\"7628e905-2936-4cbe-8401-3e088f81b8f8\",\"type\":\"BoxZoomTool\"},{\"id\":\"da39a3a5-6a52-40c2-b43b-d4a974e88ac1\",\"type\":\"SaveTool\"},{\"id\":\"a2e77389-2bac-47c2-ae88-cd1605ada523\",\"type\":\"ResetTool\"},{\"id\":\"42d06a26-f1a6-4e74-8c18-0ea8738b1a5a\",\"type\":\"HelpTool\"},{\"id\":\"e616266d-1c90-4b3b-9be8-a66a8680e90f\",\"type\":\"HoverTool\"}]},\"id\":\"de139ed5-396d-499b-ab4e-285122f4957d\",\"type\":\"Toolbar\"},{\"attributes\":{},\"id\":\"a3f7c38d-ab82-472e-949c-5408fd15d974\",\"type\":\"CategoricalTicker\"},{\"attributes\":{\"callback\":null,\"mode\":\"vline\",\"renderers\":[{\"id\":\"d841442b-dbf1-40dc-ae41-3db961dc2bd7\",\"type\":\"GlyphRenderer\"}],\"tooltips\":[[\"word\",\"@x\"],[\"count\",\"@top\"]]},\"id\":\"e616266d-1c90-4b3b-9be8-a66a8680e90f\",\"type\":\"HoverTool\"},{\"attributes\":{\"fill_color\":{\"value\":\"#1f77b4\"},\"line_color\":{\"value\":\"#1f77b4\"},\"top\":{\"field\":\"top\"},\"width\":{\"value\":0.4},\"x\":{\"field\":\"x\"}},\"id\":\"ee441056-a7ad-4f48-b783-f1a3d4212ff5\",\"type\":\"VBar\"},{\"attributes\":{},\"id\":\"da39a3a5-6a52-40c2-b43b-d4a974e88ac1\",\"type\":\"SaveTool\"},{\"attributes\":{\"grid_line_alpha\":{\"value\":0.75},\"plot\":{\"id\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\",\"subtype\":\"Figure\",\"type\":\"Plot\"},\"ticker\":{\"id\":\"a3f7c38d-ab82-472e-949c-5408fd15d974\",\"type\":\"CategoricalTicker\"}},\"id\":\"bda35ee6-03d9-4721-bcbf-d98fe730d727\",\"type\":\"Grid\"},{\"attributes\":{\"callback\":null,\"factors\":[\"the\",\",\",\".\",\"of\",\"to\",\"a\",\"and\",\"in\",\"senate\",\"that\",\"bill\",\"for\",\"on\",\"'s\",\"by\",\"is\",\"mr.\",\"would\",\"''\",\"was\",\"said\",\"DGDG\",\"he\",\"as\",\"with\",\"it\",\"house\",\"an\",\"abortion\",\"committee\",\"be\",\"who\",\"from\",\"his\",\"has\",\"at\",\"not\",\"have\",\"new\",\"DGDGDGDG\",\"but\",\"state\",\"$\",\"this\",\"president\",\"are\",\"will\",\"had\",\"republican\",\"senator\",\"which\",\"``\",\"congress\",\"today\",\"or\",\"they\",\"last\",\"their\",\"DG\",\"year\",\"more\",\"its\",\"vote\",\"been\",\":\",\"one\",\"were\",\"about\",\"when\",\"bush\",\"after\",\"before\",\"democrat\",\"states\",\"if\",\"two\",\"rights\",\"percent\",\"than\",\"over\",\"DGDGDG\",\"democratic\",\"leader\",\"also\",\"other\",\"passed\",\"court\",\"some\",\"week\",\"up\",\"years\",\"federal\",\"chairman\",\"democrats\",\"could\",\"united\",\"administration\",\"approved\",\"majority\",\"tax\"]},\"id\":\"9b020c3e-b300-446e-a130-3b8ebbd05319\",\"type\":\"FactorRange\"}],\"root_ids\":[\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\"]},\"title\":\"Bokeh Application\",\"version\":\"0.12.13\"}};\n",
       "  var render_items = [{\"docid\":\"536a3271-9ad1-4b51-8e9b-c15f37431053\",\"elementid\":\"7c434da8-d39a-4891-aa77-e9388372d49e\",\"modelid\":\"a17cd909-f1f1-4cf5-8b60-528218ab0b51\"}];\n",
       "  root.Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "\n",
       "  }\n",
       "  if (root.Bokeh !== undefined) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (root.Bokeh !== undefined) {\n",
       "        embed_document(root);\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "      attempts++;\n",
       "      if (attempts > 100) {\n",
       "        console.log(\"Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing\")\n",
       "        clearInterval(timer);\n",
       "      }\n",
       "    }, 10, root)\n",
       "  }\n",
       "})(window);"
      ],
      "application/vnd.bokehjs_exec.v0+json": ""
     },
     "metadata": {
      "application/vnd.bokehjs_exec.v0+json": {
       "id": "a17cd909-f1f1-4cf5-8b60-528218ab0b51"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "nplot = 100\n",
    "fig = bp.figure(x_range=unigram_list_U[:nplot], plot_width=1000, plot_height=600)\n",
    "bars = fig.vbar(x=unigram_list_U[:nplot], width=0.4, top=unigram_count_U[:nplot], hover_fill_color=\"firebrick\")\n",
    "fig.add_tools(HoverTool(tooltips=[(\"word\", \"@x\"), (\"count\", \"@top\")], renderers=[bars], mode=\"vline\"))\n",
    "fig.y_range.start = 0\n",
    "fig.y_range.end = 1.2*max(unigram_count_U)\n",
    "fig.yaxis.axis_label = \"Count(w)\"\n",
    "fig.xgrid.grid_line_alpha = 0.75\n",
    "fig.xaxis.major_label_orientation = \"vertical\"\n",
    "bp.show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50th percentile:  2.0 \n",
      "49th percentile:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"50th percentile: \",np.percentile(unigram_count_X, 50),\"\\n49th percentile: \",np.percentile(unigram_count_X, 49))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50th percentile:  2.0 \n",
      "49th percentile:  1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"50th percentile: \",np.percentile(unigram_count_U, 42),\"\\n49th percentile: \",np.percentile(unigram_count_U, 41))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initially we set vocabulary size equal to total number of unique tokens in the dataset\n",
    "\n",
    "mergelist = set(unigram_list_X + unigram_list_U)\n",
    "\n",
    "V = len(mergelist)\n",
    "\n",
    "mergelist_XU = X_final + U_final\n",
    "\n",
    "\n",
    "vocab = vocabulary.Vocabulary(mergelist_XU, size=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the maximum length of sentences\n",
    "lengths_X = map(len,X_tokens_canon)\n",
    "lengths_U = map(len,U_tokens_canon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths_X = list(lengths_X)\n",
    "lengths_U = list(lengths_U)\n",
    "max_len1 = max(lengths_X)\n",
    "max_len2 = max(lengths_U)\n",
    "max_len = max(max_len1,max_len2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ids = []\n",
    "U_ids = []\n",
    "for i in range(len(X_tokens_canon)):\n",
    "    X_ids.append(vocab.words_to_ids(X_tokens_canon[i]))\n",
    "    \n",
    "for i in range(len(U_tokens_canon)):\n",
    "    U_ids.append(vocab.words_to_ids(U_tokens_canon[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final padded sentences\n",
    "X_sent, ns = utils.pad_np_array(X_ids, max_len=max_len, pad_id=0)\n",
    "U_sent, u_ns = utils.pad_np_array(U_ids, max_len=max_len, pad_id=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4326, 493)\n",
      "(113299, 493)\n"
     ]
    }
   ],
   "source": [
    "print(X_sent.shape)\n",
    "print(U_sent.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = ibc_database[1]\n",
    "Y_u = bias_sentences['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = list(Y)\n",
    "Y_u = list(Y_u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final labels\n",
    "Y_label = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "    if Y[i] == 'liberal':\n",
    "        Y_label.append(0)\n",
    "    elif Y[i] == 'neutral':\n",
    "        Y_label.append(1)\n",
    "    else:\n",
    "        Y_label.append(2)\n",
    "\n",
    "Y_u_label = []\n",
    "\n",
    "for i in range(len(Y_u)):\n",
    "    if Y_u[i] == 'liberal':\n",
    "        Y_u_label.append(0)\n",
    "    elif Y_u[i] == 'neutral':\n",
    "        Y_u_label.append(1)\n",
    "    else:\n",
    "        Y_u_label.append(2)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_w = ibc_database['weak_label']\n",
    "\n",
    "Y_w =list(Y_w)\n",
    "Y_weak = []\n",
    "for i in range(len(Y)):\n",
    "    if Y_w[i] == 'liberal':\n",
    "        Y_weak.append(0)\n",
    "    elif Y_w[i] == 'neutral':\n",
    "        Y_weak.append(1)\n",
    "    else:\n",
    "        Y_weak.append(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = []\n",
    "for i in range(X_sent.shape[0]):\n",
    "    X_sent_w.append(np.append(X_sent[i], [Y_weak[i],ns[i]]))\n",
    "\n",
    "U_sent_w = []\n",
    "for i in range(U_sent.shape[0]):\n",
    "    U_sent_w.append(np.append(U_sent[i], [u_ns[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sent_w = np.reshape(X_sent_w, newshape= (X_sent.shape[0],X_sent.shape[1]+2))\n",
    "U_sent_w = np.reshape(U_sent_w, newshape= (U_sent.shape[0],X_sent.shape[1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train,X_test,y_train, y_test = train_test_split(X_sent_w, Y_label, test_size=0.2, random_state=42)\n",
    "U_train,U_test,y_u_train, y_u_test = train_test_split(U_sent_w, Y_u_label, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns_train = []\n",
    "ns_test = []\n",
    "\n",
    "ns_u_train = []\n",
    "ns_u_test = []\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    ns_train.append(X_train[i][-1])\n",
    "for i in range(X_test.shape[0]):\n",
    "    ns_test.append(X_test[i][-1])\n",
    "    \n",
    "for i in range(U_train.shape[0]):\n",
    "    ns_u_train.append(U_train[i][-1])\n",
    "for i in range(U_test.shape[0]):\n",
    "    ns_u_test.append(U_test[i][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_n_train = np.delete(X_train,len(X_train[1])-1,axis=1)\n",
    "X_n_test  = np.delete(X_test,len(X_test[1])-1,axis=1)\n",
    "\n",
    "U_n_train = np.delete(U_train,len(U_train[1])-1,axis=1)\n",
    "U_n_test  = np.delete(U_test,len(U_test[1])-1,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we define our model\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def embedding_layer(ids_, V, embed_dim, init_scale=0.001):\n",
    "    \n",
    "    W_embed_ = tf.get_variable(name = 'W_embed', shape=[V,embed_dim],dtype=tf.float32\n",
    "                               ,initializer=tf.random_uniform_initializer(minval= -init_scale\n",
    "                                                                          ,maxval =init_scale),trainable=True)\n",
    "    xs_ = tf.nn.embedding_lookup(W_embed_, ids_)\n",
    "    return xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fully_connected_layers(h0_, hidden_dims, activation=tf.tanh,\n",
    "                           dropout_rate=0, is_training=False):\n",
    "    h_ = h0_\n",
    "    for i, hdim in enumerate(hidden_dims):\n",
    "        \n",
    "        h_ = tf.layers.dense(h_, hdim, activation=activation,    \n",
    "                             kernel_initializer=tf.contrib.layers.xavier_initializer()\n",
    "                             , bias_initializer=tf.zeros_initializer(), name = 'Hidden_%d'%i)\n",
    "        \n",
    "        if dropout_rate > 0:\n",
    "            h_ = tf.nn.dropout(h_, keep_prob=dropout_rate)\n",
    "            \n",
    "        #if dropout_rate > 0:\n",
    "            #h_ = tf.layers.dropout(inputs = h_, rate=dropout_rate, training = is_training)\n",
    "        \n",
    "    return h_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer(h_, labels_, num_classes):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        loss_ = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BOW_encoder(ids_, ns_, V, embed_dim, hidden_dims, dropout_rate=0,is_training=None,\n",
    "                **unused_kw):\n",
    "    \n",
    "    assert is_training is not None, \"is_training must be explicitly set to True or False\"\n",
    "    \n",
    "    with tf.variable_scope(\"Embedding_Layer\"):\n",
    "        xs_ = embedding_layer(ids_, V, embed_dim, init_scale=0.001) \n",
    "    mask_ = tf.expand_dims(tf.sequence_mask(ns_, xs_.shape[1],dtype=tf.float32), -1)\n",
    "    xs_ =  mask_ * xs_\n",
    "    x_ = tf.reduce_sum(xs_,axis = 1)\n",
    "    h_ = fully_connected_layers(x_, hidden_dims, activation=tf.tanh, dropout_rate=dropout_rate,is_training=is_training)   \n",
    "        \n",
    "    return h_, xs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer(h_, labels, params['num_classes'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"Regularization\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  500 examples, moving-average loss 0.74\n",
      "1,000 examples, moving-average loss 0.71\n",
      "1,500 examples, moving-average loss 0.66\n",
      "2,000 examples, moving-average loss 0.74\n",
      "2,500 examples, moving-average loss 0.69\n",
      "3,000 examples, moving-average loss 0.76\n",
      "Completed no. 1 epoch in 0:00:19\n",
      "  500 examples, moving-average loss 0.50\n",
      "1,000 examples, moving-average loss 0.53\n",
      "1,500 examples, moving-average loss 0.53\n",
      "2,000 examples, moving-average loss 0.59\n",
      "2,500 examples, moving-average loss 0.58\n",
      "3,000 examples, moving-average loss 0.68\n",
      "Completed no. 2 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.37\n",
      "1,000 examples, moving-average loss 0.40\n",
      "1,500 examples, moving-average loss 0.40\n",
      "2,000 examples, moving-average loss 0.40\n",
      "2,500 examples, moving-average loss 0.42\n",
      "3,000 examples, moving-average loss 0.49\n",
      "Completed no. 3 epoch in 0:00:21\n",
      "  500 examples, moving-average loss 0.29\n",
      "1,000 examples, moving-average loss 0.32\n",
      "1,500 examples, moving-average loss 0.31\n",
      "2,000 examples, moving-average loss 0.29\n",
      "2,500 examples, moving-average loss 0.34\n",
      "3,000 examples, moving-average loss 0.32\n",
      "Completed no. 4 epoch in 0:00:19\n",
      "  500 examples, moving-average loss 0.26\n",
      "1,000 examples, moving-average loss 0.28\n",
      "1,500 examples, moving-average loss 0.26\n",
      "2,000 examples, moving-average loss 0.26\n",
      "2,500 examples, moving-average loss 0.31\n",
      "3,000 examples, moving-average loss 0.27\n",
      "Completed no. 5 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.24\n",
      "1,000 examples, moving-average loss 0.26\n",
      "1,500 examples, moving-average loss 0.25\n",
      "2,000 examples, moving-average loss 0.24\n",
      "2,500 examples, moving-average loss 0.29\n",
      "3,000 examples, moving-average loss 0.25\n",
      "Completed no. 6 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.24\n",
      "1,000 examples, moving-average loss 0.26\n",
      "1,500 examples, moving-average loss 0.24\n",
      "2,000 examples, moving-average loss 0.24\n",
      "2,500 examples, moving-average loss 0.28\n",
      "3,000 examples, moving-average loss 0.24\n",
      "Completed no. 7 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.25\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.28\n",
      "3,000 examples, moving-average loss 0.24\n",
      "Completed no. 8 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.25\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.27\n",
      "3,000 examples, moving-average loss 0.23\n",
      "Completed no. 9 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.27\n",
      "3,000 examples, moving-average loss 0.23\n",
      "Completed no. 10 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.27\n",
      "3,000 examples, moving-average loss 0.23\n",
      "Completed no. 11 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.26\n",
      "3,000 examples, moving-average loss 0.23\n",
      "Completed no. 12 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.23\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.23\n",
      "2,000 examples, moving-average loss 0.23\n",
      "2,500 examples, moving-average loss 0.26\n",
      "3,000 examples, moving-average loss 0.23\n",
      "Completed no. 13 epoch in 0:00:20\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.26\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 14 epoch in 0:00:18\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.24\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.26\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 15 epoch in 0:00:17\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.23\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.25\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 16 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.23\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.25\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 17 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.23\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.25\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 18 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.23\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.25\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 19 epoch in 0:00:16\n",
      "  500 examples, moving-average loss 0.22\n",
      "1,000 examples, moving-average loss 0.23\n",
      "1,500 examples, moving-average loss 0.22\n",
      "2,000 examples, moving-average loss 0.22\n",
      "2,500 examples, moving-average loss 0.25\n",
      "3,000 examples, moving-average loss 0.22\n",
      "Completed no. 20 epoch in 0:00:16\n"
     ]
    }
   ],
   "source": [
    "# training the model\n",
    "\n",
    "batch_size = 5\n",
    "e = 175 # embedding dimension\n",
    "h = [32,64,128] # hidden dimensions\n",
    "l = 0.001 # learning rate\n",
    "num_epoch = 20 #no. of epochs\n",
    "\n",
    "# Specify model hyperparameters as used by model_fn\n",
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=2,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "model_fn = classifier_model_fn\n",
    "\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_train.shape[1]])  # [batch_size, max_len]\n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])              # [batch_size]\n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                              params=model_params)\n",
    "    loss_     = estimator_spec.loss\n",
    "    train_op_ = estimator_spec.train_op\n",
    "    \n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    total_loss = 0\n",
    "    loss_ema = np.log(2)  # track exponential-moving-average of loss\n",
    "    ema_decay = np.exp(-1/10)  # decay parameter for moving average = np.exp(-1/history_length)\n",
    "    \n",
    "    \n",
    "    y_conf = []\n",
    "     \n",
    "    for i in range(len(y_train)):\n",
    "        if y_train[i] == Y_weak[i]:\n",
    "            y_conf.append(1)\n",
    "        else:\n",
    "            y_conf.append(0)\n",
    "    \n",
    "    y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "    ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "    \n",
    "    for j in range(num_epoch):\n",
    "        \n",
    "        t0 = time.time()\n",
    "        total_batches = 0\n",
    "        total_examples = 0\n",
    "        \n",
    "        for (bx, bns, by) in utils.multi_batch_generator(batch_size, X_n_train, ns_train, y_conf):\n",
    "            # feed NumPy arrays into the placeholder Tensors\n",
    "            feed_dict = {x_ph_: bx, ns_ph_: bns, y_ph_: by}\n",
    "            batch_loss, _ = sess.run([loss_, train_op_], feed_dict=feed_dict)\n",
    "        \n",
    "            # Compute some statistics\n",
    "            total_batches += 1\n",
    "            total_examples += len(bx)\n",
    "            total_loss += batch_loss * len(bx)  # re-scale, since batch loss is mean\n",
    "            # Compute moving average to smooth out noisy per-batch loss\n",
    "            loss_ema = ema_decay * loss_ema + (1 - ema_decay) * batch_loss\n",
    "        \n",
    "            if (total_batches % 100 == 0):\n",
    "                print(\"{:5,} examples, moving-average loss {:.2f}\".format(total_examples, \n",
    "                                                                      loss_ema))    \n",
    "        print(\"Completed no. {:d} epoch in {:s}\".format(j+1,utils.pretty_timedelta(since=t0)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Graph().as_default(), tf.Session() as sess:\n",
    "    \n",
    "    x_ph_  = tf.placeholder(tf.int32, shape=[None, X_n_test.shape[1]])  \n",
    "    ns_ph_ = tf.placeholder(tf.int32, shape=[None])              \n",
    "    y_ph_  = tf.placeholder(tf.int32, shape=[None])             \n",
    "    \n",
    "    # Construct the graph using model_fn\n",
    "    features = {\"ids\": x_ph_, \"ns\": ns_ph_}  # note that values are Tensors\n",
    "    estimator_spec = model_fn(features, labels=y_ph_, mode=tf.estimator.ModeKeys.PREDICT,\n",
    "                              params=model_params)\n",
    "    predict_ = estimator_spec.predictions\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    t = X_n_train.shape[0]\n",
    "    \n",
    "    true_conf_score = []\n",
    "    for i in range(X_n_test.shape[0]):\n",
    "        if y_test[i]==Y_weak[t+i]:\n",
    "            true_conf_score.append(1)\n",
    "        else: \n",
    "            true_conf_score.append(0)\n",
    "    \n",
    "    \n",
    "    feed_dict = {x_ph_: X_n_test, ns_ph_: ns_test, y_ph_: true_conf_score}\n",
    "    predictions = sess.run(predict_, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "716"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_conf_score.count(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_conf_score = predictions['max']\n",
    "pred_conf_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6472273 0.3527727]  test_label: 2  weak_label: 2\n",
      "[0.656216 0.343784]  test_label: 0  weak_label: 1\n",
      "[0.6283817  0.37161827]  test_label: 2  weak_label: 1\n",
      "[0.6267714  0.37322864]  test_label: 2  weak_label: 1\n",
      "[0.64782465 0.35217535]  test_label: 0  weak_label: 1\n",
      "[0.62867707 0.37132287]  test_label: 2  weak_label: 1\n",
      "[0.62146634 0.37853366]  test_label: 0  weak_label: 1\n",
      "[0.640592  0.3594081]  test_label: 0  weak_label: 1\n",
      "[0.6461013  0.35389867]  test_label: 0  weak_label: 1\n",
      "[0.62043357 0.37956646]  test_label: 0  weak_label: 0\n",
      "[0.62229455 0.37770543]  test_label: 1  weak_label: 1\n",
      "[0.6403636  0.35963637]  test_label: 0  weak_label: 1\n",
      "[0.63509357 0.3649064 ]  test_label: 0  weak_label: 0\n",
      "[0.6447592 0.3552408]  test_label: 2  weak_label: 1\n",
      "[0.6542419  0.34575808]  test_label: 2  weak_label: 1\n",
      "[0.6334456 0.3665544]  test_label: 2  weak_label: 1\n",
      "[0.6225757 0.3774243]  test_label: 2  weak_label: 1\n",
      "[0.62681633 0.37318373]  test_label: 0  weak_label: 1\n",
      "[0.64406806 0.3559319 ]  test_label: 2  weak_label: 1\n",
      "[0.6282849  0.37171516]  test_label: 1  weak_label: 1\n",
      "[0.6316417  0.36835828]  test_label: 2  weak_label: 1\n",
      "[0.637321   0.36267903]  test_label: 2  weak_label: 0\n",
      "[0.6494623  0.35053772]  test_label: 2  weak_label: 1\n",
      "[0.62181205 0.37818792]  test_label: 2  weak_label: 2\n",
      "[0.6275898  0.37241018]  test_label: 1  weak_label: 1\n",
      "[0.63371474 0.36628526]  test_label: 1  weak_label: 1\n",
      "[0.6218002  0.37819982]  test_label: 2  weak_label: 1\n",
      "[0.65134245 0.34865758]  test_label: 1  weak_label: 1\n",
      "[0.6523012  0.34769884]  test_label: 0  weak_label: 1\n",
      "[0.6505585 0.3494415]  test_label: 0  weak_label: 0\n",
      "[0.62863386 0.3713661 ]  test_label: 2  weak_label: 2\n",
      "[0.63601375 0.36398625]  test_label: 2  weak_label: 1\n",
      "[0.65026134 0.34973866]  test_label: 1  weak_label: 1\n",
      "[0.6075113  0.39248878]  test_label: 2  weak_label: 1\n",
      "[0.6466285  0.35337147]  test_label: 2  weak_label: 1\n",
      "[0.6330255  0.36697447]  test_label: 2  weak_label: 0\n",
      "[0.6316317  0.36836833]  test_label: 2  weak_label: 1\n",
      "[0.6210423 0.3789577]  test_label: 2  weak_label: 1\n",
      "[0.63992727 0.3600727 ]  test_label: 2  weak_label: 1\n",
      "[0.64008754 0.35991243]  test_label: 0  weak_label: 0\n",
      "[0.6331598  0.36684012]  test_label: 0  weak_label: 1\n",
      "[0.6264838  0.37351617]  test_label: 0  weak_label: 1\n",
      "[0.62954426 0.37045574]  test_label: 0  weak_label: 1\n",
      "[0.64354163 0.35645828]  test_label: 1  weak_label: 1\n",
      "[0.641797   0.35820293]  test_label: 1  weak_label: 1\n",
      "[0.65526485 0.3447351 ]  test_label: 2  weak_label: 1\n",
      "[0.64349896 0.35650104]  test_label: 2  weak_label: 1\n",
      "[0.61728615 0.3827138 ]  test_label: 2  weak_label: 1\n",
      "[0.61696494 0.3830351 ]  test_label: 2  weak_label: 2\n",
      "[0.62148404 0.378516  ]  test_label: 2  weak_label: 2\n",
      "[0.6452998  0.35470027]  test_label: 0  weak_label: 1\n",
      "[0.6369194  0.36308056]  test_label: 0  weak_label: 1\n",
      "[0.6299978 0.3700022]  test_label: 0  weak_label: 1\n",
      "[0.63772804 0.3622719 ]  test_label: 0  weak_label: 1\n",
      "[0.6329283 0.3670717]  test_label: 1  weak_label: 0\n",
      "[0.6494749  0.35052508]  test_label: 0  weak_label: 1\n",
      "[0.6491552 0.3508448]  test_label: 1  weak_label: 2\n",
      "[0.61621314 0.38378683]  test_label: 0  weak_label: 0\n",
      "[0.6295111  0.37048885]  test_label: 0  weak_label: 1\n",
      "[0.6453629  0.35463706]  test_label: 2  weak_label: 2\n",
      "[0.61769396 0.38230607]  test_label: 0  weak_label: 1\n",
      "[0.6481885  0.35181156]  test_label: 1  weak_label: 0\n",
      "[0.6295591 0.3704409]  test_label: 0  weak_label: 1\n",
      "[0.6414602  0.35853982]  test_label: 2  weak_label: 0\n",
      "[0.65442824 0.34557173]  test_label: 2  weak_label: 1\n",
      "[0.62841165 0.37158838]  test_label: 0  weak_label: 1\n",
      "[0.6694292  0.33057082]  test_label: 0  weak_label: 1\n",
      "[0.6441039  0.35589615]  test_label: 2  weak_label: 1\n",
      "[0.65783656 0.3421634 ]  test_label: 0  weak_label: 1\n",
      "[0.63267606 0.36732396]  test_label: 0  weak_label: 1\n",
      "[0.62967104 0.37032896]  test_label: 0  weak_label: 1\n",
      "[0.6232721  0.37672794]  test_label: 2  weak_label: 1\n",
      "[0.6152434 0.3847567]  test_label: 2  weak_label: 1\n",
      "[0.64743835 0.35256165]  test_label: 2  weak_label: 1\n",
      "[0.6717905  0.32820952]  test_label: 0  weak_label: 1\n",
      "[0.6233703  0.37662974]  test_label: 0  weak_label: 2\n",
      "[0.6463928  0.35360715]  test_label: 2  weak_label: 0\n",
      "[0.6150491  0.38495088]  test_label: 0  weak_label: 0\n",
      "[0.6133349 0.3866652]  test_label: 0  weak_label: 1\n",
      "[0.64361066 0.35638937]  test_label: 2  weak_label: 1\n",
      "[0.6537416  0.34625846]  test_label: 0  weak_label: 0\n",
      "[0.6510071  0.34899285]  test_label: 0  weak_label: 1\n",
      "[0.6783219  0.32167813]  test_label: 2  weak_label: 1\n",
      "[0.6406344 0.3593656]  test_label: 0  weak_label: 1\n",
      "[0.6400815  0.35991848]  test_label: 0  weak_label: 1\n",
      "[0.6419127  0.35808733]  test_label: 0  weak_label: 1\n",
      "[0.6347851 0.3652149]  test_label: 2  weak_label: 1\n",
      "[0.629397   0.37060302]  test_label: 2  weak_label: 1\n",
      "[0.6506489  0.34935108]  test_label: 1  weak_label: 0\n",
      "[0.6334639  0.36653608]  test_label: 2  weak_label: 1\n",
      "[0.64847374 0.3515263 ]  test_label: 2  weak_label: 1\n",
      "[0.6237518  0.37624818]  test_label: 0  weak_label: 2\n",
      "[0.63464767 0.36535236]  test_label: 0  weak_label: 0\n",
      "[0.61585647 0.3841435 ]  test_label: 0  weak_label: 1\n",
      "[0.63599294 0.36400703]  test_label: 0  weak_label: 1\n",
      "[0.61041325 0.38958675]  test_label: 1  weak_label: 1\n",
      "[0.61695796 0.38304213]  test_label: 2  weak_label: 1\n",
      "[0.66212904 0.337871  ]  test_label: 2  weak_label: 0\n",
      "[0.62403005 0.37596995]  test_label: 0  weak_label: 1\n",
      "[0.6081448  0.39185518]  test_label: 2  weak_label: 2\n",
      "[0.63659304 0.36340702]  test_label: 2  weak_label: 0\n",
      "[0.6223382  0.37766185]  test_label: 0  weak_label: 1\n",
      "[0.6363577 0.3636423]  test_label: 0  weak_label: 1\n",
      "[0.64019674 0.35980326]  test_label: 0  weak_label: 1\n",
      "[0.61213565 0.38786432]  test_label: 1  weak_label: 1\n",
      "[0.630792 0.369208]  test_label: 1  weak_label: 1\n",
      "[0.62594235 0.37405762]  test_label: 2  weak_label: 1\n",
      "[0.6389148  0.36108512]  test_label: 0  weak_label: 1\n",
      "[0.6220169  0.37798318]  test_label: 2  weak_label: 1\n",
      "[0.6402321 0.3597679]  test_label: 1  weak_label: 1\n",
      "[0.6556079 0.3443921]  test_label: 0  weak_label: 1\n",
      "[0.63099927 0.3690007 ]  test_label: 0  weak_label: 0\n",
      "[0.6404546  0.35954544]  test_label: 2  weak_label: 1\n",
      "[0.6401405  0.35985956]  test_label: 0  weak_label: 1\n",
      "[0.65892667 0.3410734 ]  test_label: 2  weak_label: 1\n",
      "[0.63375413 0.36624584]  test_label: 2  weak_label: 1\n",
      "[0.64447594 0.35552406]  test_label: 0  weak_label: 1\n",
      "[0.6353698 0.3646302]  test_label: 0  weak_label: 1\n",
      "[0.63783586 0.36216414]  test_label: 1  weak_label: 1\n",
      "[0.6403358  0.35966414]  test_label: 0  weak_label: 1\n",
      "[0.6326321 0.3673679]  test_label: 0  weak_label: 0\n",
      "[0.6558127  0.34418735]  test_label: 0  weak_label: 1\n",
      "[0.6260871  0.37391284]  test_label: 0  weak_label: 1\n",
      "[0.6164459  0.38355416]  test_label: 0  weak_label: 1\n",
      "[0.6409673  0.35903266]  test_label: 0  weak_label: 1\n",
      "[0.6207334 0.3792666]  test_label: 0  weak_label: 1\n",
      "[0.6313191  0.36868086]  test_label: 2  weak_label: 1\n",
      "[0.6322265 0.3677735]  test_label: 2  weak_label: 2\n",
      "[0.64405394 0.35594606]  test_label: 2  weak_label: 1\n",
      "[0.6328575  0.36714247]  test_label: 0  weak_label: 1\n",
      "[0.61672723 0.38327277]  test_label: 2  weak_label: 1\n",
      "[0.633514   0.36648604]  test_label: 2  weak_label: 1\n",
      "[0.634006   0.36599395]  test_label: 0  weak_label: 1\n",
      "[0.63297707 0.3670229 ]  test_label: 0  weak_label: 2\n",
      "[0.62048393 0.37951604]  test_label: 0  weak_label: 1\n",
      "[0.6586789 0.3413211]  test_label: 0  weak_label: 1\n",
      "[0.64725274 0.35274726]  test_label: 2  weak_label: 0\n",
      "[0.63040614 0.36959392]  test_label: 0  weak_label: 0\n",
      "[0.6386005 0.3613995]  test_label: 2  weak_label: 1\n",
      "[0.64015734 0.35984263]  test_label: 0  weak_label: 1\n",
      "[0.6497678  0.35023212]  test_label: 2  weak_label: 1\n",
      "[0.63404065 0.36595932]  test_label: 0  weak_label: 1\n",
      "[0.6488215  0.35117856]  test_label: 1  weak_label: 1\n",
      "[0.6356021  0.36439794]  test_label: 2  weak_label: 1\n",
      "[0.6176262  0.38237384]  test_label: 1  weak_label: 1\n",
      "[0.63541955 0.36458042]  test_label: 2  weak_label: 1\n",
      "[0.63254535 0.36745465]  test_label: 2  weak_label: 1\n",
      "[0.64032614 0.35967386]  test_label: 2  weak_label: 2\n",
      "[0.63112503 0.36887494]  test_label: 2  weak_label: 1\n",
      "[0.6506902  0.34930983]  test_label: 2  weak_label: 1\n",
      "[0.6431287 0.3568713]  test_label: 2  weak_label: 1\n",
      "[0.6305941  0.36940596]  test_label: 0  weak_label: 1\n",
      "[0.62655663 0.37344337]  test_label: 2  weak_label: 1\n",
      "[0.63323367 0.3667663 ]  test_label: 2  weak_label: 1\n",
      "[0.6150509  0.38494912]  test_label: 0  weak_label: 1\n",
      "[0.63852316 0.36147687]  test_label: 2  weak_label: 1\n",
      "[0.6438064 0.3561936]  test_label: 0  weak_label: 0\n",
      "[0.6237022 0.3762978]  test_label: 1  weak_label: 1\n",
      "[0.6385107  0.36148927]  test_label: 0  weak_label: 1\n",
      "[0.6543794 0.3456206]  test_label: 2  weak_label: 1\n",
      "[0.62564373 0.37435627]  test_label: 1  weak_label: 1\n",
      "[0.6248278  0.37517223]  test_label: 0  weak_label: 1\n",
      "[0.65670586 0.34329417]  test_label: 2  weak_label: 1\n",
      "[0.67327327 0.3267267 ]  test_label: 0  weak_label: 1\n",
      "[0.6353359 0.3646641]  test_label: 0  weak_label: 1\n",
      "[0.6320418  0.36795816]  test_label: 1  weak_label: 1\n",
      "[0.6207456  0.37925437]  test_label: 0  weak_label: 1\n",
      "[0.63183683 0.3681632 ]  test_label: 2  weak_label: 0\n",
      "[0.63829714 0.36170286]  test_label: 1  weak_label: 1\n",
      "[0.6376587  0.36234128]  test_label: 0  weak_label: 1\n",
      "[0.64978796 0.35021204]  test_label: 2  weak_label: 1\n",
      "[0.60571575 0.39428428]  test_label: 0  weak_label: 1\n",
      "[0.60769403 0.39230603]  test_label: 2  weak_label: 1\n",
      "[0.6537896  0.34621045]  test_label: 2  weak_label: 1\n",
      "[0.62035894 0.37964103]  test_label: 2  weak_label: 1\n",
      "[0.6127542  0.38724574]  test_label: 2  weak_label: 1\n",
      "[0.6250246  0.37497532]  test_label: 0  weak_label: 1\n",
      "[0.63077855 0.36922145]  test_label: 1  weak_label: 1\n",
      "[0.63968784 0.3603121 ]  test_label: 2  weak_label: 1\n",
      "[0.63306856 0.36693144]  test_label: 2  weak_label: 1\n",
      "[0.63194674 0.36805326]  test_label: 0  weak_label: 1\n",
      "[0.6251997  0.37480032]  test_label: 2  weak_label: 1\n",
      "[0.6325121  0.36748797]  test_label: 2  weak_label: 1\n",
      "[0.63188916 0.3681108 ]  test_label: 2  weak_label: 1\n",
      "[0.6327351 0.3672649]  test_label: 2  weak_label: 0\n",
      "[0.62677616 0.37322384]  test_label: 2  weak_label: 1\n",
      "[0.6266828 0.3733172]  test_label: 2  weak_label: 1\n",
      "[0.6296728  0.37032714]  test_label: 0  weak_label: 1\n",
      "[0.64832044 0.3516795 ]  test_label: 1  weak_label: 0\n",
      "[0.65775543 0.34224448]  test_label: 2  weak_label: 1\n",
      "[0.6471633 0.3528367]  test_label: 0  weak_label: 0\n",
      "[0.61628246 0.38371745]  test_label: 0  weak_label: 1\n",
      "[0.62969494 0.37030506]  test_label: 2  weak_label: 1\n",
      "[0.6211479 0.3788522]  test_label: 2  weak_label: 0\n",
      "[0.64246935 0.35753068]  test_label: 0  weak_label: 1\n",
      "[0.6188948  0.38110518]  test_label: 2  weak_label: 1\n",
      "[0.63543457 0.3645655 ]  test_label: 0  weak_label: 2\n",
      "[0.6314576  0.36854228]  test_label: 0  weak_label: 1\n",
      "[0.638579   0.36142102]  test_label: 2  weak_label: 1\n",
      "[0.6378965  0.36210346]  test_label: 2  weak_label: 1\n",
      "[0.6128873  0.38711277]  test_label: 1  weak_label: 1\n",
      "[0.63835925 0.36164075]  test_label: 0  weak_label: 1\n",
      "[0.6468168  0.35318318]  test_label: 1  weak_label: 0\n",
      "[0.664108   0.33589205]  test_label: 0  weak_label: 1\n",
      "[0.64294183 0.35705817]  test_label: 0  weak_label: 1\n",
      "[0.65153074 0.3484693 ]  test_label: 2  weak_label: 1\n",
      "[0.6566846  0.34331542]  test_label: 2  weak_label: 1\n",
      "[0.6266269  0.37337312]  test_label: 2  weak_label: 1\n",
      "[0.62850875 0.3714913 ]  test_label: 0  weak_label: 1\n",
      "[0.6283257 0.3716743]  test_label: 0  weak_label: 0\n",
      "[0.64931077 0.35068926]  test_label: 2  weak_label: 1\n",
      "[0.5979045 0.4020955]  test_label: 2  weak_label: 1\n",
      "[0.64174044 0.35825953]  test_label: 2  weak_label: 1\n",
      "[0.62931085 0.3706891 ]  test_label: 0  weak_label: 1\n",
      "[0.6396757  0.36032432]  test_label: 0  weak_label: 1\n",
      "[0.62977606 0.37022394]  test_label: 2  weak_label: 1\n",
      "[0.620765   0.37923503]  test_label: 0  weak_label: 1\n",
      "[0.621795   0.37820503]  test_label: 2  weak_label: 0\n",
      "[0.61186355 0.3881364 ]  test_label: 0  weak_label: 1\n",
      "[0.6237411  0.37625888]  test_label: 2  weak_label: 1\n",
      "[0.63025564 0.36974436]  test_label: 0  weak_label: 1\n",
      "[0.6196781  0.38032198]  test_label: 1  weak_label: 1\n",
      "[0.636559   0.36344096]  test_label: 0  weak_label: 1\n",
      "[0.63338596 0.36661407]  test_label: 1  weak_label: 0\n",
      "[0.6429869  0.35701308]  test_label: 2  weak_label: 2\n",
      "[0.66265523 0.3373448 ]  test_label: 0  weak_label: 1\n",
      "[0.6460162  0.35398385]  test_label: 1  weak_label: 1\n",
      "[0.6432022  0.35679784]  test_label: 0  weak_label: 1\n",
      "[0.62098557 0.3790144 ]  test_label: 0  weak_label: 1\n",
      "[0.63587856 0.36412147]  test_label: 0  weak_label: 0\n",
      "[0.62873423 0.37126577]  test_label: 0  weak_label: 1\n",
      "[0.6613799  0.33862016]  test_label: 0  weak_label: 1\n",
      "[0.6183369  0.38166305]  test_label: 1  weak_label: 1\n",
      "[0.62633526 0.37366483]  test_label: 2  weak_label: 1\n",
      "[0.6362428  0.36375716]  test_label: 0  weak_label: 1\n",
      "[0.63326025 0.36673975]  test_label: 0  weak_label: 1\n",
      "[0.6268161  0.37318388]  test_label: 0  weak_label: 1\n",
      "[0.6165289  0.38347107]  test_label: 1  weak_label: 1\n",
      "[0.6471798 0.3528202]  test_label: 0  weak_label: 1\n",
      "[0.6567954  0.34320462]  test_label: 0  weak_label: 1\n",
      "[0.64313716 0.35686287]  test_label: 0  weak_label: 1\n",
      "[0.63265914 0.36734083]  test_label: 0  weak_label: 1\n",
      "[0.6465576  0.35344243]  test_label: 0  weak_label: 1\n",
      "[0.65542716 0.34457284]  test_label: 0  weak_label: 1\n",
      "[0.61240506 0.3875949 ]  test_label: 0  weak_label: 1\n",
      "[0.63768935 0.3623107 ]  test_label: 2  weak_label: 1\n",
      "[0.6405255  0.35947442]  test_label: 0  weak_label: 1\n",
      "[0.61988777 0.38011214]  test_label: 1  weak_label: 1\n",
      "[0.6411256 0.3588744]  test_label: 2  weak_label: 1\n",
      "[0.656921 0.343079]  test_label: 0  weak_label: 2\n",
      "[0.64540464 0.3545954 ]  test_label: 0  weak_label: 1\n",
      "[0.64020836 0.35979158]  test_label: 0  weak_label: 1\n",
      "[0.6250929 0.3749071]  test_label: 2  weak_label: 0\n",
      "[0.6331978  0.36680222]  test_label: 0  weak_label: 1\n",
      "[0.64614785 0.35385218]  test_label: 0  weak_label: 1\n",
      "[0.61907685 0.38092318]  test_label: 1  weak_label: 1\n",
      "[0.65505826 0.3449417 ]  test_label: 1  weak_label: 1\n",
      "[0.62115604 0.378844  ]  test_label: 0  weak_label: 1\n",
      "[0.6325936 0.3674065]  test_label: 2  weak_label: 1\n",
      "[0.63559395 0.36440608]  test_label: 0  weak_label: 1\n",
      "[0.627972   0.37202802]  test_label: 0  weak_label: 1\n",
      "[0.63010967 0.3698903 ]  test_label: 2  weak_label: 1\n",
      "[0.6563741  0.34362587]  test_label: 2  weak_label: 1\n",
      "[0.63627255 0.36372754]  test_label: 2  weak_label: 1\n",
      "[0.64056224 0.3594378 ]  test_label: 0  weak_label: 1\n",
      "[0.6346704 0.3653296]  test_label: 0  weak_label: 1\n",
      "[0.64025813 0.3597419 ]  test_label: 1  weak_label: 1\n",
      "[0.6259129 0.3740871]  test_label: 0  weak_label: 1\n",
      "[0.6202938  0.37970617]  test_label: 0  weak_label: 1\n",
      "[0.62881505 0.3711849 ]  test_label: 0  weak_label: 1\n",
      "[0.64586776 0.35413224]  test_label: 2  weak_label: 1\n",
      "[0.62219226 0.37780777]  test_label: 2  weak_label: 1\n",
      "[0.62456226 0.3754378 ]  test_label: 0  weak_label: 1\n",
      "[0.62920445 0.3707955 ]  test_label: 0  weak_label: 1\n",
      "[0.6234157  0.37658435]  test_label: 2  weak_label: 1\n",
      "[0.6291124 0.3708875]  test_label: 2  weak_label: 1\n",
      "[0.64049476 0.35950524]  test_label: 0  weak_label: 1\n",
      "[0.64194626 0.35805374]  test_label: 0  weak_label: 1\n",
      "[0.6557469  0.34425306]  test_label: 1  weak_label: 1\n",
      "[0.64563125 0.35436872]  test_label: 2  weak_label: 1\n",
      "[0.64470345 0.35529655]  test_label: 1  weak_label: 1\n",
      "[0.6366199  0.36338004]  test_label: 0  weak_label: 1\n",
      "[0.6496276  0.35037234]  test_label: 1  weak_label: 1\n",
      "[0.6504933  0.34950665]  test_label: 1  weak_label: 1\n",
      "[0.64163375 0.35836628]  test_label: 2  weak_label: 1\n",
      "[0.63859993 0.36140004]  test_label: 1  weak_label: 1\n",
      "[0.6453487 0.3546513]  test_label: 0  weak_label: 1\n",
      "[0.6050661  0.39493385]  test_label: 2  weak_label: 1\n",
      "[0.6211937 0.3788063]  test_label: 0  weak_label: 1\n",
      "[0.6374087  0.36259136]  test_label: 0  weak_label: 1\n",
      "[0.62499505 0.37500495]  test_label: 1  weak_label: 1\n",
      "[0.6546862  0.34531382]  test_label: 2  weak_label: 1\n",
      "[0.6236065  0.37639353]  test_label: 0  weak_label: 1\n",
      "[0.63310784 0.3668922 ]  test_label: 0  weak_label: 1\n",
      "[0.6253369  0.37466305]  test_label: 0  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6269115  0.37308842]  test_label: 2  weak_label: 1\n",
      "[0.6230406 0.3769594]  test_label: 0  weak_label: 1\n",
      "[0.6349419  0.36505818]  test_label: 2  weak_label: 1\n",
      "[0.64329094 0.35670912]  test_label: 2  weak_label: 1\n",
      "[0.63036984 0.3696302 ]  test_label: 2  weak_label: 1\n",
      "[0.6526043  0.34739572]  test_label: 0  weak_label: 1\n",
      "[0.62266964 0.3773304 ]  test_label: 0  weak_label: 1\n",
      "[0.6151637  0.38483638]  test_label: 0  weak_label: 1\n",
      "[0.6534483  0.34655172]  test_label: 2  weak_label: 1\n",
      "[0.66679597 0.33320406]  test_label: 0  weak_label: 1\n",
      "[0.62140113 0.37859893]  test_label: 0  weak_label: 0\n",
      "[0.6522335 0.3477666]  test_label: 0  weak_label: 1\n",
      "[0.6268697  0.37313035]  test_label: 0  weak_label: 1\n",
      "[0.6441465  0.35585347]  test_label: 2  weak_label: 1\n",
      "[0.6345181 0.3654819]  test_label: 0  weak_label: 1\n",
      "[0.62630355 0.37369648]  test_label: 2  weak_label: 1\n",
      "[0.6266446  0.37335542]  test_label: 1  weak_label: 2\n",
      "[0.6466268  0.35337323]  test_label: 0  weak_label: 1\n",
      "[0.64367133 0.35632867]  test_label: 1  weak_label: 1\n",
      "[0.6356237  0.36437628]  test_label: 0  weak_label: 1\n",
      "[0.6281247  0.37187526]  test_label: 0  weak_label: 1\n",
      "[0.639989   0.36001098]  test_label: 2  weak_label: 1\n",
      "[0.6428626 0.3571374]  test_label: 2  weak_label: 0\n",
      "[0.6155562  0.38444385]  test_label: 1  weak_label: 1\n",
      "[0.647674 0.352326]  test_label: 2  weak_label: 1\n",
      "[0.6140592 0.3859408]  test_label: 0  weak_label: 1\n",
      "[0.6255681  0.37443194]  test_label: 2  weak_label: 1\n",
      "[0.65252835 0.34747168]  test_label: 0  weak_label: 1\n",
      "[0.63539135 0.3646087 ]  test_label: 2  weak_label: 1\n",
      "[0.62203413 0.37796584]  test_label: 0  weak_label: 1\n",
      "[0.6296355 0.3703645]  test_label: 2  weak_label: 1\n",
      "[0.6282691 0.3717309]  test_label: 0  weak_label: 1\n",
      "[0.65775377 0.34224623]  test_label: 2  weak_label: 1\n",
      "[0.62469274 0.3753072 ]  test_label: 0  weak_label: 1\n",
      "[0.61893195 0.38106808]  test_label: 0  weak_label: 1\n",
      "[0.62722963 0.3727704 ]  test_label: 2  weak_label: 1\n",
      "[0.6296491  0.37035087]  test_label: 1  weak_label: 0\n",
      "[0.63069624 0.36930382]  test_label: 0  weak_label: 1\n",
      "[0.6387924  0.36120763]  test_label: 1  weak_label: 1\n",
      "[0.65726316 0.34273684]  test_label: 1  weak_label: 1\n",
      "[0.6232551 0.3767449]  test_label: 2  weak_label: 1\n",
      "[0.66710997 0.33289003]  test_label: 2  weak_label: 1\n",
      "[0.6517912 0.3482088]  test_label: 0  weak_label: 1\n",
      "[0.6179618  0.38203818]  test_label: 0  weak_label: 1\n",
      "[0.62697387 0.3730261 ]  test_label: 0  weak_label: 1\n",
      "[0.6172991  0.38270095]  test_label: 0  weak_label: 1\n",
      "[0.64814055 0.35185948]  test_label: 0  weak_label: 1\n",
      "[0.6100242 0.3899758]  test_label: 1  weak_label: 1\n",
      "[0.63732046 0.36267957]  test_label: 0  weak_label: 1\n",
      "[0.6284266  0.37157333]  test_label: 0  weak_label: 1\n",
      "[0.63965625 0.36034375]  test_label: 2  weak_label: 1\n",
      "[0.6364161  0.36358395]  test_label: 0  weak_label: 1\n",
      "[0.64394826 0.3560517 ]  test_label: 2  weak_label: 1\n",
      "[0.6178835 0.3821165]  test_label: 0  weak_label: 1\n",
      "[0.6236414  0.37635866]  test_label: 1  weak_label: 1\n",
      "[0.6488037  0.35119626]  test_label: 0  weak_label: 1\n",
      "[0.62605315 0.37394682]  test_label: 1  weak_label: 1\n",
      "[0.62261105 0.37738895]  test_label: 0  weak_label: 1\n",
      "[0.6488552 0.3511448]  test_label: 2  weak_label: 1\n",
      "[0.6371762  0.36282378]  test_label: 0  weak_label: 1\n",
      "[0.62363017 0.3763699 ]  test_label: 2  weak_label: 1\n",
      "[0.6269672  0.37303284]  test_label: 1  weak_label: 1\n",
      "[0.638408   0.36159196]  test_label: 2  weak_label: 1\n",
      "[0.64402854 0.35597146]  test_label: 2  weak_label: 1\n",
      "[0.6296355 0.3703645]  test_label: 0  weak_label: 1\n",
      "[0.6421268  0.35787317]  test_label: 0  weak_label: 1\n",
      "[0.6522437  0.34775624]  test_label: 0  weak_label: 1\n",
      "[0.62941164 0.37058842]  test_label: 2  weak_label: 1\n",
      "[0.61777794 0.38222203]  test_label: 1  weak_label: 1\n",
      "[0.6324765 0.3675235]  test_label: 1  weak_label: 1\n",
      "[0.64756954 0.35243046]  test_label: 0  weak_label: 1\n",
      "[0.63552785 0.36447218]  test_label: 2  weak_label: 1\n",
      "[0.6389255 0.3610745]  test_label: 2  weak_label: 1\n",
      "[0.62072897 0.379271  ]  test_label: 2  weak_label: 1\n",
      "[0.66855055 0.33144945]  test_label: 0  weak_label: 1\n",
      "[0.6291386  0.37086138]  test_label: 1  weak_label: 1\n",
      "[0.63061345 0.36938652]  test_label: 2  weak_label: 1\n",
      "[0.6230946  0.37690544]  test_label: 1  weak_label: 1\n",
      "[0.6664468 0.3335532]  test_label: 0  weak_label: 0\n",
      "[0.627555   0.37244502]  test_label: 1  weak_label: 1\n",
      "[0.6265276 0.3734724]  test_label: 2  weak_label: 1\n",
      "[0.63223827 0.3677618 ]  test_label: 1  weak_label: 1\n",
      "[0.64173615 0.35826385]  test_label: 2  weak_label: 1\n",
      "[0.6477115  0.35228845]  test_label: 2  weak_label: 1\n",
      "[0.62912375 0.37087628]  test_label: 0  weak_label: 1\n",
      "[0.61150855 0.38849145]  test_label: 1  weak_label: 1\n",
      "[0.639628 0.360372]  test_label: 0  weak_label: 1\n",
      "[0.6435512 0.3564488]  test_label: 1  weak_label: 1\n",
      "[0.6252352  0.37476486]  test_label: 2  weak_label: 1\n",
      "[0.6324688  0.36753118]  test_label: 2  weak_label: 1\n",
      "[0.6443097  0.35569033]  test_label: 2  weak_label: 1\n",
      "[0.6322354  0.36776465]  test_label: 0  weak_label: 1\n",
      "[0.62688875 0.37311122]  test_label: 0  weak_label: 1\n",
      "[0.6572143 0.3427857]  test_label: 0  weak_label: 1\n",
      "[0.63445693 0.36554304]  test_label: 0  weak_label: 1\n",
      "[0.63387483 0.36612514]  test_label: 2  weak_label: 1\n",
      "[0.63251954 0.36748052]  test_label: 0  weak_label: 1\n",
      "[0.6228334  0.37716666]  test_label: 0  weak_label: 1\n",
      "[0.65461284 0.34538722]  test_label: 0  weak_label: 1\n",
      "[0.65466064 0.34533936]  test_label: 0  weak_label: 1\n",
      "[0.6350034 0.3649966]  test_label: 0  weak_label: 1\n",
      "[0.6380402  0.36195984]  test_label: 0  weak_label: 1\n",
      "[0.6499911 0.3500089]  test_label: 2  weak_label: 1\n",
      "[0.6274069  0.37259313]  test_label: 0  weak_label: 1\n",
      "[0.6130334 0.3869666]  test_label: 0  weak_label: 2\n",
      "[0.62646955 0.3735304 ]  test_label: 0  weak_label: 1\n",
      "[0.6263503  0.37364975]  test_label: 1  weak_label: 1\n",
      "[0.6252173 0.3747827]  test_label: 0  weak_label: 1\n",
      "[0.6268155  0.37318447]  test_label: 1  weak_label: 1\n",
      "[0.6328872  0.36711282]  test_label: 2  weak_label: 1\n",
      "[0.6192069  0.38079306]  test_label: 0  weak_label: 1\n",
      "[0.6592852  0.34071484]  test_label: 2  weak_label: 1\n",
      "[0.63951683 0.3604832 ]  test_label: 0  weak_label: 1\n",
      "[0.64301455 0.35698545]  test_label: 2  weak_label: 1\n",
      "[0.62471133 0.3752887 ]  test_label: 2  weak_label: 1\n",
      "[0.63190436 0.36809567]  test_label: 0  weak_label: 1\n",
      "[0.61723113 0.3827689 ]  test_label: 2  weak_label: 1\n",
      "[0.63265216 0.36734793]  test_label: 2  weak_label: 1\n",
      "[0.6326304 0.3673696]  test_label: 1  weak_label: 1\n",
      "[0.632966   0.36703402]  test_label: 0  weak_label: 1\n",
      "[0.6404921  0.35950798]  test_label: 1  weak_label: 1\n",
      "[0.6369622 0.3630378]  test_label: 0  weak_label: 1\n",
      "[0.63195944 0.36804053]  test_label: 0  weak_label: 1\n",
      "[0.6304877 0.3695124]  test_label: 0  weak_label: 1\n",
      "[0.624337   0.37566307]  test_label: 0  weak_label: 1\n",
      "[0.6195111 0.3804889]  test_label: 1  weak_label: 1\n",
      "[0.6004956  0.39950442]  test_label: 1  weak_label: 1\n",
      "[0.62863386 0.3713661 ]  test_label: 2  weak_label: 1\n",
      "[0.62241954 0.37758043]  test_label: 0  weak_label: 1\n",
      "[0.6240139  0.37598613]  test_label: 1  weak_label: 1\n",
      "[0.60395676 0.39604324]  test_label: 1  weak_label: 0\n",
      "[0.63258487 0.36741516]  test_label: 2  weak_label: 1\n",
      "[0.66436905 0.3356309 ]  test_label: 2  weak_label: 1\n",
      "[0.6415408 0.3584592]  test_label: 0  weak_label: 1\n",
      "[0.6279457 0.3720543]  test_label: 2  weak_label: 1\n",
      "[0.63236856 0.36763144]  test_label: 2  weak_label: 1\n",
      "[0.6589899  0.34101006]  test_label: 0  weak_label: 1\n",
      "[0.62724555 0.37275448]  test_label: 0  weak_label: 1\n",
      "[0.6462661  0.35373393]  test_label: 0  weak_label: 1\n",
      "[0.6463602  0.35363975]  test_label: 1  weak_label: 1\n",
      "[0.6436554  0.35634458]  test_label: 1  weak_label: 1\n",
      "[0.62123084 0.37876916]  test_label: 0  weak_label: 1\n",
      "[0.6380422  0.36195776]  test_label: 0  weak_label: 1\n",
      "[0.6247419  0.37525806]  test_label: 0  weak_label: 1\n",
      "[0.6405066 0.3594934]  test_label: 0  weak_label: 1\n",
      "[0.6223207  0.37767926]  test_label: 2  weak_label: 1\n",
      "[0.6127433 0.3872567]  test_label: 2  weak_label: 1\n",
      "[0.63156545 0.36843458]  test_label: 0  weak_label: 1\n",
      "[0.606692   0.39330795]  test_label: 0  weak_label: 1\n",
      "[0.6212329 0.3787671]  test_label: 0  weak_label: 1\n",
      "[0.6472414 0.3527586]  test_label: 0  weak_label: 1\n",
      "[0.6363651  0.36363488]  test_label: 2  weak_label: 1\n",
      "[0.6240689 0.3759311]  test_label: 0  weak_label: 1\n",
      "[0.63236654 0.3676335 ]  test_label: 1  weak_label: 1\n",
      "[0.6328661  0.36713392]  test_label: 0  weak_label: 1\n",
      "[0.6350749  0.36492512]  test_label: 2  weak_label: 1\n",
      "[0.6323285  0.36767152]  test_label: 2  weak_label: 1\n",
      "[0.6321154  0.36788455]  test_label: 1  weak_label: 1\n",
      "[0.6230305  0.37696952]  test_label: 2  weak_label: 1\n",
      "[0.6408653 0.3591347]  test_label: 1  weak_label: 1\n",
      "[0.6308589  0.36914113]  test_label: 0  weak_label: 1\n",
      "[0.62895155 0.37104845]  test_label: 0  weak_label: 1\n",
      "[0.64205265 0.35794738]  test_label: 0  weak_label: 1\n",
      "[0.6161599  0.38384014]  test_label: 0  weak_label: 1\n",
      "[0.6157281 0.3842719]  test_label: 0  weak_label: 1\n",
      "[0.6223994 0.3776006]  test_label: 0  weak_label: 1\n",
      "[0.63577276 0.3642273 ]  test_label: 2  weak_label: 1\n",
      "[0.6317851 0.3682149]  test_label: 1  weak_label: 1\n",
      "[0.6238867  0.37611327]  test_label: 2  weak_label: 1\n",
      "[0.62343615 0.37656385]  test_label: 0  weak_label: 1\n",
      "[0.65430653 0.3456935 ]  test_label: 2  weak_label: 1\n",
      "[0.6342808  0.36571917]  test_label: 0  weak_label: 1\n",
      "[0.6536677  0.34633234]  test_label: 1  weak_label: 1\n",
      "[0.6214987 0.3785013]  test_label: 0  weak_label: 1\n",
      "[0.6314515 0.3685485]  test_label: 2  weak_label: 1\n",
      "[0.63813645 0.36186355]  test_label: 2  weak_label: 1\n",
      "[0.63874835 0.36125165]  test_label: 1  weak_label: 1\n",
      "[0.63754624 0.36245382]  test_label: 2  weak_label: 1\n",
      "[0.6424351  0.35756496]  test_label: 0  weak_label: 1\n",
      "[0.6146995 0.3853005]  test_label: 0  weak_label: 1\n",
      "[0.62206566 0.3779343 ]  test_label: 0  weak_label: 1\n",
      "[0.62977505 0.37022495]  test_label: 0  weak_label: 1\n",
      "[0.6297336  0.37026638]  test_label: 2  weak_label: 1\n",
      "[0.62215585 0.37784413]  test_label: 0  weak_label: 1\n",
      "[0.62268996 0.37731007]  test_label: 0  weak_label: 1\n",
      "[0.639854 0.360146]  test_label: 0  weak_label: 1\n",
      "[0.6418596  0.35814047]  test_label: 0  weak_label: 1\n",
      "[0.6188113 0.3811887]  test_label: 0  weak_label: 1\n",
      "[0.6274433  0.37255666]  test_label: 1  weak_label: 1\n",
      "[0.6372736  0.36272636]  test_label: 2  weak_label: 1\n",
      "[0.62919396 0.37080607]  test_label: 1  weak_label: 1\n",
      "[0.64757067 0.3524293 ]  test_label: 0  weak_label: 1\n",
      "[0.6307093  0.36929074]  test_label: 2  weak_label: 1\n",
      "[0.6376835 0.3623165]  test_label: 2  weak_label: 1\n",
      "[0.65823627 0.3417637 ]  test_label: 0  weak_label: 1\n",
      "[0.6469229  0.35307702]  test_label: 2  weak_label: 1\n",
      "[0.62209195 0.37790808]  test_label: 1  weak_label: 1\n",
      "[0.62946624 0.3705338 ]  test_label: 0  weak_label: 1\n",
      "[0.63603616 0.3639638 ]  test_label: 2  weak_label: 1\n",
      "[0.62523973 0.37476018]  test_label: 0  weak_label: 1\n",
      "[0.61877346 0.38122648]  test_label: 0  weak_label: 0\n",
      "[0.64195675 0.35804322]  test_label: 2  weak_label: 1\n",
      "[0.6556787  0.34432134]  test_label: 0  weak_label: 1\n",
      "[0.62658215 0.37341788]  test_label: 2  weak_label: 1\n",
      "[0.6550603  0.34493965]  test_label: 0  weak_label: 1\n",
      "[0.64483076 0.35516924]  test_label: 0  weak_label: 1\n",
      "[0.6400207  0.35997927]  test_label: 0  weak_label: 1\n",
      "[0.64070344 0.35929653]  test_label: 0  weak_label: 1\n",
      "[0.6226173  0.37738273]  test_label: 0  weak_label: 1\n",
      "[0.61561555 0.38438442]  test_label: 2  weak_label: 1\n",
      "[0.6477097 0.3522903]  test_label: 0  weak_label: 1\n",
      "[0.63827187 0.36172816]  test_label: 0  weak_label: 1\n",
      "[0.6327211  0.36727884]  test_label: 0  weak_label: 1\n",
      "[0.63147193 0.36852804]  test_label: 2  weak_label: 1\n",
      "[0.6552227  0.34477726]  test_label: 2  weak_label: 1\n",
      "[0.63678914 0.36321083]  test_label: 2  weak_label: 1\n",
      "[0.64953864 0.35046136]  test_label: 1  weak_label: 1\n",
      "[0.6236388 0.3763612]  test_label: 1  weak_label: 1\n",
      "[0.6194602  0.38053975]  test_label: 1  weak_label: 1\n",
      "[0.63105035 0.36894962]  test_label: 1  weak_label: 1\n",
      "[0.6300336  0.36996636]  test_label: 2  weak_label: 1\n",
      "[0.6534401  0.34655988]  test_label: 0  weak_label: 1\n",
      "[0.63174707 0.36825293]  test_label: 0  weak_label: 1\n",
      "[0.63019997 0.36980006]  test_label: 0  weak_label: 1\n",
      "[0.62990636 0.37009367]  test_label: 0  weak_label: 1\n",
      "[0.6302486 0.3697514]  test_label: 1  weak_label: 1\n",
      "[0.6503612  0.34963885]  test_label: 2  weak_label: 1\n",
      "[0.6307647 0.3692353]  test_label: 0  weak_label: 1\n",
      "[0.6399389  0.36006114]  test_label: 1  weak_label: 1\n",
      "[0.6542027 0.3457974]  test_label: 2  weak_label: 1\n",
      "[0.6434819  0.35651803]  test_label: 0  weak_label: 1\n",
      "[0.6205028 0.3794972]  test_label: 1  weak_label: 1\n",
      "[0.6522317  0.34776834]  test_label: 2  weak_label: 1\n",
      "[0.6345562  0.36544383]  test_label: 0  weak_label: 1\n",
      "[0.6359334  0.36406663]  test_label: 2  weak_label: 0\n",
      "[0.6443091  0.35569093]  test_label: 2  weak_label: 1\n",
      "[0.6297381  0.37026182]  test_label: 0  weak_label: 1\n",
      "[0.62378013 0.37621984]  test_label: 1  weak_label: 1\n",
      "[0.62972736 0.3702727 ]  test_label: 0  weak_label: 1\n",
      "[0.6600641  0.33993587]  test_label: 0  weak_label: 1\n",
      "[0.65012383 0.34987617]  test_label: 2  weak_label: 1\n",
      "[0.6489128  0.35108718]  test_label: 0  weak_label: 1\n",
      "[0.61507463 0.38492534]  test_label: 0  weak_label: 1\n",
      "[0.6238151 0.3761849]  test_label: 2  weak_label: 1\n",
      "[0.62829447 0.3717056 ]  test_label: 1  weak_label: 1\n",
      "[0.6238333  0.37616667]  test_label: 0  weak_label: 1\n",
      "[0.63045615 0.36954385]  test_label: 2  weak_label: 1\n",
      "[0.6548707  0.34512928]  test_label: 0  weak_label: 1\n",
      "[0.62098706 0.37901294]  test_label: 0  weak_label: 2\n",
      "[0.6249067  0.37509325]  test_label: 1  weak_label: 1\n",
      "[0.6285376 0.3714624]  test_label: 2  weak_label: 1\n",
      "[0.6316956  0.36830434]  test_label: 0  weak_label: 1\n",
      "[0.6320794  0.36792052]  test_label: 0  weak_label: 1\n",
      "[0.619241  0.3807589]  test_label: 2  weak_label: 1\n",
      "[0.6478871 0.3521129]  test_label: 0  weak_label: 1\n",
      "[0.6421128  0.35788718]  test_label: 0  weak_label: 0\n",
      "[0.6321329  0.36786714]  test_label: 0  weak_label: 1\n",
      "[0.62885344 0.37114653]  test_label: 2  weak_label: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6339943  0.36600566]  test_label: 1  weak_label: 1\n",
      "[0.6539899  0.34601012]  test_label: 2  weak_label: 1\n",
      "[0.62852925 0.37147072]  test_label: 2  weak_label: 1\n",
      "[0.6301748  0.36982518]  test_label: 2  weak_label: 1\n",
      "[0.61833733 0.3816627 ]  test_label: 0  weak_label: 1\n",
      "[0.6125366 0.3874634]  test_label: 2  weak_label: 1\n",
      "[0.64940774 0.35059223]  test_label: 1  weak_label: 1\n",
      "[0.6399311  0.36006898]  test_label: 1  weak_label: 1\n",
      "[0.64470047 0.35529956]  test_label: 0  weak_label: 1\n",
      "[0.64882725 0.35117272]  test_label: 1  weak_label: 2\n",
      "[0.6343508  0.36564916]  test_label: 2  weak_label: 1\n",
      "[0.62448525 0.37551478]  test_label: 2  weak_label: 1\n",
      "[0.64922184 0.35077813]  test_label: 2  weak_label: 1\n",
      "[0.6359945 0.3640056]  test_label: 2  weak_label: 1\n",
      "[0.63818324 0.36181673]  test_label: 1  weak_label: 1\n",
      "[0.6426894 0.3573106]  test_label: 1  weak_label: 1\n",
      "[0.62958264 0.37041736]  test_label: 0  weak_label: 1\n",
      "[0.64304715 0.35695288]  test_label: 2  weak_label: 1\n",
      "[0.6352608  0.36473912]  test_label: 0  weak_label: 1\n",
      "[0.63512456 0.36487553]  test_label: 0  weak_label: 1\n",
      "[0.64353126 0.35646874]  test_label: 0  weak_label: 1\n",
      "[0.61584145 0.38415858]  test_label: 2  weak_label: 1\n",
      "[0.6387023  0.36129773]  test_label: 0  weak_label: 1\n",
      "[0.6397732  0.36022684]  test_label: 0  weak_label: 1\n",
      "[0.6292761  0.37072387]  test_label: 2  weak_label: 1\n",
      "[0.6190352  0.38096476]  test_label: 0  weak_label: 1\n",
      "[0.6461792  0.35382083]  test_label: 2  weak_label: 1\n",
      "[0.63347816 0.3665218 ]  test_label: 0  weak_label: 1\n",
      "[0.64204854 0.35795143]  test_label: 2  weak_label: 1\n",
      "[0.6153336 0.3846664]  test_label: 2  weak_label: 1\n",
      "[0.62323517 0.3767649 ]  test_label: 0  weak_label: 1\n",
      "[0.63214564 0.36785433]  test_label: 2  weak_label: 1\n",
      "[0.66162264 0.33837733]  test_label: 2  weak_label: 1\n",
      "[0.634814   0.36518598]  test_label: 0  weak_label: 1\n",
      "[0.6395812  0.36041883]  test_label: 0  weak_label: 1\n",
      "[0.6415001  0.35849988]  test_label: 2  weak_label: 1\n",
      "[0.6189729 0.3810271]  test_label: 0  weak_label: 1\n",
      "[0.6220806  0.37791938]  test_label: 0  weak_label: 1\n",
      "[0.6316697  0.36833033]  test_label: 2  weak_label: 1\n",
      "[0.62745494 0.37254503]  test_label: 1  weak_label: 1\n",
      "[0.6313015  0.36869845]  test_label: 2  weak_label: 0\n",
      "[0.626567   0.37343296]  test_label: 0  weak_label: 1\n",
      "[0.6392945 0.3607055]  test_label: 0  weak_label: 1\n",
      "[0.6377581  0.36224192]  test_label: 1  weak_label: 1\n",
      "[0.63987887 0.3601211 ]  test_label: 0  weak_label: 1\n",
      "[0.6336313 0.3663687]  test_label: 2  weak_label: 1\n",
      "[0.63613194 0.36386803]  test_label: 0  weak_label: 1\n",
      "[0.65699375 0.34300622]  test_label: 2  weak_label: 1\n",
      "[0.6325937 0.3674063]  test_label: 2  weak_label: 1\n",
      "[0.6429741  0.35702586]  test_label: 0  weak_label: 1\n",
      "[0.6527633  0.34723663]  test_label: 2  weak_label: 1\n",
      "[0.61673504 0.383265  ]  test_label: 0  weak_label: 1\n",
      "[0.6277423  0.37225768]  test_label: 0  weak_label: 1\n",
      "[0.6475951 0.3524048]  test_label: 2  weak_label: 1\n",
      "[0.6561886  0.34381142]  test_label: 0  weak_label: 1\n",
      "[0.6341654  0.36583465]  test_label: 2  weak_label: 1\n",
      "[0.6472785  0.35272154]  test_label: 2  weak_label: 1\n",
      "[0.63505    0.36495003]  test_label: 1  weak_label: 1\n",
      "[0.6280528 0.3719473]  test_label: 2  weak_label: 1\n",
      "[0.62215    0.37784997]  test_label: 0  weak_label: 1\n",
      "[0.63785    0.36215004]  test_label: 2  weak_label: 1\n",
      "[0.6245628 0.3754372]  test_label: 0  weak_label: 1\n",
      "[0.64800406 0.351996  ]  test_label: 2  weak_label: 1\n",
      "[0.62463826 0.37536168]  test_label: 2  weak_label: 1\n",
      "[0.63879114 0.3612089 ]  test_label: 1  weak_label: 1\n",
      "[0.62053764 0.37946242]  test_label: 0  weak_label: 1\n",
      "[0.63362837 0.3663716 ]  test_label: 0  weak_label: 1\n",
      "[0.64805484 0.35194513]  test_label: 0  weak_label: 1\n",
      "[0.61460847 0.38539153]  test_label: 2  weak_label: 1\n",
      "[0.6400672  0.35993275]  test_label: 0  weak_label: 2\n",
      "[0.6218729  0.37812707]  test_label: 0  weak_label: 1\n",
      "[0.64679235 0.35320765]  test_label: 0  weak_label: 1\n",
      "[0.6418047  0.35819528]  test_label: 2  weak_label: 1\n",
      "[0.62769294 0.3723071 ]  test_label: 0  weak_label: 1\n",
      "[0.6363668  0.36363322]  test_label: 0  weak_label: 1\n",
      "[0.63936204 0.36063796]  test_label: 0  weak_label: 1\n",
      "[0.6560236  0.34397635]  test_label: 0  weak_label: 1\n",
      "[0.63875496 0.36124507]  test_label: 0  weak_label: 1\n",
      "[0.6441994  0.35580072]  test_label: 0  weak_label: 1\n",
      "[0.63437444 0.36562556]  test_label: 2  weak_label: 1\n",
      "[0.63112426 0.3688757 ]  test_label: 0  weak_label: 1\n",
      "[0.6292551 0.3707449]  test_label: 0  weak_label: 1\n",
      "[0.6422969  0.35770318]  test_label: 2  weak_label: 1\n",
      "[0.6281973 0.3718026]  test_label: 1  weak_label: 1\n",
      "[0.6333281 0.3666719]  test_label: 0  weak_label: 1\n",
      "[0.6298836  0.37011644]  test_label: 2  weak_label: 1\n",
      "[0.6316882  0.36831185]  test_label: 2  weak_label: 1\n",
      "[0.6420953 0.3579047]  test_label: 0  weak_label: 1\n",
      "[0.6332972  0.36670274]  test_label: 0  weak_label: 1\n",
      "[0.6349463  0.36505368]  test_label: 2  weak_label: 2\n",
      "[0.6339332 0.3660669]  test_label: 0  weak_label: 1\n",
      "[0.62825024 0.37174985]  test_label: 1  weak_label: 1\n",
      "[0.6295538  0.37044623]  test_label: 0  weak_label: 1\n",
      "[0.6187025 0.3812975]  test_label: 0  weak_label: 1\n",
      "[0.6213697  0.37863028]  test_label: 0  weak_label: 1\n",
      "[0.62101763 0.37898234]  test_label: 0  weak_label: 1\n",
      "[0.60943973 0.3905603 ]  test_label: 2  weak_label: 1\n",
      "[0.6343948  0.36560512]  test_label: 2  weak_label: 1\n",
      "[0.6236065  0.37639353]  test_label: 0  weak_label: 1\n",
      "[0.6185815  0.38141853]  test_label: 1  weak_label: 1\n",
      "[0.6495802  0.35041982]  test_label: 0  weak_label: 1\n",
      "[0.65282243 0.34717748]  test_label: 2  weak_label: 1\n",
      "[0.6484966  0.35150337]  test_label: 0  weak_label: 1\n",
      "[0.63353896 0.366461  ]  test_label: 1  weak_label: 1\n",
      "[0.64423317 0.3557668 ]  test_label: 1  weak_label: 1\n",
      "[0.63654906 0.36345094]  test_label: 2  weak_label: 1\n",
      "[0.62498367 0.3750164 ]  test_label: 2  weak_label: 1\n",
      "[0.6433635  0.35663652]  test_label: 2  weak_label: 0\n",
      "[0.6276097  0.37239024]  test_label: 2  weak_label: 1\n",
      "[0.62459075 0.37540925]  test_label: 2  weak_label: 1\n",
      "[0.61615735 0.38384265]  test_label: 0  weak_label: 1\n",
      "[0.6430858 0.3569142]  test_label: 0  weak_label: 1\n",
      "[0.6363316 0.3636684]  test_label: 2  weak_label: 1\n",
      "[0.61986643 0.38013357]  test_label: 0  weak_label: 1\n",
      "[0.6325458 0.3674541]  test_label: 2  weak_label: 1\n",
      "[0.5989315 0.4010685]  test_label: 0  weak_label: 1\n",
      "[0.64875287 0.3512472 ]  test_label: 0  weak_label: 1\n",
      "[0.6406668  0.35933325]  test_label: 2  weak_label: 1\n",
      "[0.63440186 0.36559817]  test_label: 0  weak_label: 1\n",
      "[0.64667237 0.35332763]  test_label: 0  weak_label: 2\n",
      "[0.63991475 0.3600852 ]  test_label: 0  weak_label: 1\n",
      "[0.63399786 0.36600208]  test_label: 0  weak_label: 1\n",
      "[0.63823205 0.36176795]  test_label: 0  weak_label: 1\n",
      "[0.62834233 0.37165767]  test_label: 0  weak_label: 1\n",
      "[0.61818045 0.38181955]  test_label: 0  weak_label: 1\n",
      "[0.62817305 0.37182698]  test_label: 0  weak_label: 1\n",
      "[0.62253016 0.37746984]  test_label: 2  weak_label: 1\n",
      "[0.6401865  0.35981357]  test_label: 2  weak_label: 1\n",
      "[0.6522573  0.34774265]  test_label: 2  weak_label: 1\n",
      "[0.6762922 0.3237078]  test_label: 0  weak_label: 1\n",
      "[0.6613378  0.33866224]  test_label: 2  weak_label: 1\n",
      "[0.6445472  0.35545284]  test_label: 0  weak_label: 1\n",
      "[0.6313315  0.36866844]  test_label: 2  weak_label: 1\n",
      "[0.64505225 0.35494772]  test_label: 2  weak_label: 1\n",
      "[0.6361395 0.3638604]  test_label: 0  weak_label: 1\n",
      "[0.6060279 0.3939721]  test_label: 2  weak_label: 1\n",
      "[0.6452776  0.35472235]  test_label: 2  weak_label: 1\n",
      "[0.6281722  0.37182778]  test_label: 2  weak_label: 1\n",
      "[0.66002566 0.33997434]  test_label: 1  weak_label: 1\n",
      "[0.6607129  0.33928713]  test_label: 0  weak_label: 1\n",
      "[0.6493964  0.35060358]  test_label: 0  weak_label: 1\n",
      "[0.62415665 0.37584338]  test_label: 2  weak_label: 1\n",
      "[0.638415 0.361585]  test_label: 2  weak_label: 1\n",
      "[0.62493026 0.37506965]  test_label: 1  weak_label: 1\n",
      "[0.634093 0.365907]  test_label: 0  weak_label: 1\n",
      "[0.6159102  0.38408977]  test_label: 2  weak_label: 1\n",
      "[0.6189768 0.3810232]  test_label: 0  weak_label: 1\n",
      "[0.6192334 0.3807666]  test_label: 2  weak_label: 0\n",
      "[0.64428926 0.35571074]  test_label: 1  weak_label: 1\n",
      "[0.664202   0.33579805]  test_label: 0  weak_label: 1\n",
      "[0.63018215 0.36981785]  test_label: 0  weak_label: 1\n",
      "[0.63802993 0.36197007]  test_label: 2  weak_label: 1\n",
      "[0.6211319 0.3788681]  test_label: 2  weak_label: 1\n",
      "[0.62402254 0.37597743]  test_label: 0  weak_label: 1\n",
      "[0.616705   0.38329497]  test_label: 0  weak_label: 1\n",
      "[0.6198476  0.38015243]  test_label: 2  weak_label: 1\n",
      "[0.63740647 0.3625936 ]  test_label: 0  weak_label: 1\n",
      "[0.63446164 0.36553842]  test_label: 0  weak_label: 1\n",
      "[0.6225892  0.37741086]  test_label: 1  weak_label: 1\n",
      "[0.63473606 0.36526397]  test_label: 2  weak_label: 1\n",
      "[0.6496959  0.35030416]  test_label: 0  weak_label: 2\n",
      "[0.6441938  0.35580614]  test_label: 2  weak_label: 1\n",
      "[0.65365267 0.34634742]  test_label: 0  weak_label: 1\n",
      "[0.64734983 0.35265017]  test_label: 0  weak_label: 1\n",
      "[0.63745457 0.36254546]  test_label: 1  weak_label: 1\n",
      "[0.6406951  0.35930496]  test_label: 0  weak_label: 1\n",
      "[0.6274211  0.37257892]  test_label: 0  weak_label: 1\n",
      "[0.6146386 0.3853614]  test_label: 0  weak_label: 1\n",
      "[0.65189385 0.34810618]  test_label: 2  weak_label: 1\n",
      "[0.64534575 0.35465425]  test_label: 0  weak_label: 1\n",
      "[0.61950505 0.38049498]  test_label: 1  weak_label: 1\n",
      "[0.63378316 0.36621687]  test_label: 2  weak_label: 1\n",
      "[0.6663803  0.33361977]  test_label: 0  weak_label: 1\n",
      "[0.6281496  0.37185037]  test_label: 2  weak_label: 1\n",
      "[0.63019806 0.3698019 ]  test_label: 2  weak_label: 1\n",
      "[0.6315884  0.36841166]  test_label: 0  weak_label: 1\n",
      "[0.6462904  0.35370958]  test_label: 0  weak_label: 1\n",
      "[0.6377956 0.3622043]  test_label: 1  weak_label: 1\n",
      "[0.6308394  0.36916056]  test_label: 2  weak_label: 1\n",
      "[0.6108621 0.3891379]  test_label: 1  weak_label: 1\n",
      "[0.62987924 0.3701208 ]  test_label: 0  weak_label: 1\n",
      "[0.63901955 0.36098042]  test_label: 0  weak_label: 2\n",
      "[0.6478384 0.3521616]  test_label: 2  weak_label: 1\n",
      "[0.661873   0.33812702]  test_label: 2  weak_label: 1\n",
      "[0.6357428  0.36425725]  test_label: 1  weak_label: 1\n",
      "[0.6043991 0.3956009]  test_label: 0  weak_label: 1\n",
      "[0.6504681  0.34953186]  test_label: 0  weak_label: 1\n",
      "[0.6303939  0.36960602]  test_label: 1  weak_label: 1\n",
      "[0.6360321  0.36396787]  test_label: 0  weak_label: 1\n",
      "[0.6411926 0.3588074]  test_label: 1  weak_label: 1\n",
      "[0.6240112  0.37598872]  test_label: 0  weak_label: 1\n",
      "[0.6632922  0.33670774]  test_label: 0  weak_label: 1\n",
      "[0.6274685 0.3725314]  test_label: 2  weak_label: 1\n",
      "[0.66233397 0.337666  ]  test_label: 1  weak_label: 1\n",
      "[0.6561679  0.34383208]  test_label: 0  weak_label: 1\n",
      "[0.62960815 0.37039188]  test_label: 0  weak_label: 0\n",
      "[0.641061   0.35893896]  test_label: 0  weak_label: 1\n",
      "[0.6224165  0.37758347]  test_label: 1  weak_label: 1\n",
      "[0.64489824 0.35510176]  test_label: 1  weak_label: 1\n",
      "[0.63202393 0.36797604]  test_label: 0  weak_label: 1\n",
      "[0.62082285 0.37917712]  test_label: 0  weak_label: 1\n",
      "[0.64444464 0.3555554 ]  test_label: 2  weak_label: 1\n",
      "[0.6272071  0.37279293]  test_label: 2  weak_label: 1\n",
      "[0.62662786 0.37337214]  test_label: 0  weak_label: 0\n",
      "[0.6439912 0.3560088]  test_label: 0  weak_label: 1\n",
      "[0.6187333 0.3812667]  test_label: 2  weak_label: 1\n",
      "[0.66776264 0.33223736]  test_label: 2  weak_label: 1\n",
      "[0.6409486  0.35905138]  test_label: 2  weak_label: 1\n",
      "[0.6090357  0.39096436]  test_label: 2  weak_label: 1\n",
      "[0.6198183 0.3801818]  test_label: 0  weak_label: 1\n",
      "[0.61900026 0.38099974]  test_label: 0  weak_label: 1\n",
      "[0.6309528 0.3690472]  test_label: 0  weak_label: 1\n",
      "[0.6204534  0.37954664]  test_label: 0  weak_label: 1\n",
      "[0.64972836 0.35027158]  test_label: 0  weak_label: 1\n",
      "[0.6266076  0.37339243]  test_label: 0  weak_label: 1\n",
      "[0.6335929  0.36640707]  test_label: 2  weak_label: 1\n",
      "[0.6541075 0.3458925]  test_label: 2  weak_label: 1\n",
      "[0.6322338  0.36776623]  test_label: 0  weak_label: 1\n",
      "[0.6399214  0.36007854]  test_label: 0  weak_label: 1\n",
      "[0.6336478  0.36635217]  test_label: 2  weak_label: 1\n",
      "[0.6383106  0.36168936]  test_label: 0  weak_label: 1\n",
      "[0.6371964  0.36280355]  test_label: 2  weak_label: 1\n",
      "[0.6417415  0.35825852]  test_label: 0  weak_label: 1\n",
      "[0.6351731  0.36482683]  test_label: 0  weak_label: 1\n",
      "[0.6087767  0.39122328]  test_label: 2  weak_label: 1\n",
      "[0.64787513 0.3521249 ]  test_label: 1  weak_label: 1\n",
      "[0.6374991 0.3625009]  test_label: 2  weak_label: 1\n",
      "[0.64042574 0.3595743 ]  test_label: 0  weak_label: 1\n",
      "[0.637105   0.36289498]  test_label: 2  weak_label: 1\n",
      "[0.61954427 0.3804557 ]  test_label: 0  weak_label: 0\n",
      "[0.6586482 0.3413518]  test_label: 2  weak_label: 1\n",
      "[0.63553756 0.36446238]  test_label: 0  weak_label: 1\n",
      "[0.63923925 0.36076078]  test_label: 0  weak_label: 1\n",
      "[0.6436192 0.3563808]  test_label: 1  weak_label: 1\n",
      "[0.64673793 0.3532621 ]  test_label: 2  weak_label: 1\n",
      "[0.61522293 0.38477704]  test_label: 2  weak_label: 1\n",
      "[0.6359964  0.36400366]  test_label: 2  weak_label: 1\n",
      "[0.6252266  0.37477335]  test_label: 0  weak_label: 1\n",
      "[0.63248986 0.3675101 ]  test_label: 0  weak_label: 1\n",
      "[0.6502893  0.34971076]  test_label: 2  weak_label: 1\n",
      "[0.6231361 0.376864 ]  test_label: 2  weak_label: 1\n",
      "[0.6549323  0.34506768]  test_label: 2  weak_label: 1\n",
      "[0.63822246 0.36177757]  test_label: 0  weak_label: 1\n",
      "[0.6696653  0.33033475]  test_label: 0  weak_label: 1\n",
      "[0.6373139 0.3626861]  test_label: 0  weak_label: 1\n",
      "[0.6576567 0.3423433]  test_label: 2  weak_label: 1\n",
      "[0.6511892  0.34881076]  test_label: 2  weak_label: 1\n",
      "[0.63195497 0.36804506]  test_label: 0  weak_label: 1\n",
      "[0.63651603 0.363484  ]  test_label: 0  weak_label: 1\n",
      "[0.6343734 0.3656265]  test_label: 2  weak_label: 1\n",
      "[0.6363941  0.36360592]  test_label: 1  weak_label: 1\n",
      "[0.6150725  0.38492748]  test_label: 0  weak_label: 1\n",
      "[0.63166946 0.36833054]  test_label: 2  weak_label: 1\n",
      "[0.61419827 0.38580176]  test_label: 2  weak_label: 1\n",
      "[0.6355982 0.3644018]  test_label: 0  weak_label: 1\n",
      "[0.63402736 0.3659726 ]  test_label: 0  weak_label: 1\n",
      "[0.62877625 0.37122375]  test_label: 0  weak_label: 1\n",
      "[0.6298383  0.37016168]  test_label: 2  weak_label: 1\n",
      "[0.62239945 0.37760058]  test_label: 2  weak_label: 1\n",
      "[0.62874883 0.37125114]  test_label: 2  weak_label: 1\n",
      "[0.6394968  0.36050326]  test_label: 0  weak_label: 1\n",
      "[0.62605524 0.37394476]  test_label: 2  weak_label: 1\n",
      "[0.6324508 0.3675492]  test_label: 2  weak_label: 1\n",
      "[0.6341623 0.3658377]  test_label: 0  weak_label: 1\n",
      "[0.62839764 0.37160236]  test_label: 0  weak_label: 1\n",
      "[0.63192743 0.3680725 ]  test_label: 0  weak_label: 1\n",
      "[0.66330135 0.33669865]  test_label: 0  weak_label: 1\n",
      "[0.6217735 0.3782265]  test_label: 0  weak_label: 1\n",
      "[0.6517586  0.34824136]  test_label: 0  weak_label: 1\n",
      "[0.61902106 0.38097897]  test_label: 2  weak_label: 1\n",
      "[0.63920313 0.36079684]  test_label: 1  weak_label: 1\n",
      "[0.63289315 0.36710688]  test_label: 0  weak_label: 0\n",
      "[0.6394108 0.3605892]  test_label: 2  weak_label: 1\n",
      "[0.62881714 0.37118283]  test_label: 2  weak_label: 1\n",
      "[0.62857133 0.3714287 ]  test_label: 0  weak_label: 1\n",
      "[0.6197454 0.3802546]  test_label: 1  weak_label: 0\n",
      "[0.6315485 0.3684514]  test_label: 2  weak_label: 1\n",
      "[0.63279283 0.36720717]  test_label: 2  weak_label: 1\n",
      "[0.62343085 0.37656915]  test_label: 2  weak_label: 1\n",
      "[0.62930894 0.3706911 ]  test_label: 2  weak_label: 1\n",
      "[0.63656443 0.36343557]  test_label: 2  weak_label: 1\n",
      "[0.63646805 0.36353195]  test_label: 1  weak_label: 1\n",
      "[0.6576883  0.34231168]  test_label: 0  weak_label: 1\n",
      "[0.6526815 0.3473185]  test_label: 2  weak_label: 1\n",
      "[0.6617482 0.3382518]  test_label: 2  weak_label: 1\n",
      "[0.63790035 0.36209962]  test_label: 0  weak_label: 1\n",
      "[0.6189581  0.38104188]  test_label: 0  weak_label: 1\n",
      "[0.61764616 0.38235378]  test_label: 1  weak_label: 1\n",
      "[0.6276432  0.37235677]  test_label: 2  weak_label: 1\n",
      "[0.6188377 0.3811623]  test_label: 2  weak_label: 1\n",
      "[0.6372732 0.3627268]  test_label: 0  weak_label: 1\n",
      "[0.63752186 0.3624782 ]  test_label: 0  weak_label: 1\n",
      "[0.62343466 0.3765653 ]  test_label: 2  weak_label: 1\n",
      "[0.61897635 0.38102362]  test_label: 1  weak_label: 1\n",
      "[0.62474376 0.3752562 ]  test_label: 2  weak_label: 1\n",
      "[0.62805885 0.37194115]  test_label: 0  weak_label: 1\n",
      "[0.6402471  0.35975286]  test_label: 2  weak_label: 1\n",
      "[0.63265115 0.36734888]  test_label: 2  weak_label: 1\n",
      "[0.6147945  0.38520554]  test_label: 0  weak_label: 1\n",
      "[0.6279883 0.3720117]  test_label: 0  weak_label: 1\n",
      "[0.6445964 0.3554036]  test_label: 1  weak_label: 1\n",
      "[0.6468498  0.35315016]  test_label: 0  weak_label: 1\n",
      "[0.6273886 0.3726114]  test_label: 2  weak_label: 1\n",
      "[0.65525866 0.34474134]  test_label: 2  weak_label: 1\n",
      "[0.63614064 0.36385936]  test_label: 2  weak_label: 1\n",
      "[0.64910054 0.35089946]  test_label: 2  weak_label: 1\n",
      "[0.627268 0.372732]  test_label: 0  weak_label: 1\n",
      "[0.6564607  0.34353927]  test_label: 2  weak_label: 1\n",
      "[0.62101763 0.37898234]  test_label: 0  weak_label: 1\n",
      "[0.6329239  0.36707613]  test_label: 0  weak_label: 1\n",
      "[0.6340519 0.3659481]  test_label: 2  weak_label: 1\n",
      "[0.6650887 0.3349113]  test_label: 0  weak_label: 1\n",
      "[0.6213929  0.37860706]  test_label: 1  weak_label: 1\n"
     ]
    }
   ],
   "source": [
    "conf_prob = predictions['proba']\n",
    "\n",
    "t = X_n_train.shape[0]\n",
    "\n",
    "for i in range(X_n_test.shape[0]):\n",
    "    print(conf_prob[i],\" test_label:\",y_test[i],\" weak_label:\",Y_weak[t+i])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8267898383371824"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct = [pred_conf_score == true_conf_score]\n",
    "sum(sum(num_correct))/len(pred_conf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a405efe10>, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_task_id': 0, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_model_dir': '/var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4', '_tf_random_seed': None, '_task_type': 'worker', '_service': None, '_session_config': None, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "model = tf.estimator.Estimator(model_fn=classifier_model_fn, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training params, just used in this cell for the input_fn-s\n",
    "train_params = dict(batch_size=5, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_train = np.reshape(ns_train,newshape=(len(ns_train),))\n",
    "y_conf = np.reshape(y_conf,newshape=(len(y_conf),))\n",
    "\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_train, \"ns\": ns_train}, y=y_conf,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_test = np.reshape(ns_test,newshape=(len(ns_test),))\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": X_n_test, \"ns\": ns_test}, y=true_conf_score,\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "true_conf_score = np.reshape(true_conf_score,newshape=(len(true_conf_score),))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.78918195, step = 1\n",
      "INFO:tensorflow:global_step/sec: 36.7895\n",
      "INFO:tensorflow:loss = 0.5528136, step = 101 (2.720 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4626\n",
      "INFO:tensorflow:loss = 0.47720048, step = 201 (2.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6546\n",
      "INFO:tensorflow:loss = 0.808532, step = 301 (2.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6618\n",
      "INFO:tensorflow:loss = 0.40018374, step = 401 (2.459 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4115\n",
      "INFO:tensorflow:loss = 0.42256653, step = 501 (2.415 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7854\n",
      "INFO:tensorflow:loss = 0.47191364, step = 601 (2.394 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 692 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 1.017024.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:24:06\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-692\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:24:08\n",
      "INFO:tensorflow:Saving dict for global step 692: accuracy = 0.82678986, cross_entropy_loss = 0.44407418, global_step = 692, loss = 0.67367166\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-692\n",
      "INFO:tensorflow:Saving checkpoints for 693 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.40594018, step = 693\n",
      "INFO:tensorflow:global_step/sec: 40.3727\n",
      "INFO:tensorflow:loss = 0.4674796, step = 793 (2.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2531\n",
      "INFO:tensorflow:loss = 0.35165495, step = 893 (2.425 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.675\n",
      "INFO:tensorflow:loss = 0.652002, step = 993 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6574\n",
      "INFO:tensorflow:loss = 0.35187942, step = 1093 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.6132\n",
      "INFO:tensorflow:loss = 0.37600708, step = 1193 (2.590 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0304\n",
      "INFO:tensorflow:loss = 0.46939075, step = 1293 (2.498 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1384 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.84630966.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:24:28\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-1384\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:24:30\n",
      "INFO:tensorflow:Saving dict for global step 1384: accuracy = 0.8013857, cross_entropy_loss = 0.47144157, global_step = 1384, loss = 0.7005391\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-1384\n",
      "INFO:tensorflow:Saving checkpoints for 1385 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.31948167, step = 1385\n",
      "INFO:tensorflow:global_step/sec: 41.3028\n",
      "INFO:tensorflow:loss = 0.36428353, step = 1485 (2.423 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.251\n",
      "INFO:tensorflow:loss = 0.27291858, step = 1585 (2.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6036\n",
      "INFO:tensorflow:loss = 0.5059952, step = 1685 (2.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4532\n",
      "INFO:tensorflow:loss = 0.29967266, step = 1785 (2.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8836\n",
      "INFO:tensorflow:loss = 0.2800197, step = 1885 (2.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7301\n",
      "INFO:tensorflow:loss = 0.33817083, step = 1985 (2.455 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2076 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.4247337.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:24:53\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-2076\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:24:55\n",
      "INFO:tensorflow:Saving dict for global step 2076: accuracy = 0.7829099, cross_entropy_loss = 0.56441855, global_step = 2076, loss = 0.7932226\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-2076\n",
      "INFO:tensorflow:Saving checkpoints for 2077 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.33246532, step = 2077\n",
      "INFO:tensorflow:global_step/sec: 38.8245\n",
      "INFO:tensorflow:loss = 0.24344033, step = 2177 (2.578 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4221\n",
      "INFO:tensorflow:loss = 0.24218462, step = 2277 (2.474 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8941\n",
      "INFO:tensorflow:loss = 0.38473338, step = 2377 (2.446 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.0013\n",
      "INFO:tensorflow:loss = 0.28324905, step = 2477 (2.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.3205\n",
      "INFO:tensorflow:loss = 0.25484535, step = 2577 (2.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7767\n",
      "INFO:tensorflow:loss = 0.28082165, step = 2677 (2.452 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2768 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2675727.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:25:15\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-2768\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:25:17\n",
      "INFO:tensorflow:Saving dict for global step 2768: accuracy = 0.77482677, cross_entropy_loss = 0.6649444, global_step = 2768, loss = 0.8933252\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-2768\n",
      "INFO:tensorflow:Saving checkpoints for 2769 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.26805386, step = 2769\n",
      "INFO:tensorflow:global_step/sec: 39.5531\n",
      "INFO:tensorflow:loss = 0.23270148, step = 2869 (2.530 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.217\n",
      "INFO:tensorflow:loss = 0.23218758, step = 2969 (2.426 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.1197\n",
      "INFO:tensorflow:loss = 0.3268313, step = 3069 (2.493 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.864\n",
      "INFO:tensorflow:loss = 0.2613752, step = 3169 (2.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.472\n",
      "INFO:tensorflow:loss = 0.23638017, step = 3269 (2.470 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0967\n",
      "INFO:tensorflow:loss = 0.25290504, step = 3369 (2.494 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3460 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.25199416.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:25:37\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-3460\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:25:39\n",
      "INFO:tensorflow:Saving dict for global step 3460: accuracy = 0.778291, cross_entropy_loss = 0.7496585, global_step = 3460, loss = 0.9774487\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-3460\n",
      "INFO:tensorflow:Saving checkpoints for 3461 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.27307197, step = 3461\n",
      "INFO:tensorflow:global_step/sec: 39.2362\n",
      "INFO:tensorflow:loss = 0.22953378, step = 3561 (2.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 39.3896\n",
      "INFO:tensorflow:loss = 0.22898091, step = 3661 (2.538 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4345\n",
      "INFO:tensorflow:loss = 0.28704524, step = 3761 (2.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6637\n",
      "INFO:tensorflow:loss = 0.24306397, step = 3861 (2.400 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1019\n",
      "INFO:tensorflow:loss = 0.23093174, step = 3961 (2.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5229\n",
      "INFO:tensorflow:loss = 0.24179769, step = 4061 (2.408 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4152 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loss for final step: 0.24218413.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:25:59\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-4152\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:26:01\n",
      "INFO:tensorflow:Saving dict for global step 4152: accuracy = 0.7817552, cross_entropy_loss = 0.81140476, global_step = 4152, loss = 1.0385015\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-4152\n",
      "INFO:tensorflow:Saving checkpoints for 4153 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.23081917, step = 4153\n",
      "INFO:tensorflow:global_step/sec: 39.8406\n",
      "INFO:tensorflow:loss = 0.24123655, step = 4253 (2.512 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.256\n",
      "INFO:tensorflow:loss = 0.22757725, step = 4353 (2.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.5313\n",
      "INFO:tensorflow:loss = 0.26656294, step = 4453 (2.408 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.3441\n",
      "INFO:tensorflow:loss = 0.23688945, step = 4553 (2.478 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.9822\n",
      "INFO:tensorflow:loss = 0.22897497, step = 4653 (2.440 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1303\n",
      "INFO:tensorflow:loss = 0.2372545, step = 4753 (2.432 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4844 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23678933.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:26:23\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-4844\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:26:26\n",
      "INFO:tensorflow:Saving dict for global step 4844: accuracy = 0.7806005, cross_entropy_loss = 0.8540979, global_step = 4844, loss = 1.0804517\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-4844\n",
      "INFO:tensorflow:Saving checkpoints for 4845 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.23358168, step = 4845\n",
      "INFO:tensorflow:global_step/sec: 40.8026\n",
      "INFO:tensorflow:loss = 0.2267577, step = 4945 (2.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9453\n",
      "INFO:tensorflow:loss = 0.22651935, step = 5045 (2.384 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5667\n",
      "INFO:tensorflow:loss = 0.25565368, step = 5145 (2.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.54\n",
      "INFO:tensorflow:loss = 0.23372209, step = 5245 (2.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7175\n",
      "INFO:tensorflow:loss = 0.2277555, step = 5345 (2.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.6638\n",
      "INFO:tensorflow:loss = 0.23459081, step = 5445 (2.400 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5536 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23306334.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:26:45\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-5536\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:26:47\n",
      "INFO:tensorflow:Saving dict for global step 5536: accuracy = 0.77713627, cross_entropy_loss = 0.8853695, global_step = 5536, loss = 1.1109551\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-5536\n",
      "INFO:tensorflow:Saving checkpoints for 5537 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2285513, step = 5537\n",
      "INFO:tensorflow:global_step/sec: 39.2249\n",
      "INFO:tensorflow:loss = 0.22578633, step = 5637 (2.551 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1898\n",
      "INFO:tensorflow:loss = 0.22559461, step = 5737 (2.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4721\n",
      "INFO:tensorflow:loss = 0.24851856, step = 5837 (2.471 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6152\n",
      "INFO:tensorflow:loss = 0.23106411, step = 5937 (2.463 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4391\n",
      "INFO:tensorflow:loss = 0.22675152, step = 6037 (2.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.9495\n",
      "INFO:tensorflow:loss = 0.23273648, step = 6137 (2.442 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6228 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.23064825.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:27:08\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-6228\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:27:10\n",
      "INFO:tensorflow:Saving dict for global step 6228: accuracy = 0.77482677, cross_entropy_loss = 0.90947825, global_step = 6228, loss = 1.1342821\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-6228\n",
      "INFO:tensorflow:Saving checkpoints for 6229 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22671382, step = 6229\n",
      "INFO:tensorflow:global_step/sec: 37.8337\n",
      "INFO:tensorflow:loss = 0.22489935, step = 6329 (2.646 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.8614\n",
      "INFO:tensorflow:loss = 0.22471291, step = 6429 (2.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4544\n",
      "INFO:tensorflow:loss = 0.24389262, step = 6529 (2.472 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.6192\n",
      "INFO:tensorflow:loss = 0.22891048, step = 6629 (2.462 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4543\n",
      "INFO:tensorflow:loss = 0.22573891, step = 6729 (2.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9698\n",
      "INFO:tensorflow:loss = 0.23075348, step = 6829 (2.383 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6920 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22863893.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:27:29\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-6920\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:27:32\n",
      "INFO:tensorflow:Saving dict for global step 6920: accuracy = 0.77598155, cross_entropy_loss = 0.92659247, global_step = 6920, loss = 1.1506069\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-6920\n",
      "INFO:tensorflow:Saving checkpoints for 6921 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22771935, step = 6921\n",
      "INFO:tensorflow:global_step/sec: 41.6284\n",
      "INFO:tensorflow:loss = 0.22405626, step = 7021 (2.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7313\n",
      "INFO:tensorflow:loss = 0.22386944, step = 7121 (2.340 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7047\n",
      "INFO:tensorflow:loss = 0.24048972, step = 7221 (2.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.2102\n",
      "INFO:tensorflow:loss = 0.22731087, step = 7321 (2.369 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4655\n",
      "INFO:tensorflow:loss = 0.2247715, step = 7421 (2.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9395\n",
      "INFO:tensorflow:loss = 0.22897062, step = 7521 (2.385 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7612 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22701725.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:27:51\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-7612\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:27:54\n",
      "INFO:tensorflow:Saving dict for global step 7612: accuracy = 0.77482677, cross_entropy_loss = 0.9397123, global_step = 7612, loss = 1.1629336\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-7612\n",
      "INFO:tensorflow:Saving checkpoints for 7613 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22446832, step = 7613\n",
      "INFO:tensorflow:global_step/sec: 40.1967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.2232295, step = 7713 (2.489 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7629\n",
      "INFO:tensorflow:loss = 0.22304432, step = 7813 (2.454 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1743\n",
      "INFO:tensorflow:loss = 0.23770247, step = 7913 (2.429 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3823\n",
      "INFO:tensorflow:loss = 0.22594857, step = 8013 (2.416 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4282\n",
      "INFO:tensorflow:loss = 0.22383144, step = 8113 (2.414 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5703\n",
      "INFO:tensorflow:loss = 0.22742851, step = 8213 (2.464 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8304 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2256478.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:28:14\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-8304\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:28:16\n",
      "INFO:tensorflow:Saving dict for global step 8304: accuracy = 0.77367204, cross_entropy_loss = 0.9504886, global_step = 8304, loss = 1.1729165\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-8304\n",
      "INFO:tensorflow:Saving checkpoints for 8305 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22453557, step = 8305\n",
      "INFO:tensorflow:global_step/sec: 40.0779\n",
      "INFO:tensorflow:loss = 0.22241999, step = 8405 (2.497 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2691\n",
      "INFO:tensorflow:loss = 0.22223434, step = 8505 (2.424 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3656\n",
      "INFO:tensorflow:loss = 0.23539339, step = 8605 (2.417 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.4557\n",
      "INFO:tensorflow:loss = 0.22477473, step = 8705 (2.413 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.2097\n",
      "INFO:tensorflow:loss = 0.22289361, step = 8805 (2.427 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.4468\n",
      "INFO:tensorflow:loss = 0.22608288, step = 8905 (2.472 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 8996 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22446375.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:28:35\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-8996\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:28:37\n",
      "INFO:tensorflow:Saving dict for global step 8996: accuracy = 0.77367204, cross_entropy_loss = 0.9595468, global_step = 8996, loss = 1.1811823\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-8996\n",
      "INFO:tensorflow:Saving checkpoints for 8997 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22240357, step = 8997\n",
      "INFO:tensorflow:global_step/sec: 40.5758\n",
      "INFO:tensorflow:loss = 0.22161503, step = 9097 (2.466 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.1739\n",
      "INFO:tensorflow:loss = 0.22142877, step = 9197 (2.317 sec)\n",
      "INFO:tensorflow:global_step/sec: 43.2117\n",
      "INFO:tensorflow:loss = 0.23347606, step = 9297 (2.314 sec)\n",
      "INFO:tensorflow:global_step/sec: 38.4194\n",
      "INFO:tensorflow:loss = 0.22367963, step = 9397 (2.603 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.4567\n",
      "INFO:tensorflow:loss = 0.22201854, step = 9497 (2.355 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.8772\n",
      "INFO:tensorflow:loss = 0.22477223, step = 9597 (2.388 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 9688 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22335723.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:28:58\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-9688\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:29:00\n",
      "INFO:tensorflow:Saving dict for global step 9688: accuracy = 0.77367204, cross_entropy_loss = 0.96754134, global_step = 9688, loss = 1.1883875\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-9688\n",
      "INFO:tensorflow:Saving checkpoints for 9689 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22579111, step = 9689\n",
      "INFO:tensorflow:global_step/sec: 38.5996\n",
      "INFO:tensorflow:loss = 0.22081816, step = 9789 (2.592 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3175\n",
      "INFO:tensorflow:loss = 0.22063431, step = 9889 (2.421 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.583\n",
      "INFO:tensorflow:loss = 0.23171386, step = 9989 (2.348 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5091\n",
      "INFO:tensorflow:loss = 0.22266723, step = 10089 (2.468 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.2724\n",
      "INFO:tensorflow:loss = 0.22112457, step = 10189 (2.365 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.0651\n",
      "INFO:tensorflow:loss = 0.22359489, step = 10289 (2.377 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10380 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22233717.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:29:19\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-10380\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:29:21\n",
      "INFO:tensorflow:Saving dict for global step 10380: accuracy = 0.7725173, cross_entropy_loss = 0.9746204, global_step = 10380, loss = 1.1946805\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-10380\n",
      "INFO:tensorflow:Saving checkpoints for 10381 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22072153, step = 10381\n",
      "INFO:tensorflow:global_step/sec: 39.6047\n",
      "INFO:tensorflow:loss = 0.22002907, step = 10481 (2.527 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.0581\n",
      "INFO:tensorflow:loss = 0.2198429, step = 10581 (2.496 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.7083\n",
      "INFO:tensorflow:loss = 0.23015277, step = 10681 (2.456 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1867\n",
      "INFO:tensorflow:loss = 0.22170293, step = 10781 (2.428 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7553\n",
      "INFO:tensorflow:loss = 0.22026229, step = 10881 (2.395 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.9951\n",
      "INFO:tensorflow:loss = 0.22246453, step = 10981 (2.326 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11072 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22136275.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:29:41\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-11072\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:29:44\n",
      "INFO:tensorflow:Saving dict for global step 11072: accuracy = 0.7713626, cross_entropy_loss = 0.9809916, global_step = 11072, loss = 1.20027\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-11072\n",
      "INFO:tensorflow:Saving checkpoints for 11073 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22769529, step = 11073\n",
      "INFO:tensorflow:global_step/sec: 41.1637\n",
      "INFO:tensorflow:loss = 0.21924517, step = 11173 (2.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5438\n",
      "INFO:tensorflow:loss = 0.21905953, step = 11273 (2.351 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.7001\n",
      "INFO:tensorflow:loss = 0.22868413, step = 11373 (2.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.9977\n",
      "INFO:tensorflow:loss = 0.22079743, step = 11473 (2.326 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.5827\n",
      "INFO:tensorflow:loss = 0.21940725, step = 11573 (2.349 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9982\n",
      "INFO:tensorflow:loss = 0.22139168, step = 11673 (2.381 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 11764 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.22043608.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:30:03\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-11764\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:30:05\n",
      "INFO:tensorflow:Saving dict for global step 11764: accuracy = 0.76674366, cross_entropy_loss = 0.986861, global_step = 11764, loss = 1.2053622\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-11764\n",
      "INFO:tensorflow:Saving checkpoints for 11765 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22350213, step = 11765\n",
      "INFO:tensorflow:global_step/sec: 42.3589\n",
      "INFO:tensorflow:loss = 0.21846674, step = 11865 (2.363 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1585\n",
      "INFO:tensorflow:loss = 0.21828145, step = 11965 (2.430 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.9447\n",
      "INFO:tensorflow:loss = 0.2273313, step = 12065 (2.383 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.7909\n",
      "INFO:tensorflow:loss = 0.21988875, step = 12165 (2.394 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.3558\n",
      "INFO:tensorflow:loss = 0.21856536, step = 12265 (2.418 sec)\n",
      "INFO:tensorflow:global_step/sec: 40.5499\n",
      "INFO:tensorflow:loss = 0.22037141, step = 12365 (2.465 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 12456 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21953593.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:30:25\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-12456\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:30:27\n",
      "INFO:tensorflow:Saving dict for global step 12456: accuracy = 0.7678984, cross_entropy_loss = 0.99226266, global_step = 12456, loss = 1.2099922\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-12456\n",
      "INFO:tensorflow:Saving checkpoints for 12457 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.21998252, step = 12457\n",
      "INFO:tensorflow:global_step/sec: 40.255\n",
      "INFO:tensorflow:loss = 0.22150244, step = 12557 (2.486 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.5353\n",
      "INFO:tensorflow:loss = 0.21751405, step = 12657 (3.075 sec)\n",
      "INFO:tensorflow:global_step/sec: 31.1517\n",
      "INFO:tensorflow:loss = 0.22608858, step = 12757 (3.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.2305\n",
      "INFO:tensorflow:loss = 0.2190597, step = 12857 (2.367 sec)\n",
      "INFO:tensorflow:global_step/sec: 42.8652\n",
      "INFO:tensorflow:loss = 0.21773759, step = 12957 (2.334 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.8053\n",
      "INFO:tensorflow:loss = 0.21937147, step = 13057 (2.392 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13148 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.21865436.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:30:48\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13148\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:30:51\n",
      "INFO:tensorflow:Saving dict for global step 13148: accuracy = 0.7678984, cross_entropy_loss = 0.9972896, global_step = 13148, loss = 1.2142524\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13148\n",
      "INFO:tensorflow:Saving checkpoints for 13149 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.22444855, step = 13149\n",
      "INFO:tensorflow:global_step/sec: 41.7223\n",
      "INFO:tensorflow:loss = 0.21692738, step = 13249 (2.399 sec)\n",
      "INFO:tensorflow:global_step/sec: 41.1078\n",
      "INFO:tensorflow:loss = 0.21674359, step = 13349 (2.432 sec)\n",
      "INFO:tensorflow:global_step/sec: 27.2455\n",
      "INFO:tensorflow:loss = 0.22484127, step = 13449 (3.671 sec)\n",
      "INFO:tensorflow:global_step/sec: 24.2589\n",
      "INFO:tensorflow:loss = 0.21822673, step = 13549 (4.123 sec)\n",
      "INFO:tensorflow:global_step/sec: 32.0438\n",
      "INFO:tensorflow:loss = 0.21693003, step = 13649 (3.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 37.9975\n",
      "INFO:tensorflow:loss = 0.21844393, step = 13749 (2.631 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 13840 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2178053.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:31:14\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13840\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:31:19\n",
      "INFO:tensorflow:Saving dict for global step 13840: accuracy = 0.7678984, cross_entropy_loss = 1.0020665, global_step = 13840, loss = 1.218267\n"
     ]
    }
   ],
   "source": [
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "    model.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Starting evaluation at 2018-03-23-22:31:53\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13840\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-23-22:31:56\n",
      "INFO:tensorflow:Saving dict for global step 13840: accuracy = 0.7678984, cross_entropy_loss = 1.0020665, global_step = 13840, loss = 1.218267\n",
      "Accuracy on test set: 76.79%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.7678984,\n",
       " 'cross_entropy_loss': 1.0020665,\n",
       " 'global_step': 13840,\n",
       " 'loss': 1.218267}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_metrics = model.evaluate(input_fn=test_input_fn, name=\"test\")  # replace with result of model.evaluate(...)\n",
    "\n",
    "\n",
    "print(\"Accuracy on test set: {:.02%}\".format(eval_metrics['accuracy']))\n",
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_output_layer_conf(h_, labels_, num_classes, conf):\n",
    "    \n",
    "    n = h_.get_shape().as_list()\n",
    "    \n",
    "    with tf.variable_scope(\"Logits\"):\n",
    "        W_out_ = tf.get_variable(name = 'W_out', shape=[n[1],num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        b_out_ = tf.get_variable(name = 'b_out', shape=[num_classes],dtype=tf.float32\n",
    "                               ,initializer=tf.random_normal_initializer(),trainable=True)\n",
    "        logits_ = tf.matmul(h_,W_out_) + b_out_\n",
    "        \n",
    "        \n",
    "\n",
    "    # If no labels provided, don't try to compute loss.\n",
    "    if labels_ is None:\n",
    "        return None, logits_\n",
    "\n",
    "    with tf.name_scope(\"Softmax\"):\n",
    "        #c = tf.constant(conf,dtype = tf.float32)\n",
    "        loss_ = tf.reduce_mean(tf.cast(conf,dtype = tf.float32) * tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels_,logits=logits_)) \n",
    "        \n",
    "    \n",
    "    return loss_, logits_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_model_fn_conf(features, labels, mode, params):\n",
    "    # Seed the RNG for repeatability\n",
    "    tf.set_random_seed(params.get('rseed', 10))\n",
    "\n",
    "    # Check if this graph is going to be used for training.\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    if params['encoder_type'] == 'bow':\n",
    "        with tf.variable_scope(\"Encoder\"):\n",
    "            h_, xs_ = BOW_encoder(features['ids'], features['ns'],\n",
    "                                  is_training=is_training,\n",
    "                                  **params)\n",
    "    else:\n",
    "        raise ValueError(\"Error: unsupported encoder type \"\n",
    "                         \"'{:s}'\".format(params['encoder_type']))\n",
    "\n",
    "    # Construct softmax layer and loss functions\n",
    "    with tf.variable_scope(\"Output_Layer\"):\n",
    "        ce_loss_, logits_ = softmax_output_layer_conf(h_, labels, params['num_classes'],conf=features['conf'])\n",
    "\n",
    "    with tf.name_scope(\"Prediction\"):\n",
    "        pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "        pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "        predictions_dict = {\"proba\": pred_proba_, \"max\": pred_max_}\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        # If predict mode, don't bother computing loss.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          predictions=predictions_dict)\n",
    "\n",
    "    # L2 regularization (weight decay) on parameters, from all layers\n",
    "    with tf.variable_scope(\"loss\"):\n",
    "        l2_penalty_ = tf.nn.l2_loss(xs_)  # l2 loss on embeddings\n",
    "        for var_ in tf.trainable_variables():\n",
    "            if \"Embedding_Layer\" in var_.name:\n",
    "                continue\n",
    "            l2_penalty_ += tf.nn.l2_loss(var_)\n",
    "        l2_penalty_ *= params['beta']  # scale by regularization strength\n",
    "        tf.summary.scalar(\"l2_penalty\", l2_penalty_)\n",
    "        regularized_loss_ = ce_loss_ + l2_penalty_\n",
    "        \n",
    "\n",
    "    with tf.variable_scope(\"Training\"):\n",
    "        if params['optimizer'] == 'adagrad':\n",
    "            optimizer_ = tf.train.AdagradOptimizer(params['lr'])\n",
    "        else:\n",
    "            optimizer_ = tf.train.GradientDescentOptimizer(params['lr'])\n",
    "        train_op_ = optimizer_.minimize(regularized_loss_,\n",
    "                                        global_step=tf.train.get_global_step())\n",
    "\n",
    "    tf.summary.scalar(\"cross_entropy_loss\", ce_loss_)\n",
    "    eval_metrics = {\"cross_entropy_loss\": tf.metrics.mean(ce_loss_),\n",
    "                    \"accuracy\": tf.metrics.accuracy(labels, pred_max_)}\n",
    "\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions_dict,\n",
    "                                      loss=regularized_loss_,\n",
    "                                      train_op=train_op_,\n",
    "                                      eval_metric_ops=eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_conf_score = []\n",
    "for i in range(U_n_train.shape[0]):\n",
    "    true_conf_score.append(1)\n",
    "    \n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train},\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_train = model.predict(input_fn=train_input_fn)  \n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test},\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions_test = model.predict(input_fn=test_input_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13840\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmp5tv2hvj4/model.ckpt-13840\n"
     ]
    }
   ],
   "source": [
    "predictions_train =list(predictions_train)\n",
    "predictions_test =list(predictions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u_train = []\n",
    "\n",
    "for i in range(len(predictions_train)):\n",
    "    conf_score_u_train.append(predictions_train[i]['max'])\n",
    "    \n",
    "conf_score_u_test = []\n",
    "\n",
    "for i in range(len(predictions_test)):\n",
    "    conf_score_u_test.append(predictions_test[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65\n",
      "INFO:tensorflow:Using config: {'_keep_checkpoint_every_n_hours': 10000, '_master': '', '_num_worker_replicas': 1, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1a86a214e0>, '_num_ps_replicas': 0, '_save_summary_steps': 100, '_save_checkpoints_secs': 600, '_task_id': 0, '_save_checkpoints_steps': None, '_log_step_count_steps': 100, '_keep_checkpoint_max': 5, '_model_dir': '/var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65', '_tf_random_seed': None, '_task_type': 'worker', '_service': None, '_session_config': None, '_is_chief': True}\n"
     ]
    }
   ],
   "source": [
    "model_params = dict(V=V, embed_dim=e, hidden_dims=h, num_classes=3,encoder_type='bow',lr=l,optimizer='adagrad'\n",
    "                    , beta=0.001)\n",
    "\n",
    "\n",
    "model1 = tf.estimator.Estimator(model_fn=classifier_model_fn_conf, \n",
    "                               params=model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the U dataset\n",
    "train_params = dict(batch_size=250, total_epochs=num_epoch, eval_every=1)\n",
    "assert(train_params['total_epochs'] % train_params['eval_every'] == 0)\n",
    "\n",
    "# Construct and train the model, saving checkpoints to the directory above.\n",
    "\n",
    "ns_u_train = np.reshape(ns_u_train,newshape=(len(ns_u_train),))\n",
    "y_u_train = np.reshape(y_u_train,newshape=(len(y_u_train),))\n",
    "conf_score_u_train = np.reshape(conf_score_u_train,newshape=(U_n_train.shape[0],))\n",
    "\n",
    "train_input_fn = patched_numpy_io.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_train, \"ns\": ns_u_train,\"conf\": conf_score_u_train}, y=y_u_train,\n",
    "                    batch_size=train_params['batch_size'], \n",
    "                    num_epochs=train_params['eval_every'], shuffle=True, seed=42\n",
    "                 )\n",
    "\n",
    "ns_u_test = np.reshape(ns_u_test,newshape=(len(ns_u_test),))\n",
    "y_u_test = np.reshape(y_u_test,newshape=(len(y_u_test),))\n",
    "conf_score_u_test = np.reshape(conf_score_u_test,newshape=(U_n_test.shape[0],))\n",
    "\n",
    "test_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_n_test, \"ns\": ns_u_test,\"conf\": conf_score_u_test}, y=y_u_test,\n",
    "                    batch_size=1000, num_epochs=1, shuffle=False\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.47146967, step = 1\n",
      "INFO:tensorflow:global_step/sec: 0.75293\n",
      "INFO:tensorflow:loss = 0.3513472, step = 101 (132.849 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.815976\n",
      "INFO:tensorflow:loss = 0.31139106, step = 201 (122.539 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.793335\n",
      "INFO:tensorflow:loss = 0.29981112, step = 301 (126.064 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 363 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.29596138.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-15:56:41\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-363\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-15:57:33\n",
      "INFO:tensorflow:Saving dict for global step 363: accuracy = 0.91376877, cross_entropy_loss = 0.03807931, global_step = 363, loss = 0.31476352\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-363\n",
      "INFO:tensorflow:Saving checkpoints for 364 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.3117947, step = 364\n",
      "INFO:tensorflow:global_step/sec: 0.890589\n",
      "INFO:tensorflow:loss = 0.32138053, step = 464 (112.288 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.834175\n",
      "INFO:tensorflow:loss = 0.30207753, step = 564 (119.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.804497\n",
      "INFO:tensorflow:loss = 0.2892445, step = 664 (124.319 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 726 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.28261873.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:04:51\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-726\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:05:41\n",
      "INFO:tensorflow:Saving dict for global step 726: accuracy = 0.9527361, cross_entropy_loss = 0.025868656, global_step = 726, loss = 0.30284524\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-726\n",
      "INFO:tensorflow:Saving checkpoints for 727 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30582798, step = 727\n",
      "INFO:tensorflow:global_step/sec: 0.796107\n",
      "INFO:tensorflow:loss = 0.31156552, step = 827 (125.632 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.795939\n",
      "INFO:tensorflow:loss = 0.29815248, step = 927 (125.629 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.826569\n",
      "INFO:tensorflow:loss = 0.2833322, step = 1027 (120.993 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1089 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2774165.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:13:13\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1089\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:14:00\n",
      "INFO:tensorflow:Saving dict for global step 1089: accuracy = 0.96204764, cross_entropy_loss = 0.020297248, global_step = 1089, loss = 0.2969976\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1089\n",
      "INFO:tensorflow:Saving checkpoints for 1090 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30273494, step = 1090\n",
      "INFO:tensorflow:global_step/sec: 0.880993\n",
      "INFO:tensorflow:loss = 0.3066791, step = 1190 (113.519 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.876212\n",
      "INFO:tensorflow:loss = 0.29574355, step = 1290 (114.128 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.874445\n",
      "INFO:tensorflow:loss = 0.27943814, step = 1390 (114.350 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1452 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2747624.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:20:55\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1452\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:21:40\n",
      "INFO:tensorflow:Saving dict for global step 1452: accuracy = 0.9669903, cross_entropy_loss = 0.017377064, global_step = 1452, loss = 0.2934743\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1452\n",
      "INFO:tensorflow:Saving checkpoints for 1453 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.30051416, step = 1453\n",
      "INFO:tensorflow:global_step/sec: 0.902575\n",
      "INFO:tensorflow:loss = 0.30316812, step = 1553 (110.805 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.786103\n",
      "INFO:tensorflow:loss = 0.29380202, step = 1653 (127.213 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.855973\n",
      "INFO:tensorflow:loss = 0.2766583, step = 1753 (116.814 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1815 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.27303264.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:28:52\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1815\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:29:39\n",
      "INFO:tensorflow:Saving dict for global step 1815: accuracy = 0.96884376, cross_entropy_loss = 0.015622104, global_step = 1815, loss = 0.29094297\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-1815\n",
      "INFO:tensorflow:Saving checkpoints for 1816 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2985949, step = 1816\n",
      "INFO:tensorflow:global_step/sec: 0.861459\n",
      "INFO:tensorflow:loss = 0.30036327, step = 1916 (116.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.867853\n",
      "INFO:tensorflow:loss = 0.29184002, step = 2016 (115.223 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.850207\n",
      "INFO:tensorflow:loss = 0.27453974, step = 2116 (117.643 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2178 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2717091.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:36:53\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2178\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:37:47\n",
      "INFO:tensorflow:Saving dict for global step 2178: accuracy = 0.9697705, cross_entropy_loss = 0.014457578, global_step = 2178, loss = 0.28892115\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2178\n",
      "INFO:tensorflow:Saving checkpoints for 2179 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29682267, step = 2179\n",
      "INFO:tensorflow:global_step/sec: 0.801129\n",
      "INFO:tensorflow:loss = 0.29788378, step = 2279 (124.841 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.766482\n",
      "INFO:tensorflow:loss = 0.28979528, step = 2379 (130.458 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.780166\n",
      "INFO:tensorflow:loss = 0.27284604, step = 2479 (128.191 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2541 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.27060637.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:45:26\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2541\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:46:14\n",
      "INFO:tensorflow:Saving dict for global step 2541: accuracy = 0.97038835, cross_entropy_loss = 0.013619495, global_step = 2541, loss = 0.28720114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2541\n",
      "INFO:tensorflow:Saving checkpoints for 2542 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2951253, step = 2542\n",
      "INFO:tensorflow:global_step/sec: 0.789707\n",
      "INFO:tensorflow:loss = 0.29560435, step = 2642 (126.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.854008\n",
      "INFO:tensorflow:loss = 0.2876664, step = 2742 (117.085 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.895433\n",
      "INFO:tensorflow:loss = 0.27142787, step = 2842 (111.687 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 2904 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26963872.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-16:53:26\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2904\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-16:54:13\n",
      "INFO:tensorflow:Saving dict for global step 2904: accuracy = 0.970609, cross_entropy_loss = 0.01297728, global_step = 2904, loss = 0.2856813\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-2904\n",
      "INFO:tensorflow:Saving checkpoints for 2905 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29346657, step = 2905\n",
      "INFO:tensorflow:global_step/sec: 0.853553\n",
      "INFO:tensorflow:loss = 0.2934687, step = 3005 (117.159 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.807339\n",
      "INFO:tensorflow:loss = 0.28547105, step = 3105 (123.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.829743\n",
      "INFO:tensorflow:loss = 0.27019516, step = 3205 (120.523 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3267 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26876146.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:01:30\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3267\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:02:21\n",
      "INFO:tensorflow:Saving dict for global step 3267: accuracy = 0.97091794, cross_entropy_loss = 0.012460771, global_step = 3267, loss = 0.28430507\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3267\n",
      "INFO:tensorflow:Saving checkpoints for 3268 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29182962, step = 3268\n",
      "INFO:tensorflow:global_step/sec: 0.770798\n",
      "INFO:tensorflow:loss = 0.29144624, step = 3368 (129.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.803544\n",
      "INFO:tensorflow:loss = 0.28323317, step = 3468 (124.434 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.803177\n",
      "INFO:tensorflow:loss = 0.2690924, step = 3568 (124.504 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3630 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26794896.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:09:57\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3630\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:10:49\n",
      "INFO:tensorflow:Saving dict for global step 3630: accuracy = 0.9710944, cross_entropy_loss = 0.012029773, global_step = 3630, loss = 0.28304014\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3630\n",
      "INFO:tensorflow:Saving checkpoints for 3631 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.29020834, step = 3631\n",
      "INFO:tensorflow:global_step/sec: 0.802314\n",
      "INFO:tensorflow:loss = 0.2895187, step = 3731 (124.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.773717\n",
      "INFO:tensorflow:loss = 0.28098062, step = 3831 (129.268 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.73723\n",
      "INFO:tensorflow:loss = 0.2680842, step = 3931 (135.642 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 3993 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26718533.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:18:42\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3993\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:19:35\n",
      "INFO:tensorflow:Saving dict for global step 3993: accuracy = 0.9711827, cross_entropy_loss = 0.011660057, global_step = 3993, loss = 0.28186318\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-3993\n",
      "INFO:tensorflow:Saving checkpoints for 3994 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28860196, step = 3994\n",
      "INFO:tensorflow:global_step/sec: 0.806692\n",
      "INFO:tensorflow:loss = 0.2876749, step = 4094 (123.966 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.7891\n",
      "INFO:tensorflow:loss = 0.27874467, step = 4194 (126.738 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.796206\n",
      "INFO:tensorflow:loss = 0.26714748, step = 4294 (125.592 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4356 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26645958.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:27:13\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-4356\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:28:03\n",
      "INFO:tensorflow:Saving dict for global step 4356: accuracy = 0.97135925, cross_entropy_loss = 0.011336402, global_step = 4356, loss = 0.28075847\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-4356\n",
      "INFO:tensorflow:Saving checkpoints for 4357 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28701237, step = 4357\n",
      "INFO:tensorflow:global_step/sec: 0.854853\n",
      "INFO:tensorflow:loss = 0.28590718, step = 4457 (116.981 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.789293\n",
      "INFO:tensorflow:loss = 0.27655944, step = 4557 (126.717 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.804016\n",
      "INFO:tensorflow:loss = 0.26626632, step = 4657 (124.362 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4719 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26576346.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:35:32\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-4719\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:36:21\n",
      "INFO:tensorflow:Saving dict for global step 4719: accuracy = 0.97153574, cross_entropy_loss = 0.011048816, global_step = 4719, loss = 0.2797145\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-4719\n",
      "INFO:tensorflow:Saving checkpoints for 4720 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28544176, step = 4720\n",
      "INFO:tensorflow:global_step/sec: 0.797231\n",
      "INFO:tensorflow:loss = 0.28420937, step = 4820 (125.447 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.782914\n",
      "INFO:tensorflow:loss = 0.27446038, step = 4920 (127.725 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.768826\n",
      "INFO:tensorflow:loss = 0.2654297, step = 5020 (130.068 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5082 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26509008.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:44:10\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5082\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:44:58\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving dict for global step 5082: accuracy = 0.9717564, cross_entropy_loss = 0.010790435, global_step = 5082, loss = 0.27872294\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5082\n",
      "INFO:tensorflow:Saving checkpoints for 5083 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2838929, step = 5083\n",
      "INFO:tensorflow:global_step/sec: 0.791358\n",
      "INFO:tensorflow:loss = 0.28257632, step = 5183 (126.371 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.81256\n",
      "INFO:tensorflow:loss = 0.2724814, step = 5283 (123.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.886787\n",
      "INFO:tensorflow:loss = 0.26462966, step = 5383 (112.746 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5445 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2644332.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-17:52:19\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5445\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-17:53:07\n",
      "INFO:tensorflow:Saving dict for global step 5445: accuracy = 0.9717564, cross_entropy_loss = 0.010556301, global_step = 5445, loss = 0.27777672\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5445\n",
      "INFO:tensorflow:Saving checkpoints for 5446 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28236946, step = 5446\n",
      "INFO:tensorflow:global_step/sec: 0.76527\n",
      "INFO:tensorflow:loss = 0.28100303, step = 5546 (130.688 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.81341\n",
      "INFO:tensorflow:loss = 0.27065006, step = 5646 (122.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.847183\n",
      "INFO:tensorflow:loss = 0.26386043, step = 5746 (118.038 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5808 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26378712.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-18:00:35\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5808\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-18:01:21\n",
      "INFO:tensorflow:Saving dict for global step 5808: accuracy = 0.97197706, cross_entropy_loss = 0.010342679, global_step = 5808, loss = 0.2768698\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-5808\n",
      "INFO:tensorflow:Saving checkpoints for 5809 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.28087527, step = 5809\n",
      "INFO:tensorflow:global_step/sec: 0.874175\n",
      "INFO:tensorflow:loss = 0.27948463, step = 5909 (114.404 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.876154\n",
      "INFO:tensorflow:loss = 0.26898316, step = 6009 (114.127 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.878032\n",
      "INFO:tensorflow:loss = 0.26311716, step = 6109 (113.892 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6171 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26314694.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-18:08:28\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6171\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-18:09:16\n",
      "INFO:tensorflow:Saving dict for global step 6171: accuracy = 0.9721536, cross_entropy_loss = 0.010146658, global_step = 6171, loss = 0.27599728\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6171\n",
      "INFO:tensorflow:Saving checkpoints for 6172 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.279415, step = 6172\n",
      "INFO:tensorflow:global_step/sec: 0.842927\n",
      "INFO:tensorflow:loss = 0.27801707, step = 6272 (118.649 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.792262\n",
      "INFO:tensorflow:loss = 0.26748452, step = 6372 (126.230 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.852098\n",
      "INFO:tensorflow:loss = 0.26239654, step = 6472 (117.336 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6534 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26250905.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-18:16:38\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6534\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-18:17:30\n",
      "INFO:tensorflow:Saving dict for global step 6534: accuracy = 0.972286, cross_entropy_loss = 0.0099659255, global_step = 6534, loss = 0.27515483\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6534\n",
      "INFO:tensorflow:Saving checkpoints for 6535 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.2779934, step = 6535\n",
      "INFO:tensorflow:global_step/sec: 0.849831\n",
      "INFO:tensorflow:loss = 0.27659672, step = 6635 (117.683 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.753447\n",
      "INFO:tensorflow:loss = 0.26614514, step = 6735 (132.726 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.794932\n",
      "INFO:tensorflow:loss = 0.26169544, step = 6835 (125.799 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6897 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.2618712.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-18:25:14\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6897\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-18:26:08\n",
      "INFO:tensorflow:Saving dict for global step 6897: accuracy = 0.9724625, cross_entropy_loss = 0.009798609, global_step = 6897, loss = 0.27433953\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-6897\n",
      "INFO:tensorflow:Saving checkpoints for 6898 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.27661514, step = 6898\n",
      "INFO:tensorflow:global_step/sec: 0.754032\n",
      "INFO:tensorflow:loss = 0.27522078, step = 6998 (132.640 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.763344\n",
      "INFO:tensorflow:loss = 0.26494768, step = 7098 (131.006 sec)\n",
      "INFO:tensorflow:global_step/sec: 0.801881\n",
      "INFO:tensorflow:loss = 0.26101157, step = 7198 (124.712 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 7260 into /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.26123226.\n",
      "INFO:tensorflow:Starting evaluation at 2018-03-24-18:33:57\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-7260\n",
      "INFO:tensorflow:Finished evaluation at 2018-03-24-18:34:47\n",
      "INFO:tensorflow:Saving dict for global step 7260: accuracy = 0.97255075, cross_entropy_loss = 0.009643179, global_step = 7260, loss = 0.27354783\n"
     ]
    }
   ],
   "source": [
    "for _ in range(train_params['total_epochs'] // train_params['eval_every']):\n",
    "    # Train for a few epochs, then evaluate on test\n",
    "    model1.train(input_fn=train_input_fn)\n",
    "    eval_metrics = model1.evaluate(input_fn=test_input_fn, name=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_score_u = list(conf_score_u_train) + list(conf_score_u_test)\n",
    "conf_score_u = np.reshape(conf_score_u,newshape = (len(conf_score_u),))\n",
    "\n",
    "predict_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "                    x={\"ids\": U_sent, \"ns\":u_ns , \"conf\": conf_score_u},\n",
    "                    batch_size=20, num_epochs=1, shuffle=False\n",
    "                )\n",
    "\n",
    "predictions = model1.predict(input_fn=predict_input_fn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from /var/folders/01/3kbvf6m17_n2z52_db_v4fxm0000gn/T/tmpmh6cxg65/model.ckpt-7260\n"
     ]
    }
   ],
   "source": [
    "predictions = list(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_U = []\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    pred_label_U.append(predictions[i]['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
